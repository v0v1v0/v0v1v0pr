<div class="container">

<table style="width: 100%;"><tr>
<td>rm.trees</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Remove trees from a random Uniform Forest
</h2>

<h3>Description</h3>

<p>Remove any number of trees from a random Uniform Forest, especially in case of incremental learning.
</p>


<h3>Usage</h3>

<pre><code class="language-R">rm.trees(rufObject, X = NULL, Y = NULL, 
	method = c("default", "random", "oldest", "newest", "optimal", "quantile"), 
	howMany = NULL, 
	rm.sample = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>rufObject</code></td>
<td>

<p>a object of class randomUniformForest.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>

<p>a validation sample, in case of removing 'method' is "optimal". 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>

<p>a vector of responses for validation sample.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>

<p>method to used for removing trees. Note that "optimal" and "quantile" are not fully tested and can lead to unexpected results. "default" work only if OOB evaluation is not missing. Use "random" if only one forest has been set. If ensemble of forests (e.g. incremental learning), use "oldest" or "newest" (but a validation sample may be necessary).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>howMany</code></td>
<td>

<p>how many trees to remove ?
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rm.sample</code></td>
<td>

<p>if method = "random", 'rm.sample' is the proportion of trees to remove.
</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Saip Ciss <a href="mailto:saip.ciss@wanadoo.fr">saip.ciss@wanadoo.fr</a>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Classification : "breast cancer" data 
## (http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))

## data(breastCancer)
# breastCancer.data &lt;- breastCancer 

## remove ID (first column) and divide data in train and test set
# breastCancer.data = breastCancer.data[,-1]

# n &lt;- nrow(breastCancer.data)
# p &lt;- ncol(breastCancer.data)

# trainTestIdx &lt;- cut(sample(1:n, n), 2, labels= FALSE)

## train examples
# breastCancer.data.train &lt;- breastCancer.data[trainTestIdx == 1, -p]
# breastCancer.class.train &lt;- as.factor(breastCancer.data[trainTestIdx == 1, p])

## rename class in benign (class 2) and malignant (class 4) to have a better view
# levels(breastCancer.class.train) = c("benign", "malignant")

## test data
# breastCancer.data.test &lt;- breastCancer.data[trainTestIdx == 2, -p]
# breastCancer.class.test &lt;- as.factor(breastCancer.data[trainTestIdx == 2, p])
# levels(breastCancer.class.test) = c("benign", "malignant")

## compute many trees in a fast way, removing OOB evaluation
## breastCancer.ruf &lt;- randomUniformForest(breastCancer.data.train, breastCancer.class.train, 
## classwt = c(1, 3.5), maxnodes = 10, OOB = FALSE, ntree = 500, threads = 2)

## predict and see results
## pred.breastCancer.ruf &lt;- predict(breastCancer.ruf, breastCancer.data.test)
# confusion.matrix(as.numeric(pred.breastCancer.ruf), as.numeric(breastCancer.class.test))

## remove some trees at random. Default method can not be used since we don't have OOB data
# breastCancer.ruf &lt;- rm.trees(breastCancer.ruf, method = "random", rm.sample = 0.2)

# breastCancer.ruf # old number of trees is still printed, but trees have been removed

## predict and see results again 
# pred.breastCancer.ruf &lt;- predict(breastCancer.ruf, breastCancer.data.test)
# confusion.matrix(as.numeric(pred.breastCancer.ruf), as.numeric(breastCancer.class.test))
</code></pre>


</div>