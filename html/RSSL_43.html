<div class="container">

<table style="width: 100%;"><tr>
<td>LaplacianKernelLeastSquaresClassifier</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Laplacian Regularized Least Squares Classifier</h2>

<h3>Description</h3>

<p>Implements manifold regularization through the graph Laplacian as proposed by Belkin et al. 2006. As an adjacency matrix, we use the k nearest neighbour graph based on a chosen distance (default: euclidean).
</p>


<h3>Usage</h3>

<pre><code class="language-R">LaplacianKernelLeastSquaresClassifier(X, y, X_u, lambda = 0, gamma = 0,
  kernel = kernlab::vanilladot(), adjacency_distance = "euclidean",
  adjacency_k = 6, x_center = TRUE, scale = TRUE, y_scale = TRUE,
  normalized_laplacian = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X_u</code></td>
<td>
<p>matrix; Design matrix for unlabeled data</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>numeric; L2 regularization parameter</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>numeric; Weight of the unlabeled data</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel</code></td>
<td>
<p>kernlab::kernel to use</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>adjacency_distance</code></td>
<td>
<p>character; distance metric used to construct adjacency graph from the dist function. Default: "euclidean"</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>adjacency_k</code></td>
<td>
<p>integer; Number of of neighbours used to construct adjacency graph.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y_scale</code></td>
<td>
<p>logical; whether the target vector should be centered</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>normalized_laplacian</code></td>
<td>
<p>logical; If TRUE use the normalized Laplacian, otherwise, the Laplacian is used</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Belkin, M., Niyogi, P. &amp; Sindhwani, V., 2006. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Journal of Machine Learning Research, 7, pp.2399-2434.
</p>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code>EMLeastSquaresClassifier</code>,
<code>EMLinearDiscriminantClassifier</code>,
<code>GRFClassifier</code>,
<code>ICLeastSquaresClassifier</code>,
<code>ICLinearDiscriminantClassifier</code>,
<code>KernelLeastSquaresClassifier</code>,
<code>LaplacianSVM</code>,
<code>LeastSquaresClassifier</code>,
<code>LinearDiscriminantClassifier</code>,
<code>LinearSVM</code>,
<code>LinearTSVM()</code>,
<code>LogisticLossClassifier</code>,
<code>LogisticRegression</code>,
<code>MCLinearDiscriminantClassifier</code>,
<code>MCNearestMeanClassifier</code>,
<code>MCPLDA</code>,
<code>MajorityClassClassifier</code>,
<code>NearestMeanClassifier</code>,
<code>QuadraticDiscriminantClassifier</code>,
<code>S4VM</code>,
<code>SVM</code>,
<code>SelfLearning</code>,
<code>TSVM</code>,
<code>USMLeastSquaresClassifier</code>,
<code>WellSVM</code>,
<code>svmlin()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(RSSL)
library(ggplot2)
library(dplyr)

## Example 1: Half moons

# Generate a dataset
set.seed(2)
df_orig &lt;- generateCrescentMoon(100,sigma = 0.3) 
df &lt;- df_orig %&gt;% 
  add_missinglabels_mar(Class~.,0.98)

lambda &lt;- 0.01
gamma &lt;- 10000
rbf_param &lt;- 0.125

# Train classifiers
## Not run: 
class_sup &lt;- KernelLeastSquaresClassifier(
                Class~.,df,
                kernel=kernlab::rbfdot(rbf_param),
                lambda=lambda,scale=FALSE)

class_lap &lt;- LaplacianKernelLeastSquaresClassifier(
                    Class~.,df,
                    kernel=kernlab::rbfdot(rbf_param),
                    lambda=lambda,gamma=gamma,
                    normalized_laplacian = TRUE,
                    scale=FALSE)

classifiers &lt;- list("Lap"=class_lap,"Sup"=class_sup)

# Plot classifiers (can take a couple of seconds)

df %&gt;% 
  ggplot(aes(x=X1,y=X2,color=Class)) +
  geom_point() +
  coord_equal() +
  stat_classifier(aes(linetype=..classifier..),
                  classifiers = classifiers ,
                  color="black")


# Calculate the loss
lapply(classifiers,function(c) mean(loss(c,df_orig)))

## End(Not run)

## Example 2: Two circles
set.seed(1)
df_orig &lt;- generateTwoCircles(1000,noise=0.05)
df &lt;- df_orig %&gt;% 
  add_missinglabels_mar(Class~.,0.994)

lambda &lt;- 10e-12
gamma &lt;- 100
rbf_param &lt;- 0.1

# Train classifiers
## Not run: 
class_sup &lt;- KernelLeastSquaresClassifier(
  Class~.,df,
  kernel=kernlab::rbfdot(rbf_param),
  lambda=lambda,scale=TRUE)

class_lap &lt;- LaplacianKernelLeastSquaresClassifier(
  Class~.,df,
  kernel=kernlab::rbfdot(rbf_param),
  adjacency_k = 30,
  lambda=lambda,gamma=gamma,
  normalized_laplacian = TRUE,
  scale=TRUE)

classifiers &lt;- list("Lap"=class_lap,"Sup"=class_sup)

# Plot classifiers (Can take a couple of seconds)
df %&gt;% 
  ggplot(aes(x=X1,y=X2,color=Class,size=Class)) +
  scale_size_manual(values=c("1"=3,"2"=3),na.value=1) +
  geom_point() +
  coord_equal() +
  stat_classifier(aes(linetype=..classifier..),
                  classifiers = classifiers ,
                  color="black",size=1)

## End(Not run)
</code></pre>


</div>