<div class="container">

<table style="width: 100%;"><tr>
<td>performance</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Function to create performance objects</h2>

<h3>Description</h3>

<p>All kinds of predictor evaluations are performed using this function.
</p>


<h3>Usage</h3>

<pre><code class="language-R">performance(prediction.obj, measure, x.measure = "cutoff", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>prediction.obj</code></td>
<td>
<p>An object of class <code>prediction</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>measure</code></td>
<td>
<p>Performance measure to use for the evaluation. A complete list
of the performance measures that are available for <code>measure</code> and
<code>x.measure</code> is given in the 'Details' section.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x.measure</code></td>
<td>
<p>A second performance measure. If different from the default,
a two-dimensional curve, with <code>x.measure</code> taken to be the unit in
direction of the x axis, and <code>measure</code> to be the unit in direction of
the y axis, is created. This curve is parametrized with the cutoff.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Optional arguments (specific to individual performance measures).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Here is the list of available performance measures. Let Y and
<code class="reqn">\hat{Y}</code> be random variables representing the class and the prediction for
a randomly drawn sample, respectively. We denote by
<code class="reqn">\oplus</code> and <code class="reqn">\ominus</code> the positive and
negative class, respectively. Further, we use the following
abbreviations for empirical quantities: P (\# positive
samples), N (\# negative samples), TP (\# true positives), TN (\# true
negatives), FP (\# false positives), FN (\# false negatives).
</p>

<dl>
<dt>
<code>acc</code>:</dt>
<dd>
<p>Accuracy. <code class="reqn">P(\hat{Y}=Y)</code>. Estimated
as: <code class="reqn">\frac{TP+TN}{P+N}</code>.</p>
</dd>
<dt>
<code>err</code>:</dt>
<dd>
<p>Error rate. <code class="reqn">P(\hat{Y}\ne Y)</code>. Estimated as: <code class="reqn">\frac{FP+FN}{P+N}</code>.</p>
</dd>
<dt>
<code>fpr</code>:</dt>
<dd>
<p>False positive rate. <code class="reqn">P(\hat{Y}=\oplus | Y =
                                                   \ominus)</code>. Estimated as:
<code class="reqn">\frac{FP}{N}</code>.</p>
</dd>
<dt>
<code>fall</code>:</dt>
<dd>
<p>Fallout. Same as <code>fpr</code>.</p>
</dd>
<dt>
<code>tpr</code>:</dt>
<dd>
<p>True positive
rate. <code class="reqn">P(\hat{Y}=\oplus|Y=\oplus)</code>. Estimated
as: <code class="reqn">\frac{TP}{P}</code>.</p>
</dd>
<dt>
<code>rec</code>:</dt>
<dd>
<p>Recall. Same as <code>tpr</code>.</p>
</dd>
<dt>
<code>sens</code>:</dt>
<dd>
<p>Sensitivity. Same as <code>tpr</code>.</p>
</dd>
<dt>
<code>fnr</code>:</dt>
<dd>
<p>False negative
rate. <code class="reqn">P(\hat{Y}=\ominus|Y=\oplus)</code>. Estimated as: <code class="reqn">\frac{FN}{P}</code>.</p>
</dd>
<dt>
<code>miss</code>:</dt>
<dd>
<p>Miss. Same as <code>fnr</code>.</p>
</dd>
<dt>
<code>tnr</code>:</dt>
<dd>
<p>True negative rate. <code class="reqn">P(\hat{Y} =
                                                  \ominus|Y=\ominus)</code>.</p>
</dd>
<dt>
<code>spec</code>:</dt>
<dd>
<p>Specificity. Same as <code>tnr</code>.</p>
</dd>
<dt>
<code>ppv</code>:</dt>
<dd>
<p>Positive predictive
value. <code class="reqn">P(Y=\oplus|\hat{Y}=\oplus)</code>. Estimated as: <code class="reqn">\frac{TP}{TP+FP}</code>.</p>
</dd>
<dt>
<code>prec</code>:</dt>
<dd>
<p>Precision. Same as <code>ppv</code>.</p>
</dd>
<dt>
<code>npv</code>:</dt>
<dd>
<p>Negative predictive
value. <code class="reqn">P(Y=\ominus|\hat{Y}=\ominus)</code>. Estimated as: <code class="reqn">\frac{TN}{TN+FN}</code>.</p>
</dd>
<dt>
<code>pcfall</code>:</dt>
<dd>
<p>Prediction-conditioned
fallout. <code class="reqn">P(Y=\ominus|\hat{Y}=\oplus)</code>. Estimated as: <code class="reqn">\frac{FP}{TP+FP}</code>.</p>
</dd>
<dt>
<code>pcmiss</code>:</dt>
<dd>
<p>Prediction-conditioned
miss. <code class="reqn">P(Y=\oplus|\hat{Y}=\ominus)</code>. Estimated as: <code class="reqn">\frac{FN}{TN+FN}</code>.</p>
</dd>
<dt>
<code>rpp</code>:</dt>
<dd>
<p>Rate of positive predictions. <code class="reqn">P( \hat{Y} =
                                                           \oplus)</code>. Estimated as: (TP+FP)/(TP+FP+TN+FN).</p>
</dd>
<dt>
<code>rnp</code>:</dt>
<dd>
<p>Rate of negative predictions. <code class="reqn">P( \hat{Y} =
                                                           \ominus)</code>. Estimated as: (TN+FN)/(TP+FP+TN+FN).</p>
</dd>
<dt>
<code>phi</code>:</dt>
<dd>
<p>Phi correlation coefficient. <code class="reqn">\frac{TP \cdot
   TN - FP \cdot FN}{\sqrt{ (TP+FN) \cdot (TN+FP) \cdot (TP+FP)
     \cdot (TN+FN)}}</code>. Yields a
number between -1 and 1, with 1 indicating a perfect
prediction, 0 indicating a random prediction. Values below 0
indicate a worse than random prediction.</p>
</dd>
<dt>
<code>mat</code>:</dt>
<dd>
<p>Matthews correlation coefficient. Same as <code>phi</code>.</p>
</dd>
<dt>
<code>mi</code>:</dt>
<dd>
<p>Mutual information. <code class="reqn">I(\hat{Y},Y) := H(Y) -
     H(Y|\hat{Y})</code>, where H is the
(conditional) entropy. Entropies are estimated naively (no bias
correction).</p>
</dd>
<dt>
<code>chisq</code>:</dt>
<dd>
<p>Chi square test statistic. <code>?chisq.test</code>
for details. Note that R might raise a warning if the sample size
is too small.</p>
</dd>
<dt>
<code>odds</code>:</dt>
<dd>
<p>Odds ratio. <code class="reqn">\frac{TP \cdot TN}{FN \cdot
   FP}</code>. Note that odds ratio produces
Inf or NA values for all cutoffs corresponding to FN=0 or
FP=0. This can substantially decrease the plotted cutoff region.</p>
</dd>
<dt>
<code>lift</code>:</dt>
<dd>
<p>Lift
value. <code class="reqn">\frac{P(\hat{Y}=\oplus|Y=\oplus)}{P(\hat{Y}=\oplus)}</code>.</p>
</dd>
<dt>
<code>f</code>:</dt>
<dd>
<p>Precision-recall F measure (van Rijsbergen, 1979). Weighted
harmonic mean of precision (P) and recall (R). <code class="reqn">F =
     \frac{1}{\alpha \frac{1}{P} + (1-\alpha)\frac{1}{R}}</code>. If
<code class="reqn">\alpha=\frac{1}{2}</code>, the mean is balanced. A
frequent equivalent formulation is
<code class="reqn">F = \frac{(\beta^2+1) \cdot P \cdot R}{R + \beta^2 \cdot
     P}</code>. In this formulation, the
mean is balanced if <code class="reqn">\beta=1</code>. Currently, ROCR only accepts
the alpha version as input (e.g. <code class="reqn">\alpha=0.5</code>). If no 
value for alpha is given, the mean will be balanced by default.</p>
</dd>
<dt>
<code>rch</code>:</dt>
<dd>
<p>ROC convex hull. A ROC (=<code>tpr</code> vs <code>fpr</code>) curve 
with concavities (which represent suboptimal choices of cutoff) removed 
(Fawcett 2001). Since the result is already a parametric performance 
curve, it cannot be used in combination with other measures.</p>
</dd>
<dt>
<code>auc</code>:</dt>
<dd>
<p>Area under the ROC curve. This is equal to the value of the
Wilcoxon-Mann-Whitney test statistic and also the probability that the
classifier will score are randomly drawn positive sample higher than a
randomly drawn negative sample. Since the output of
<code>auc</code> is cutoff-independent, this
measure cannot be combined with other measures into a parametric
curve. The partial area under the ROC curve up to a given false
positive rate can be calculated by passing the optional parameter
<code>fpr.stop=0.5</code> (or any other value between 0 and 1) to 
<code>performance</code>.</p>
</dd>
<dt>
<code>aucpr</code>:</dt>
<dd>
<p>Area under the Precision/Recall curve. Since the output
of <code>aucpr</code> is cutoff-independent, this measure cannot be combined 
with other measures into a parametric curve.</p>
</dd>
<dt>
<code>prbe</code>:</dt>
<dd>
<p>Precision-recall break-even point. The cutoff(s) where
precision and recall are equal. At this point, positive and negative
predictions are made at the same rate as their prevalence in the
data. Since the output of
<code>prbe</code> is just a cutoff-independent scalar, this
measure cannot be combined with other measures into a parametric curve.</p>
</dd>
<dt>
<code>cal</code>:</dt>
<dd>
<p>Calibration error. The calibration error is the
absolute difference between predicted confidence and actual reliability. This
error is estimated at all cutoffs by sliding a window across the
range of possible cutoffs. The default window size of 100 can be
adjusted by passing the optional parameter <code>window.size=200</code>
to <code>performance</code>. E.g., if for several
positive samples the output of the classifier is around 0.75, you might
expect from a well-calibrated classifier that the fraction of them
which is correctly predicted as positive is also around 0.75. In a
well-calibrated classifier, the probabilistic confidence estimates
are realistic. Only for use with
probabilistic output (i.e. scores between 0 and 1).</p>
</dd>
<dt>
<code>mxe</code>:</dt>
<dd>
<p>Mean cross-entropy. Only for use with
probabilistic output. <code class="reqn">MXE :=-\frac{1}{P+N}( \sum_{y_i=\oplus}
                                                   ln(\hat{y}_i) + \sum_{y_i=\ominus} ln(1-\hat{y}_i))</code>. Since the output of
<code>mxe</code> is just a cutoff-independent scalar, this
measure cannot be combined with other measures into a parametric curve.</p>
</dd>
<dt>
<code>rmse</code>:</dt>
<dd>
<p>Root-mean-squared error. Only for use with
numerical class labels. <code class="reqn">RMSE:=\sqrt{\frac{1}{P+N}\sum_i (y_i
                                                                 - \hat{y}_i)^2}</code>. Since the output of
<code>rmse</code> is just a cutoff-independent scalar, this
measure cannot be combined with other measures into a parametric curve.</p>
</dd>
<dt>
<code>sar</code>:</dt>
<dd>
<p>Score combinining performance measures of different
characteristics, in the attempt of creating a more "robust"
measure (cf. Caruana R., ROCAI2004):
SAR = 1/3 * ( Accuracy + Area under the ROC curve + Root
mean-squared error ).</p>
</dd>
<dt>
<code>ecost</code>:</dt>
<dd>
<p>Expected cost. For details on cost curves,
cf. Drummond&amp;Holte 2000,2004. <code>ecost</code> has an obligatory x
axis, the so-called 'probability-cost function'; thus it cannot be
combined with other measures. While using <code>ecost</code> one is
interested in the lower envelope of a set of lines, it might be
instructive to plot the whole set of lines in addition to the lower
envelope. An example is given in <code>demo(ROCR)</code>.</p>
</dd>
<dt>
<code>cost</code>:</dt>
<dd>
<p>Cost of a classifier when
class-conditional misclassification costs are explicitly given.
Accepts the optional parameters <code>cost.fp</code> and
<code>cost.fn</code>, by which the costs for false positives and
negatives can be adjusted, respectively. By default, both are set
to 1.</p>
</dd>
</dl>
<h3>Value</h3>

<p>An S4 object of class <code>performance</code>.
</p>


<h3>Note</h3>

<p>Here is how to call <code>performance()</code> to create some standard
evaluation plots:
</p>

<dl>
<dt>ROC curves:</dt>
<dd>
<p>measure="tpr", x.measure="fpr".</p>
</dd>
<dt>Precision/recall graphs:</dt>
<dd>
<p>measure="prec", x.measure="rec".</p>
</dd>
<dt>Sensitivity/specificity plots:</dt>
<dd>
<p>measure="sens", x.measure="spec".</p>
</dd>
<dt>Lift charts:</dt>
<dd>
<p>measure="lift", x.measure="rpp".</p>
</dd>
</dl>
<h3>Author(s)</h3>

<p>Tobias Sing <a href="mailto:tobias.sing@gmail.com">tobias.sing@gmail.com</a>, Oliver Sander
<a href="mailto:osander@gmail.com">osander@gmail.com</a>
</p>


<h3>References</h3>

<p>A detailed list of references can be found on the ROCR homepage at
<a href="http://rocr.bioinf.mpi-sb.mpg.de">http://rocr.bioinf.mpi-sb.mpg.de</a>.
</p>


<h3>See Also</h3>

<p><code>prediction</code>,
<code>prediction-class</code>,
<code>performance-class</code>,
<code>plot.performance</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># computing a simple ROC curve (x-axis: fpr, y-axis: tpr)
library(ROCR)
data(ROCR.simple)
pred &lt;- prediction( ROCR.simple$predictions, ROCR.simple$labels)
pred
perf &lt;- performance(pred,"tpr","fpr")
perf
plot(perf)

# precision/recall curve (x-axis: recall, y-axis: precision)
perf &lt;- performance(pred, "prec", "rec")
perf
plot(perf)

# sensitivity/specificity curve (x-axis: specificity,
# y-axis: sensitivity)
perf &lt;- performance(pred, "sens", "spec")
perf
plot(perf)
</code></pre>


</div>