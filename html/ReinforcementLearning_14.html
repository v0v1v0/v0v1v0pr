<div class="container">

<table style="width: 100%;"><tr>
<td>selectEpsilonGreedyAction</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Performs <code class="reqn">\varepsilon</code>-greedy action selection</h2>

<h3>Description</h3>

<p>Implements <code class="reqn">\varepsilon</code>-greedy action selection. In this strategy, the agent explores the environment
by selecting an action at random with probability <code class="reqn">\varepsilon</code>. Alternatively, the agent exploits its
current knowledge by choosing the optimal action with probability <code class="reqn">1-\varepsilon</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">selectEpsilonGreedyAction(Q, state, epsilon)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Q</code></td>
<td>
<p>State-action table of type <code>hash</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>state</code></td>
<td>
<p>The current state.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>Exploration rate between 0 and 1.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Character value defining the next action.
</p>


<h3>References</h3>

<p>Sutton and Barto (1998). "Reinforcement Learning: An Introduction", MIT Press, Cambridge, MA.
</p>


</div>