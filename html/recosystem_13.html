<div class="container">

<table style="width: 100%;"><tr>
<td>tune</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Tuning Model Parameters</h2>

<h3>Description</h3>

<p>This method is a member function of class "<code>RecoSys</code>"
that uses cross validation to tune the model parameters.
</p>
<p>The common usage of this method is
</p>
<pre>r = Reco()
r$tune(train_data, opts = list(dim      = c(10L, 20L),
                               costp_l1 = c(0, 0.1),
                               costp_l2 = c(0.01, 0.1),
                               costq_l1 = c(0, 0.1),
                               costq_l2 = c(0.01, 0.1),
                               lrate    = c(0.01, 0.1))
)</pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>r</code></td>
<td>
<p>Object returned by <code>Reco</code>().</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>train_data</code></td>
<td>
<p>An object of class "DataSource" that describes the source
of training data, typically returned by function
<code>data_file()</code>, <code>data_memory()</code>,
or <code>data_matrix()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>opts</code></td>
<td>
<p>A number of candidate tuning parameter values and extra options in the
model tuning procedure. See section <strong>Parameters and Options</strong>
for details.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A list with two components:
</p>

<dl>
<dt><code>min</code></dt>
<dd>
<p>Parameter values with minimum cross validated loss.
This is a list that can be passed to the
<code>opts</code> argument in <code>$train()</code>.</p>
</dd>
<dt><code>res</code></dt>
<dd>
<p>A data frame giving the supplied candidate
values of tuning parameters, and one column showing the
loss function value associated with each combination.</p>
</dd>
</dl>
<h3>Parameters and Options</h3>

<p>The <code>opts</code> argument should be a list that provides the candidate values
of tuning parameters and some other options. For tuning parameters (<code>dim</code>,
<code>costp_l1</code>, <code>costp_l2</code>, <code>costq_l1</code>, <code>costq_l2</code>,
and <code>lrate</code>), users can provide a numeric vector for each one, so that
the model will be evaluated on each combination of the candidate values.
For other non-tuning options, users should give a single value. If a parameter
or option is not set by the user, the program will use a default one.
</p>
<p>See below for the list of available parameters and options:
</p>

<dl>
<dt><code>dim</code></dt>
<dd>
<p>Tuning parameter, the number of latent factors.
Can be specified as an integer vector, with default value
<code>c(10L, 20L)</code>.</p>
</dd>
<dt><code>costp_l1</code></dt>
<dd>
<p>Tuning parameter, the L1 regularization cost for user factors.
Can be specified as a numeric vector, with default value
<code>c(0, 0.1)</code>.</p>
</dd>
<dt><code>costp_l2</code></dt>
<dd>
<p>Tuning parameter, the L2 regularization cost for user factors.
Can be specified as a numeric vector, with default value
<code>c(0.01, 0.1)</code>.</p>
</dd>
<dt><code>costq_l1</code></dt>
<dd>
<p>Tuning parameter, the L1 regularization cost for item factors.
Can be specified as a numeric vector, with default value
<code>c(0, 0.1)</code>.</p>
</dd>
<dt><code>costq_l2</code></dt>
<dd>
<p>Tuning parameter, the L2 regularization cost for item factors.
Can be specified as a numeric vector, with default value
<code>c(0.01, 0.1)</code>.</p>
</dd>
<dt><code>lrate</code></dt>
<dd>
<p>Tuning parameter, the learning rate, which can be thought
of as the step size in gradient descent.
Can be specified as a numeric vector, with default value
<code>c(0.01, 0.1)</code>.</p>
</dd>
<dt><code>loss</code></dt>
<dd>
<p>Character string, the loss function. Default is "l2", see
section <strong>Parameters and Options</strong> in <code>$train()</code>
for details.</p>
</dd>
<dt><code>nfold</code></dt>
<dd>
<p>Integer, the number of folds in cross validation. Default is 5.</p>
</dd>
<dt><code>niter</code></dt>
<dd>
<p>Integer, the number of iterations. Default is 20.</p>
</dd>
<dt><code>nthread</code></dt>
<dd>
<p>Integer, the number of threads for parallel
computing. Default is 1.</p>
</dd>
<dt><code>nbin</code></dt>
<dd>
<p>Integer, the number of bins. Must be greater than <code>nthread</code>.
Default is 20.</p>
</dd>
<dt><code>nmf</code></dt>
<dd>
<p>Logical, whether to perform non-negative matrix factorization.
Default is <code>FALSE</code>.</p>
</dd>
<dt><code>verbose</code></dt>
<dd>
<p>Logical, whether to show detailed information. Default is
<code>FALSE</code>.</p>
</dd>
<dt><code>progress</code></dt>
<dd>
<p>Logical, whether to show a progress bar. Default is <code>TRUE</code>.</p>
</dd>
</dl>
<h3>Author(s)</h3>

<p>Yixuan Qiu &lt;<a href="https://statr.me">https://statr.me</a>&gt;
</p>


<h3>References</h3>

<p>W.-S. Chin, Y. Zhuang, Y.-C. Juan, and C.-J. Lin.
A Fast Parallel Stochastic Gradient Method for Matrix Factorization in Shared Memory Systems.
ACM TIST, 2015.
</p>
<p>W.-S. Chin, Y. Zhuang, Y.-C. Juan, and C.-J. Lin.
A Learning-rate Schedule for Stochastic Gradient Methods to Matrix Factorization.
PAKDD, 2015.
</p>
<p>W.-S. Chin, B.-W. Yuan, M.-Y. Yang, Y. Zhuang, Y.-C. Juan, and C.-J. Lin.
LIBMF: A Library for Parallel Matrix Factorization in Shared-memory Systems.
Technical report, 2015.
</p>


<h3>See Also</h3>

<p><code>$train()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
train_set = system.file("dat", "smalltrain.txt", package = "recosystem")
train_src = data_file(train_set)
r = Reco()
set.seed(123) # This is a randomized algorithm
res = r$tune(
    train_src,
    opts = list(dim = c(10, 20, 30),
                costp_l1 = 0, costq_l1 = 0,
                lrate = c(0.05, 0.1, 0.2), nthread = 2)
)
r$train(train_src, opts = res$min)

## End(Not run)

</code></pre>


</div>