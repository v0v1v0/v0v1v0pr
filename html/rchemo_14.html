<div class="container">

<table style="width: 100%;"><tr>
<td>krr</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>KRR (LS-SVMR)</h2>

<h3>Description</h3>

<p>Kernel ridge regression models (KRR = LS-SVMR) (Suykens et al. 2000, Bennett &amp; Embrechts 2003, Krell 2018). 
</p>


<h3>Usage</h3>

<pre><code class="language-R">
krr(X, Y, weights = NULL, lb = 1e-2, kern = "krbf", ...)

## S3 method for class 'Krr'
coef(object, ..., lb = NULL)  

## S3 method for class 'Krr'
predict(object, X, ..., lb = NULL)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>For main function: Training X-data (<code class="reqn">n, p</code>). — For auxiliary function: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>Training Y-data (<code class="reqn">n, q</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>Weights (<code class="reqn">n, 1</code>) to apply to the training observations. Internally, weights are "normalized" to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lb</code></td>
<td>
<p>A value of regularization parameter <code class="reqn">lambda</code>. If <code>lb = 0</code>, a pseudo-inverse is used in the RR.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kern</code></td>
<td>
<p>Name of the function defining the considered kernel for building the Gram matrix. See <code>krbf</code> for syntax, and other available kernel functions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Optional arguments to pass in the kernel function defined in <code>kern</code> (e.g. <code>gamma</code> for <code>krbf</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>— For auxiliary function: A fitted model, output of a call to the main function.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>For <code>krr</code>: 
</p>
<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>Training X-data (<code class="reqn">n, p</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>
<p>Gram matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Kt</code></td>
<td>
<p>Gram matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>U</code></td>
<td>
<p>intermediate output.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>UtDY</code></td>
<td>
<p>intermediate output.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sv</code></td>
<td>
<p>singular values of the matrix (1,n)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lb</code></td>
<td>
<p>value of regularization parameter <code class="reqn">lambda</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ymeans</code></td>
<td>
<p>the centering vector of Y (q,1)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>the weights vector of X-variables (p,1)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kern</code></td>
<td>
<p>kern function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dots</code></td>
<td>
<p>Optional arguments.</p>
</td>
</tr>
</table>
<p>For <code>coef.Krr</code>:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>int</code></td>
<td>
<p>matrix (1,nlv) with the intercepts</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>matrix (n,nlv) with the coefficients</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>df</code></td>
<td>
<p>model complexity (number of degrees of freedom)</p>
</td>
</tr>
</table>
<p>For <code>predict.Krr</code>:
</p>
<table><tr style="vertical-align: top;">
<td><code>pred</code></td>
<td>
<p>A list of matrices (<code class="reqn">m, q</code>) with the Y predicted values for the new X-data</p>
</td>
</tr></table>
<h3>Note</h3>

<p>KRR is close to the particular SVMR setting the <code class="reqn">epsilon</code> coefficient to zero (no marges excluding observations). The difference is that a L2-norm optimization is done, instead L1 in SVM.   
</p>
<p>The second example concerns the fitting of the function sinc(x) described in Rosipal &amp; Trejo 2001 p. 105-106
</p>


<h3>References</h3>

<p>Bennett, K.P., Embrechts, M.J., 2003. An optimization perspective on kernel partial least squares regression, in: Advances in Learning Theory: Methods, Models and Applications, NATO Science Series III: Computer &amp; Systems Sciences. IOS Press Amsterdam, pp. 227-250.
</p>
<p>Cawley, G.C., Talbot, N.L.C., 2002. Reduced Rank Kernel Ridge Regression. Neural Processing Letters 16, 293-302. https://doi.org/10.1023/A:1021798002258
</p>
<p>Krell, M.M., 2018. Generalizing, Decoding, and Optimizing Support Vector Machine Classification. arXiv:1801.04929.
</p>
<p>Saunders, C., Gammerman, A., Vovk, V., 1998. Ridge Regression Learning Algorithm in Dual Variables, in: In Proceedings of the 15th International Conference on Machine Learning. Morgan Kaufmann, pp. 515-521.
</p>
<p>Suykens, J.A.K., Lukas, L., Vandewalle, J., 2000. Sparse approximation using least squares support vector machines. 2000 IEEE International Symposium on Circuits and Systems. Emerging Technologies for the 21st Century. Proceedings (IEEE Cat No.00CH36353). https://doi.org/10.1109/ISCAS.2000.856439
</p>
<p>Welling, M., n.d. Kernel ridge regression. Department of Computer Science, University of Toronto, Toronto, Canada. https://www.ics.uci.edu/~welling/classnotes/papers_class/Kernel-Ridge.pdf
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## EXAMPLE 1

n &lt;- 6 ; p &lt;- 4
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(y1 = ytrain, y2 = 100 * ytrain)
m &lt;- 3
Xtest &lt;- Xtrain[1:m, , drop = FALSE] 
Ytest &lt;- Ytrain[1:m, , drop = FALSE] ; ytest &lt;- Ytest[1:m, 1]

lb &lt;- 2
fm &lt;- krr(Xtrain, Ytrain, lb = lb, kern = "krbf", gamma = .8)
coef(fm)
coef(fm, lb = .6)
predict(fm, Xtest)
predict(fm, Xtest, lb = c(0.1, .6))

pred &lt;- predict(fm, Xtest)$pred
msep(pred, Ytest)

lb &lt;- 2
fm &lt;- krr(Xtrain, Ytrain, lb = lb, kern = "kpol", degree = 2, coef0 = 10)
predict(fm, Xtest)

## EXAMPLE 2

x &lt;- seq(-10, 10, by = .2)
x[x == 0] &lt;- 1e-5
n &lt;- length(x)
zy &lt;- sin(abs(x)) / abs(x)
y &lt;- zy + rnorm(n, 0, .2)
plot(x, y, type = "p")
lines(x, zy, lty = 2)
X &lt;- matrix(x, ncol = 1)

fm &lt;- krr(X, y, lb = .1, gamma = .5)
pred &lt;- predict(fm, X)$pred
plot(X, y, type = "p")
lines(X, zy, lty = 2)
lines(X, pred, col = "red")

</code></pre>


</div>