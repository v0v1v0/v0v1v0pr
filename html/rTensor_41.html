<div class="container">

<table style="width: 100%;"><tr>
<td>mpca</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Multilinear Principal Components Analysis</h2>

<h3>Description</h3>

<p>This is basically the Tucker decomposition of a K-Tensor, <code>tucker</code>, with one of the modes uncompressed. If K = 3, then this is also known as the Generalized Low Rank Approximation of Matrices (GLRAM). This implementation assumes that the last mode is the measurement mode and hence uncompressed. This is an iterative algorithm, with two possible stopping conditions: either relative error in Frobenius norm has gotten below <code>tol</code>, or the <code>max_iter</code> number of iterations has been reached. For more details on the MPCA of tensors, consult Lu et al. (2008).
</p>


<h3>Usage</h3>

<pre><code class="language-R">mpca(tnsr, ranks = NULL, max_iter = 25, tol = 1e-05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>tnsr</code></td>
<td>
<p>Tensor with K modes</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ranks</code></td>
<td>
<p>a vector of the compressed modes of the output core Tensor, this has length K-1</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_iter</code></td>
<td>
<p>maximum number of iterations if error stays above <code>tol</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>relative Frobenius norm error tolerance</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Uses the Alternating Least Squares (ALS) estimation procedure. A progress bar is included to help monitor operations on large tensors.
</p>


<h3>Value</h3>

<p>a list containing the following:</p>

<dl>
<dt><code>Z_ext</code></dt>
<dd>
<p>the extended core tensor, with the first K-1 modes given by <code>ranks</code></p>
</dd>
<dt><code>U</code></dt>
<dd>
<p>a list of K-1 orthgonal factor matrices - one for each compressed mode, with the number of columns of the matrices given by <code>ranks</code></p>
</dd>
<dt><code>conv</code></dt>
<dd>
<p>whether or not <code>resid</code> &lt; <code>tol</code> by the last iteration</p>
</dd>
<dt><code>est</code></dt>
<dd>
<p>estimate of <code>tnsr</code> after compression</p>
</dd>
<dt><code>norm_percent</code></dt>
<dd>
<p>the percent of Frobenius norm explained by the approximation</p>
</dd>
<dt><code>fnorm_resid</code></dt>
<dd>
<p>the Frobenius norm of the error <code>fnorm(est-tnsr)</code></p>
</dd>
<dt><code>all_resids</code></dt>
<dd>
<p>vector containing the Frobenius norm of error for all the iterations</p>
</dd>
</dl>
<h3>Note</h3>

<p>The length of <code>ranks</code> must match <code>tnsr@num_modes-1</code>.
</p>


<h3>References</h3>

<p>H. Lu, K. Plataniotis, A. Venetsanopoulos, "Mpca: Multilinear principal component analysis of tensor objects". IEEE Trans. Neural networks, 2008.
</p>


<h3>See Also</h3>

<p><code>tucker</code>, <code>hosvd</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">### How to retrieve faces_tnsr from figshare
# faces_tnsr &lt;- load_orl()
# subject &lt;- faces_tnsr[,,21,]
dummy_faces_tnsr &lt;- rand_tensor(c(92,112,40,10))
subject &lt;- dummy_faces_tnsr[,,21,]
mpcaD &lt;- mpca(subject, ranks=c(10, 10))
mpcaD$conv
mpcaD$norm_percent
plot(mpcaD$all_resids)
</code></pre>


</div>