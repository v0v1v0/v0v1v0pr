<div class="container">

<table style="width: 100%;"><tr>
<td>AIC.seqModel</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Information criteria for a sequence of regression models</h2>

<h3>Description</h3>

<p>Compute the Akaike or Bayes information criterion for for a sequence of
regression models, such as submodels along a robust least angle regression
sequence, or sparse least trimmed squares regression models for a grid of
values for the penalty parameter.
</p>


<h3>Usage</h3>

<pre><code class="language-R">## S3 method for class 'seqModel'
AIC(object, ..., k = 2)

## S3 method for class 'sparseLTS'
AIC(object, ..., fit = c("reweighted", "raw", "both"), k = 2)

## S3 method for class 'seqModel'
BIC(object, ...)

## S3 method for class 'sparseLTS'
BIC(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>the model fit for which to compute the information criterion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>for the <code>BIC</code> method, additional arguments to be passed
down to the <code>AIC</code> method.  For the <code>AIC</code> method, additional
arguments are currently ignored.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>a numeric value giving the penalty per parameter to be used.  The
default is to use <code class="reqn">2</code> as in the classical definition of the AIC.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fit</code></td>
<td>
<p>a character string specifying for which fit to compute the
information criterion.  Possible values are <code>"reweighted"</code> (the
default) for the information criterion of the reweighted fit, <code>"raw"</code>
for the information criterion of the raw fit, or <code>"both"</code> for the
information criteria of both fits.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The information criteria are computed as
<code class="reqn">n (\log(2 \pi) + 1 + \log(\hat{\sigma}^2)) + df k</code>,
where <code class="reqn">n</code> denotes the number of observations, <code class="reqn">\hat{\sigma}</code>
is the robust residual scale estimate, <code class="reqn">df</code> is the number of nonzero
coefficient estimates, and <code class="reqn">k</code> is penalty per parameter.  The usual
definition of the AIC uses <code class="reqn">k = 2</code>, whereas the BIC uses
<code class="reqn">k = \log(n)</code>.  Consequently, the former is used as the
default penalty of the <code>AIC</code> method, whereas the <code>BIC</code> method calls
the <code>AIC</code> method with the latter penalty.
</p>


<h3>Value</h3>

<p>A numeric vector or matrix giving the information criteria for the requested
model fits.
</p>


<h3>Note</h3>

<p>Computing information criteria for several objects supplied via the
<code>...</code> argument (as for the default methods of <code>AIC</code>
and <code>BIC</code>) is currently not implemented.
</p>


<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>References</h3>

<p>Akaike, H. (1970) Statistical predictor identification. <em>Annals of the
Institute of Statistical Mathematics</em>, <b>22</b>(2), 203–217.
</p>
<p>Schwarz, G. (1978) Estimating the dimension of a model. <em>The Annals of
Statistics</em>, <b>6</b>(2), 461–464.
</p>


<h3>See Also</h3>

<p><code>AIC</code>, <code>rlars</code>,
<code>sparseLTS</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## generate data
# example is not high-dimensional to keep computation time low
library("mvtnorm")
set.seed(1234)  # for reproducibility
n &lt;- 100  # number of observations
p &lt;- 25   # number of variables
beta &lt;- rep.int(c(1, 0), c(5, p-5))  # coefficients
sigma &lt;- 0.5      # controls signal-to-noise ratio
epsilon &lt;- 0.1    # contamination level
Sigma &lt;- 0.5^t(sapply(1:p, function(i, j) abs(i-j), 1:p))
x &lt;- rmvnorm(n, sigma=Sigma)    # predictor matrix
e &lt;- rnorm(n)                   # error terms
i &lt;- 1:ceiling(epsilon*n)       # observations to be contaminated
e[i] &lt;- e[i] + 5                # vertical outliers
y &lt;- c(x %*% beta + sigma * e)  # response
x[i,] &lt;- x[i,] + 5              # bad leverage points


## robust LARS
# fit model
fitRlars &lt;- rlars(x, y, sMax = 10)
# compute AIC and BIC
AIC(fitRlars)
BIC(fitRlars)


## fit sparse LTS model over a grid of values for lambda
frac &lt;- seq(0.2, 0.05, by = -0.05)
fitSparseLTS &lt;- sparseLTS(x, y, lambda = frac, mode = "fraction")
# compute AIC and BIC
AIC(fitSparseLTS)
BIC(fitSparseLTS)
</code></pre>


</div>