<div class="container">

<table style="width: 100%;"><tr>
<td>sqrt_lasso</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Square-root Lasso regression</h2>

<h3>Description</h3>

<p>Fits a linear model to potentially high-dimensional data using the
square-root Lasso, also known as the scaled Lasso. The Lasso path is computed
using the <span class="pkg">glmnet</span> package.
</p>


<h3>Usage</h3>

<pre><code class="language-R">sqrt_lasso(x, y, lam0 = NULL, exclude = integer(0), output_all = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Input matrix of dimension nobs by nvars; each row is an observation
vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Response variable; shoud be a numeric vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lam0</code></td>
<td>
<p>Tuning parameter for the square-root / scaled Lasso. If left
blank (recommended) this is chosen using the method of Sun &amp; Zhang (2013)
implemented in the <span class="pkg">scalreg</span> package.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>exclude</code></td>
<td>
<p>Indices of variables to be excluded from the model; default is
none.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>output_all</code></td>
<td>
<p>In addition to the vector of coefficients, if <code>TRUE</code>,
also outputs the intercept, an estimate of the noise standard deviation,
and the output of <code>glmnet</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments to be passed to <code>glmnet</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>First the Lasso path is computed using <code>glmnet</code> from
<span class="pkg">glmnet</span>. Next the particular point on the path corresponding to the
square-root Lasso solution is found. As the path is only computed on a grid
of points, the square-root Lasso solution is approximate.
</p>


<h3>Value</h3>

<p>Either an estimated vector of regression coefficients with nvars
components or, if <code>output_all</code> is <code>true</code>, a list with components
</p>

<dl>
<dt><code>beta</code></dt>
<dd>
<p>the vector of regression coefficents</p>
</dd>
<dt><code>a0</code></dt>
<dd>
<p>an intercept term</p>
</dd>
<dt><code>sigma_hat</code></dt>
<dd>
<p>an estimate of
the noise standard deviation; this is calculated as square-root of the
average residual sums of squares</p>
</dd>
<dt><code>glmnet_obj</code></dt>
<dd>
<p>the fitted <code>glmnet</code> object, an S3 class â€œ<code>glmnet</code>"</p>
</dd>
<dt><code>lamda_index</code></dt>
<dd>
<p>the index of the lambda vector in the <code>glmnet</code> object
corresponding to the square-root Lasso solution</p>
</dd>
</dl>
<h3>References</h3>

<p>A. Belloni, V. Chernozhukov, and L. Wang. (2011)
<em>Square-root lasso: pivotal recovery of sparse signals via conic
programming. Biometrika, 98(4):791-806.</em>
</p>
<p>T. Sun and C.-H. Zhang. (2012) <em>Scaled sparse linear regression. Biometrika,
99(4):879-898.</em>
</p>
<p>T. Sun and C.-H. Zhang. (2013) <em>Sparse matrix inversion with scaled
lasso. The Journal of Machine Learning Research, 14(1):3385-3418.</em>
</p>


<h3>See Also</h3>

<p><code>glmnet</code> and <code>scalreg</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">x &lt;- matrix(rnorm(100*250), 100, 250)
y &lt;- x[, 1] + x[, 2] + rnorm(100)
out &lt;- sqrt_lasso(x, y)
</code></pre>


</div>