<div class="container">

<table style="width: 100%;"><tr>
<td>Empirical entropy</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Empirical entropy
</h2>

<h3>Description</h3>

<p>Empirical entropy.
</p>


<h3>Usage</h3>

<pre><code class="language-R">empirical.entropy(x, k = NULL, pretty = FALSE) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>A numerical vector with continuous values.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>

<p>If you want to cut the data into a specific range plug it here, otherwise this decide based upon the Freedman-Diaconis' rule. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pretty</code></td>
<td>

<p>Should the breaks be equally space upon the range of x? If yes, let this FALSE. If this is TRUE, the breaks are decided using the 
base command pretty.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The function computes the empirical entropy.
</p>


<h3>Value</h3>

<p>The estimated empirical entropy.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p><a href="https://en.wikipedia.org/wiki/Entropy_estimation">https://en.wikipedia.org/wiki/Entropy_estimation</a>
</p>
<p><a href="https://en.wikipedia.org/wiki/Histogram">https://en.wikipedia.org/wiki/Histogram</a>
</p>
<p>Freedman David and Diaconis P. (1981). On the histogram as a density estimator: L2 theory. 
Zeitschrift fur Wahrscheinlichkeitstheorie und Verwandte Gebiete. 57(4): 453-476. 
</p>


<h3>See Also</h3>

<p><code> Quantile, pretty 
</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">x &lt;- rnorm(100)
empirical.entropy(x)
empirical.entropy(x, pretty = TRUE)
</code></pre>


</div>