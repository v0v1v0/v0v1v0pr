<div class="container">

<table style="width: 100%;"><tr>
<td>rjmcmcpost</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Perform Reversible-Jump MCMC Post-Processing</h2>

<h3>Description</h3>

<p>Performs Bayesian multimodel inference, estimating Bayes factors and 
posterior model probabilities for N candidate models. Using the 'universal 
parameter' restriction in Barker &amp; Link (2013), RJMCMC is treated as a Gibbs 
sampling problem, where the algorithm alternates between updating the model 
and the model specific parameters. Transformation Jacobians are computed 
using automatic differentiation so do not need to be specified.
</p>


<h3>Usage</h3>

<pre><code class="language-R">rjmcmcpost(post.draw, g, ginv, likelihood, param.prior, model.prior,
  chainlength = 10000, TM.thin = chainlength/10, save.all = FALSE,
  progress = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>post.draw</code></td>
<td>
<p>A list of N functions that randomly draw from the posterior 
distribution under each model. Generally these functions sample from the 
output of a model fitted using MCMC.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>g</code></td>
<td>
<p>A list of N functions specifying the bijections from the universal 
parameter <code>psi</code> to each model-specific parameter set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ginv</code></td>
<td>
<p>A list of N functions specifying the bijections from each 
model-specific parameter set to <code>psi</code>. These are the inverse 
transformations of <code>g</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>likelihood</code></td>
<td>
<p>A list of N functions specifying the log-likelihood 
functions for the data under each model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>param.prior</code></td>
<td>
<p>A list of N functions specifying the log-prior 
distributions for each model-specific parameter vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model.prior</code></td>
<td>
<p>A numeric vector of the prior model probabilities. Note 
that this argument is not required to sum to one as it is automatically 
normalised.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>chainlength</code></td>
<td>
<p>How many iterations to run the Markov chain for.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>TM.thin</code></td>
<td>
<p>How regularly to calculate transition matrices as the chain 
progresses.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>save.all</code></td>
<td>
<p>A logical determining whether to save the value of the 
universal parameter at each iteration, as well as the corresponding 
likelihoods, priors and posteriors. If <code>TRUE</code>, the output object 
occupies significantly more memory.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>progress</code></td>
<td>
<p>A logical determining whether a progress bar is drawn.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Returns an object of class <code>rj</code> (see <code>rjmethods</code>). 
If <code>save.all=TRUE</code>, the output has named elements <code>result</code>, 
<code>densities</code>, <code>psidraws</code>, <code>progress</code> and <code>meta</code>. If 
<code>save.all=FALSE</code>, the <code>densities</code> and <code>psidraws</code> elements 
are omitted.
</p>
<p><code>result</code> contains useful point estimates, <code>progress</code> contains
snapshots of these estimates over time, and <code>meta</code> contains
information about the function call.
</p>


<h3>References</h3>

<p>Barker, R. J. and Link, W. A. (2013) Bayesian multimodel 
inference by RJMCMC: A Gibbs sampling approach. <em>The American 
Statistician, 67(3), 150-156</em>.
</p>


<h3>See Also</h3>

<p><code>adiff</code> <code>getsampler</code> 
<code>defaultpost</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Comparing two binomial models -- see Barker &amp; Link (2013) for further details.

y=c(8,16); sumy=sum(y)
n=c(20,30); sumn=sum(n)

L1=function(p){if((all(p&gt;=0))&amp;&amp;(all(p&lt;=1))) sum(dbinom(y,n,p,log=TRUE)) else -Inf}
L2=function(p){if((p[1]&gt;=0)&amp;&amp;(p[1]&lt;=1)) sum(dbinom(y,n,p[1],log=TRUE)) else -Inf}

g1=function(psi){p=psi}
g2=function(psi){w=n[1]/sum(n); p=c(w*psi[1]+(1-w)*psi[2],psi[2])}
ginv1=function(p){p}
ginv2=function(p){c(sum(n)/n[1]*p[1]-n[2]/n[1]*p[2],p[2])}

p.prior1=function(p){sum(dbeta(p,1,1,log=TRUE))}
p.prior2=function(p){dbeta(p[1],1,1,log=TRUE)+dbeta(p[2],17,15,log=TRUE)}

draw1=function(){rbeta(2,y+1,n-y+1)}
draw2=function(){c(rbeta(1,sumy+1,sumn-sumy+1),rbeta(1,17,15))}

out=rjmcmcpost(post.draw=list(draw1,draw2), g=list(g1,g2), ginv=list(ginv1,ginv2),
               likelihood=list(L1,L2), param.prior=list(p.prior1,p.prior2),
               model.prior=c(0.5,0.5), chainlength=1500)

</code></pre>


</div>