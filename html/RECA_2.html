<div class="container">

<table style="width: 100%;"><tr>
<td>rca</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Relevant Component Analysis</h2>

<h3>Description</h3>

<p><code>rca</code> performs relevant component analysis (RCA) for the given data.
It takes a data set and a set of positive constraints as arguments
and returns a linear transformation of the data space into better
representation, alternatively, a Mahalanobis metric over the data space.
</p>
<p>The new representation is known to be optimal in an information
theoretic sense under a constraint of keeping equivalent data
points close to each other.
</p>


<h3>Usage</h3>

<pre><code class="language-R">rca(x, chunks, useD = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p><code>n * d</code> matrix or data frame of original data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>chunks</code></td>
<td>
<p>a vector of size <code>N</code> describing the chunklets:
<code>-1</code> in the <code>i</code>-th place says that point <code>i</code> does not
belong to any chunklet; integer <code>j</code> in place <code>i</code> says
that point <code>i</code> belongs to chunklet <code>j</code>;
The chunklets indexes should be <code>1:number-of-chunklets</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>useD</code></td>
<td>
<p>optional. When not given, RCA is done in the
original dimension and <code>B</code> is full rank. When <code>useD</code> is given,
RCA is preceded by constraints based LDA which reduces
the dimension to <code>useD</code>. <code>B</code> in this case is of rank <code>useD</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The three returned objects are just different forms of the same output.
If one is interested in a Mahalanobis metric over the original data space,
the first argument is all she/he needs. If a transformation into another
space (where one can use the Euclidean metric) is preferred, the second
returned argument is sufficient. Using <code>A</code> and <code>B</code> are equivalent
in the following sense:
</p>
<p>if <code>y1 = A * x1</code>, <code>y2 = A * y2</code>  then
</p>
<p><code>(x2 - x1)^T * B * (x2 - x1) = (y2 - y1)^T * (y2 - y1)</code>
</p>


<h3>Value</h3>

<p>A list of the RCA results:
</p>

<ul>
<li> <p><code>B</code>: The RCA suggested Mahalanobis matrix.
Distances between data points <code>x1</code>, <code>x2</code> should be
computed by <code>(x2 - x1)^T * B * (x2 - x1)</code>
</p>
</li>
<li> <p><code>RCA</code>: The RCA suggested transformation of the data.
The data should be transformed by <code>RCA * data</code>
</p>
</li>
<li> <p><code>newX</code>: The data after the RCA transformation.
<code>newX = data * RCA</code>
</p>
</li>
</ul>
<h3>Note</h3>

<p>Note that any different sets of instances (chunklets),
e.g. <code>{1, 3, 7}</code> and <code>{4, 6}</code>, might belong to the
same class and might belong to different classes.
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>References</h3>

<p>Aharon Bar-Hillel, Tomer Hertz, Noam Shental, and Daphna Weinshall (2003).
Learning Distance Functions using Equivalence Relations.
<em>Proceedings of 20th International Conference on
Machine Learning (ICML2003)</em>
</p>


<h3>Examples</h3>

<pre><code class="language-R">library("MASS") # generate synthetic multivariate normal data
set.seed(42)
k &lt;- 100L # sample size of each class
n &lt;- 3L # specify how many classes
N &lt;- k * n # total sample size
x1 &lt;- mvrnorm(k, mu = c(-16, 8), matrix(c(15, 1, 2, 10), ncol = 2))
x2 &lt;- mvrnorm(k, mu = c(0, 0), matrix(c(15, 1, 2, 10), ncol = 2))
x3 &lt;- mvrnorm(k, mu = c(16, -8), matrix(c(15, 1, 2, 10), ncol = 2))
x &lt;- as.data.frame(rbind(x1, x2, x3)) # predictors
y &lt;- gl(n, k) # response

# fully labeled data set with 3 classes
# need to use a line in 2D to classify
plot(x[, 1L], x[, 2L],
  bg = c("#E41A1C", "#377EB8", "#4DAF4A")[y],
  pch = rep(c(22, 21, 25), each = k)
)
abline(a = -10, b = 1, lty = 2)
abline(a = 12, b = 1, lty = 2)

# generate synthetic chunklets
chunks &lt;- vector("list", 300)
for (i in 1:100) chunks[[i]] &lt;- sample(1L:100L, 10L)
for (i in 101:200) chunks[[i]] &lt;- sample(101L:200L, 10L)
for (i in 201:300) chunks[[i]] &lt;- sample(201L:300L, 10L)

chks &lt;- x[unlist(chunks), ]

# make "chunklet" vector to feed the chunks argument
chunksvec &lt;- rep(-1L, nrow(x))
for (i in 1L:length(chunks)) {
  for (j in 1L:length(chunks[[i]])) {
    chunksvec[chunks[[i]][j]] &lt;- i
  }
}

# relevant component analysis
rcs &lt;- rca(x, chunksvec)

# learned transformation of the data
rcs$RCA

# learned Mahalanobis distance metric
rcs$B

# whitening transformation applied to the chunklets
chkTransformed &lt;- as.matrix(chks) %*% rcs$RCA

# original data after applying RCA transformation
# easier to classify - using only horizontal lines
xnew &lt;- rcs$newX
plot(xnew[, 1L], xnew[, 2L],
  bg = c("#E41A1C", "#377EB8", "#4DAF4A")[gl(n, k)],
  pch = c(rep(22, k), rep(21, k), rep(25, k))
)
abline(a = -15, b = 0, lty = 2)
abline(a = 16, b = 0, lty = 2)
</code></pre>


</div>