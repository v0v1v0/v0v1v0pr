<div class="container">

<table style="width: 100%;"><tr>
<td>randomMachines</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Random Machines</h2>

<h3>Description</h3>

<p>Random Machines is an ensemble model which uses the combination of different kernel functions to improve the diversity in the bagging approach, improving the predictions in general. Random Machines was developed for classification and regression problems by bagging multiple kernel functions in support vector models.
</p>
<p>Random Machines uses SVMs (Cortes and Vapnik, 1995) as base learners in the bagging procedure with a random sample of kernel functions to build them.
</p>
<p>Let a training sample given by <code class="reqn">(\boldsymbol{x_{i}},y_i)</code> with <code class="reqn">i=1,\dots, n</code> observations, where <code class="reqn">\boldsymbol{x_{i}}</code> is the vector of independent variables and <code class="reqn">y_{i}</code> the dependent one. The kernel bagging method initializes by training of the <code class="reqn">r</code> single learner, where <code class="reqn">r=1,\dots,R</code> and <code class="reqn">R</code> is the total number of different kernel functions that could be used in support vector models. In this implementation the default value is <code class="reqn">R=4</code> (gaussian, polynomial, laplacian and linear). See more details below.
</p>
<p>Each single learner is internally validated and the weights <code class="reqn">\lambda_{r}</code> are calculated proportionally to the strength from the single predictive performance.
</p>
<p>Afterwards, <code class="reqn">B</code> bootstrap samples are sampled from the training set. A support vector machine model <code class="reqn">g_{b}</code> is trained for each bootstrap sample, <code class="reqn">b=i,\dots,B</code> and the kernel function that will be used for <code class="reqn">g_{b}</code> will be determined by a random choice with probability <code class="reqn">\lambda_{r}</code>. The final weight <code class="reqn">w_b</code> in the bagging procedure is calculated by out-of-bag samples.
</p>
<p>The final model <code class="reqn">G(\boldsymbol{x}_i)</code> for a new <code class="reqn">\boldsymbol{x}_i</code> is given by,
</p>
<p>The weights <code class="reqn">\lambda_{r}</code> and <code class="reqn">w_b</code> are different calculated for each task (classification, probabilistic classification and regression). See more details in the references.
</p>

<ul>
<li>
<p> For a binary classification problem <code class="reqn">\mathbin{{ G(\boldsymbol{x_{i}})= \text{sgn} \left( \sum_{b=1}^{B}w_{b}g_{b}(\boldsymbol{x_{i}})\right)}}</code>, where <code class="reqn">g_b</code> are single binary classification outputs;
</p>
</li>
<li>
<p> For a probabilistic binary classification problem <code class="reqn">\mathbin{{ G(\boldsymbol{x_{i}})= \sum_{b=1}^{B}w_{b}g_{b}(\boldsymbol{x_{i}})}}</code>, where <code class="reqn">g_b</code> are single probabilistic classification outputs;
</p>
</li>
<li>
<p> For a regression problem <code class="reqn">G(\boldsymbol{x_{i}})= \sum_{b=1}^{B}w_{b}g_{b}(\boldsymbol{x_{i}})</code>, , where <code class="reqn">g_b</code> are single regression outputs.
</p>
</li>
</ul>
<h3>Usage</h3>

<pre><code class="language-R">randomMachines(
     formula,
     train,validation,
     B = 25, cost = 1,
     automatic_tuning = FALSE,
     gamma_rbf = 1,
     gamma_lap = 1,
     degree = 2,
     poly_scale = 1,
     offset = 0,
     gamma_cau = 1,
     d_t = 2,
     kernels = c("rbfdot", "polydot", "laplacedot", "vanilladot"),
     prob_model = TRUE,
     loss_function = RMSE,
     epsilon = 0.1,
     beta = 2
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>

<p>an object of class <code>formula</code>: it should contain a symbolic description of the model to be fitted, indicating the dependent variable and all predictors that should be included.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>train</code></td>
<td>

<p>the training data <code class="reqn">\left\{\left( \mathbf{x}_{i},y_{i} \right)\right\}_{i=1}^{n}</code> used to train the model.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>validation</code></td>
<td>

<p>the validation data <code class="reqn">\left\{\left( \mathbf{x}_{i},y_{i}\right)  \right\}_{i=1}^{V}</code> used to calculate probabilities <code class="reqn">\lambda_{r}</code>. If <code>validation = NULL</code>,the validation set is going be selected as 0.25 partition from the training data, and the remaining partition is selected as the new training sample.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>B</code></td>
<td>

<p>number of bootstrap samples. The default value is <code>B=25</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cost</code></td>
<td>

<p>the <code class="reqn">C</code>-constant term of the regularization on soft margins at support vector models. The default value is <code>cost=1</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>automatic_tuning</code></td>
<td>

<p>boolean to define if the kernel hyperparameters will be selected using the <code>sigest</code> from the <code>ksvm</code> function. The default value is <code>FALSE</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma_rbf</code></td>
<td>

<p>the hyperparameter <code class="reqn">\gamma_{g}</code> used in the RBF kernel. The default value is <code>gamma_rbf=1</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma_lap</code></td>
<td>

<p>the hyperparameter <code class="reqn">\gamma_{l}</code> used in the Laplacian kernel. The default value is <code>gamma_lap=1</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>degree</code></td>
<td>

<p>the degree used in the Polynomial kernel. The default value is <code>degree=2</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>poly_scale</code></td>
<td>

<p>the scale parameter from the Polynomial kernel. The default value is <code>poly_scale=1</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>offset</code></td>
<td>

<p>the offset parameter from the Polynomial kernel. The default value is <code>offset=0</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma_cau</code></td>
<td>

<p>the hyperparameter <code class="reqn">\gamma_{c}</code> used in the Cauchy kernel. The default value is <code>gamma_cau=1</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>d_t</code></td>
<td>

<p>the <code class="reqn">d_{t}</code>-norm from the t-Student kernel. The default value is <code>d_t=2</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernels</code></td>
<td>

<p>a vector with the name of kernel functions that will be used in the Random Machines model. The default include the kernel functions: <code>c("rbfdot", "polydot", "laplacedot", "vanilladot").</code> The other kernel functions as <code>"cauchydot"</code> and <code>"tdot"</code> are exclusive to the binary classification setting.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prob_model</code></td>
<td>

<p>a boolean to define if the algorithm will be using a probabilistic approach to the define the predictions (default = <code>TRUE</code>).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss_function</code></td>
<td>

<p>Define which loss function is going to be used in the regression approach. The default is the <code>RMSE</code> function but others can be added.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>

<p>The epsilon in the loss function used from the SVR implementation. The default value is <code>epsilon=0.1</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>

<p>The correlation parameter <code class="reqn">\beta</code> which calibrates the penalisation of each kernel performance in regression tasks.  The default value is <code>beta=2</code>.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The Random Machines is an ensemble method which combines the bagging procedure proposed by Breiman (1996), using Support Vector Machine models as base learners jointly with a random selection of kernel functions that add diversity to the ensemble without harming its predictive performance. The kernel functions <code class="reqn">k(x,y)</code> are described by the functions below,
</p>

<ul>
<li> <p>Linear Kernel: <code class="reqn">k(x,y) = (x\cdot y)</code>
</p>
</li>
<li> <p>Polynomial Kernel: <code class="reqn">k(x,y) = \left(scale( x\cdot y) + offset\right)^{degree}</code>
</p>
</li>
<li> <p>Gaussian Kernel: <code class="reqn">k(x,y) = e^{-\gamma_{g}||x-y||^2}</code>
</p>
</li>
<li> <p>Laplacian Kernel:  <code class="reqn">k(x,y) = e^{-\gamma_{\ell}||x-y||}</code>
</p>
</li>
<li> <p>Cauchy Kernel: <code class="reqn">k(x,y) = \frac{1}{1 + \frac{||x-y||^{2}}{\gamma_{c}}}</code>
</p>
</li>
<li> <p>Student's t Kernel: <code class="reqn">k(x,y) = \frac{1}{1 + ||x-y||^{d_{t}}}</code>
</p>
</li>
</ul>
<h3>Value</h3>

 <p><code>randomMachines()</code> returns an object of <code>class</code> "rm_class" for classification tasks or "rm_reg" for if the target variable is a continuous numerical response. See <code>predict.rm_class</code> or <code>predict.rm_reg</code> for more details of how to obtain predictions from each model respectively.</p>


<h3>Author(s)</h3>

<p>Mateus Maia: <a href="mailto:mateusmaia11@gmail.com">mateusmaia11@gmail.com</a>,
Gabriel Felipe Ribeiro: <a href="mailto:brielribeiro08@gmail.com">brielribeiro08@gmail.com</a>,
Anderson Ara: <a href="mailto:ara@ufpr.br">ara@ufpr.br</a>
</p>


<h3>References</h3>

<p>Ara, Anderson, et al. "Regression random machines: An ensemble support vector regression model with free kernel choice." Expert Systems with Applications 202 (2022): 117107.
</p>
<p>Ara, Anderson, et al. "Random machines: A bagged-weighted support vector model with free kernel choice." Journal of Data Science 19.3 (2021): 409-428.
</p>
<p>Breiman, L. (1996). Bagging predictors. Machine learning, 24, 123-140.
</p>
<p>Cortes, C., and Vapnik, V. (1995). Support-vector networks. Machine learning, 20, 273-297.
</p>
<p>Maia, Mateus, Arthur R. Azevedo, and Anderson Ara. "Predictive comparison between random machines and random forests." Journal of Data Science 19.4 (2021): 593-614.
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(randomMachines)

# Simulation from a binary output context
sim_data &lt;- sim_class(n = 75)

## Setting the training and validation set
sim_new &lt;- sim_class(n = 75)

# Modelling Random Machines (probabilistic output)
rm_mod_prob &lt;- randomMachines(y~., train = sim_data)

## Modelling Random Machines (binary class output)
rm_mod_label &lt;- randomMachines(y~., train = sim_data,prob_model = FALSE)

## Predicting for new data
y_hat &lt;- predict(rm_mod_label,sim_new)
</code></pre>


</div>