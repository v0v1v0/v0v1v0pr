<div class="container">

<table style="width: 100%;"><tr>
<td>S4VM</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Safe Semi-supervised Support Vector Machine (S4VM)</h2>

<h3>Description</h3>

<p>R port of the MATLAB implementation of Li &amp; Zhou (2011) of the Safe Semi-supervised Support Vector Machine.
</p>


<h3>Usage</h3>

<pre><code class="language-R">S4VM(X, y, X_u = NULL, C1 = 100, C2 = 0.1, sample_time = 100,
  gamma = 0, x_center = FALSE, scale = FALSE, lambda_tradeoff = 3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>matrix; Design matrix for labeled data</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>factor or integer vector; Label vector</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X_u</code></td>
<td>
<p>matrix; Design matrix for unlabeled data</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>C1</code></td>
<td>
<p>double; Regularization parameter for labeled data</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>C2</code></td>
<td>
<p>double; Regularization parameter for unlabeled data</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sample_time</code></td>
<td>
<p>integer; Number of low-density separators that are generated</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>double; Width of RBF kernel</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda_tradeoff</code></td>
<td>
<p>numeric; Parameter that determines the amount of "risk" in obtaining a worse solution than the supervised solution, see Li &amp; Zhou (2011)</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The method randomly generates multiple low-density separators (controlled by the sample_time parameter) and merges their predictions by solving a linear programming problem meant to penalize the cost of decreasing the performance of the classifier, compared to the supervised SVM. S4VM is a bit of a misnomer, since it is a transductive method that only returns predicted labels for the unlabeled objects. The main difference in this implementation compared to the original implementation is the clustering of the low-density separators: in our implementation empty clusters are not dropped during the k-means procedure. In the paper by Li (2011) the features are first normalized to [0,1], which is not automatically done by this function. Note that the solution may not correspond to a linear classifier even if the linear kernel is used.
</p>


<h3>Value</h3>

<p>S4VM object with slots:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>predictions</code></td>
<td>
<p>Predictions on the unlabeled objects</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>labelings</code></td>
<td>
<p>Labelings for the different clusters</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Yu-Feng Li and Zhi-Hua Zhou. Towards Making Unlabeled Data Never Hurt. In: Proceedings of the 28th International Conference on Machine Learning (ICML'11), Bellevue, Washington, 2011.
</p>


<h3>See Also</h3>

<p>Other RSSL classifiers: 
<code>EMLeastSquaresClassifier</code>,
<code>EMLinearDiscriminantClassifier</code>,
<code>GRFClassifier</code>,
<code>ICLeastSquaresClassifier</code>,
<code>ICLinearDiscriminantClassifier</code>,
<code>KernelLeastSquaresClassifier</code>,
<code>LaplacianKernelLeastSquaresClassifier()</code>,
<code>LaplacianSVM</code>,
<code>LeastSquaresClassifier</code>,
<code>LinearDiscriminantClassifier</code>,
<code>LinearSVM</code>,
<code>LinearTSVM()</code>,
<code>LogisticLossClassifier</code>,
<code>LogisticRegression</code>,
<code>MCLinearDiscriminantClassifier</code>,
<code>MCNearestMeanClassifier</code>,
<code>MCPLDA</code>,
<code>MajorityClassClassifier</code>,
<code>NearestMeanClassifier</code>,
<code>QuadraticDiscriminantClassifier</code>,
<code>SVM</code>,
<code>SelfLearning</code>,
<code>TSVM</code>,
<code>USMLeastSquaresClassifier</code>,
<code>WellSVM</code>,
<code>svmlin()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(RSSL)
library(dplyr)
library(ggplot2)
library(tidyr)

set.seed(1)
df_orig &lt;- generateSlicedCookie(100,expected=TRUE)
df &lt;- df_orig %&gt;% add_missinglabels_mar(Class~.,0.95)
g_s &lt;- SVM(Class~.,df,C=1,scale=TRUE,x_center=TRUE)
g_s4 &lt;- S4VM(Class~.,df,C1=1,C2=0.1,lambda_tradeoff = 3,scale=TRUE,x_center=TRUE)

labs &lt;- g_s4@labelings[-c(1:5),]
colnames(labs) &lt;- paste("Class",seq_len(ncol(g_s4@labelings)),sep="-")

# Show the labelings that the algorithm is considering
df %&gt;%
  filter(is.na(Class)) %&gt;% 
  bind_cols(data.frame(labs,check.names = FALSE)) %&gt;% 
  select(-Class) %&gt;% 
  gather(Classifier,Label,-X1,-X2) %&gt;% 
  ggplot(aes(x=X1,y=X2,color=Label)) +
  geom_point() +
  facet_wrap(~Classifier,ncol=5)

# Plot the final labeling that was selected
# Note that this may not correspond to a linear classifier
# even if the linear kernel is used.
# The solution does not seem to make a lot of sense,
# but this is what the current implementation returns
df %&gt;% 
  filter(is.na(Class)) %&gt;% 
  mutate(prediction=g_s4@predictions) %&gt;% 
  ggplot(aes(x=X1,y=X2,color=prediction)) +
  geom_point() +
  stat_classifier(color="black", classifiers=list(g_s))
</code></pre>


</div>