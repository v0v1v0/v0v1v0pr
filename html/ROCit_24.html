<div class="container">

<table style="width: 100%;"><tr>
<td>measureit.default</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Performance Metrics of Binary Classifier</h2>

<h3>Description</h3>

<p>This function  computes various performance metrics
at different cutoff values.
</p>


<h3>Usage</h3>

<pre><code class="language-R">## Default S3 method:
measureit(
  score,
  class,
  negref = NULL,
  measure = c("ACC", "SENS"),
  step = FALSE,
  ... = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>score</code></td>
<td>
<p>An numeric array of diagnostic score.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>class</code></td>
<td>
<p>An array of equal length of score,
containing the class of the observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>negref</code></td>
<td>
<p>The reference value, same as the
<code>reference</code> in <code>convertclass</code>.
Depending on the class of <code>x</code>,
it can be numeric or character type. If specified, this value
is converted to 0 and other is converted to 1. If NULL, reference is
set alphabetically.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>measure</code></td>
<td>
<p>The performance metrics to be evaluated. See "Details"
for available options.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>step</code></td>
<td>
<p>Logical, default in <code>FALSE</code>.The algorithm used in
<code>measureit</code> first rank orders the
data and calculates TP, FP, TN, FN by treating all predicted
up to certain level as positive. If <code>step</code> is <code>TRUE</code>,
then these numbers are evaluated for all the observations,
regardless of tie in the data. If <code>step</code> is
<code>FALSE</code>, only one set of stats are retained for a single value of
<code>D</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p><code>NULL</code>. Used for S3 generic/method consistency.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Various performance metrics for binary classifier are
available that are cutoff specific. For a certain cutoff value, all the
observations having score equal or greater are predicted as
positive. Following metrics can be called for
via <code>measure</code> argument:
</p>

<ul>
<li>
<p><code>ACC:</code> Overall accuracy of  classification =
<code class="reqn">P(Y = \hat{Y})</code> = (TP + TN) / (TP + FP + TN + FN)
</p>
</li>
<li>
<p><code>MIS:</code> Misclassification rate = <code class="reqn">1 - ACC</code>
</p>
</li>
<li>
<p><code>SENS:</code> Sensitivity = <code class="reqn">P(\hat{Y} = 1|Y = 1) = TP / (TP + FN)</code>
</p>
</li>
<li>
<p><code>SPEC:</code> Specificity = <code class="reqn">P(\hat{Y} = 0|Y = 0) = TN / (TN + FP)</code>
</p>
</li>
<li>
<p><code>PREC:</code> Precision  = <code class="reqn">P(Y = 1| \hat{Y} = 1) = TP / (TP + FP)</code>
</p>
</li>
<li>
<p><code>REC:</code> Recall. Same as sensitivity.
</p>
</li>
<li>
<p><code>PPV:</code> Positive predictive value. Same as precision
</p>
</li>
<li>
<p><code>NPV:</code> Positive predictive value = <code class="reqn">P(Y = 0| \hat{Y} = 0) =
TN / (TN + FN)</code>
</p>
</li>
<li>
<p><code>TPR:</code> True positive rate. Same as sensitivity.
</p>
</li>
<li>
<p><code>FPR:</code> False positive rate. Same as <code class="reqn">1 - specificity</code>.
</p>
</li>
<li>
<p><code>TNR:</code> True negative rate. Same as specificity.
</p>
</li>
<li>
<p><code>FNR:</code> False negative rate = <code class="reqn">P(\hat{Y} = 0|Y = 1) =
FN / (FN +TP)</code>
</p>
</li>
<li>
<p><code>pDLR:</code> Positive diagnostic likelihood ratio = <code class="reqn">TPR / FPR</code>
</p>
</li>
<li>
<p><code>nDLR:</code> Negative diagnostic likelihood ratio = <code class="reqn">FNR / TNR</code>
</p>
</li>
<li>
<p><code>FSCR:</code> F-score, defined as <code class="reqn">2 * (PPV * TPR) / (PPV + TPR)</code>
</p>
</li>
</ul>
<p><em>Exact match</em> is required. If the values passed in the
<code>measure</code> argument do not match with the
available options, then ignored.
</p>


<h3>Value</h3>

<p>An object of class <code>"measureit"</code>. By default it contains the
followings:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>Cutoff</code></td>
<td>
<p>Cutoff at which metrics are evaluated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Depth</code></td>
<td>
<p>What portion of the observations fall on or above the cutoff.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>TP</code></td>
<td>
<p>Number of true positives, when the observations having
score equal or greater than cutoff are predicted positive.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>FP</code></td>
<td>
<p>Number of false positives, when the observations having
score equal or greater than cutoff are predicted positive.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>TN</code></td>
<td>
<p>Number of true negatives, when the observations having
score equal or greater than cutoff are predicted positive.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>FN</code></td>
<td>
<p>Number of false negatives, when the observations having
score equal or greater than cutoff are predicted positive.</p>
</td>
</tr>
</table>
<p>When other metrics are called via <code>measure</code>, those also appear
in the return in the order they are listed above.
</p>


<h3>Note</h3>

<p>The algorithm is designed for complete cases. If NA(s) found in
either <code>score</code> or <code>class</code>, then removed.
</p>
<p>Internally sorting is performed, with respect to the
<code>score</code>. In case of tie, sorting is done with respect to <code>class</code>.
</p>


<h3>Author(s)</h3>

<p>Riaz Khan, <a href="mailto:mdriazahmed.khan@jacks.sdstate.edu">mdriazahmed.khan@jacks.sdstate.edu</a>
</p>


<h3>See Also</h3>

<p><code>measureit.rocit</code>, <code>print.measureit</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">data("Diabetes")
logistic.model &lt;- glm(factor(dtest)~chol+age+bmi,
                      data = Diabetes,family = "binomial")
class &lt;- logistic.model$y
score &lt;- logistic.model$fitted.values
# -------------------------------------------------------------
measure &lt;- measureit(score = score, class = class,
                     measure = c("ACC", "SENS", "FSCR"))
names(measure)
plot(measure$ACC~measure$Cutoff, type = "l")
plot(measure$TP~measure$FP, type = "l")

</code></pre>


</div>