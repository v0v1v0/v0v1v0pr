<div class="container">

<table style="width: 100%;"><tr>
<td>train</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Training a Recommender Model</h2>

<h3>Description</h3>

<p>This method is a member function of class "<code>RecoSys</code>"
that trains a recommender model. It will read from a training data source and
create a model file at the specified location. The model file contains
necessary information for prediction.
</p>
<p>The common usage of this method is
</p>
<pre>r = Reco()
r$train(train_data, out_model = file.path(tempdir(), "model.txt"),
        opts = list())</pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>r</code></td>
<td>
<p>Object returned by <code>Reco</code>().</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>train_data</code></td>
<td>
<p>An object of class "DataSource" that describes the source
of training data, typically returned by function
<code>data_file()</code>, <code>data_memory()</code>,
or <code>data_matrix()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>out_model</code></td>
<td>
<p>Path to the model file that will be created.
If passing <code>NULL</code>, the model will be stored in-memory, and
model matrices can then be accessed under <code>r$model$matrices</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>opts</code></td>
<td>
<p>A number of parameters and options for the model training.
See section <strong>Parameters and Options</strong> for details.</p>
</td>
</tr>
</table>
<h3>Parameters and Options</h3>

<p>The <code>opts</code> argument is a list that can supply any of the following parameters:
</p>

<dl>
<dt><code>loss</code></dt>
<dd>
<p>Character string, the loss function. Default is "l2", see below for details.</p>
</dd>
<dt><code>dim</code></dt>
<dd>
<p>Integer, the number of latent factors. Default is 10.</p>
</dd>
<dt><code>costp_l1</code></dt>
<dd>
<p>Numeric, L1 regularization parameter for user factors. Default is 0.</p>
</dd>
<dt><code>costp_l2</code></dt>
<dd>
<p>Numeric, L2 regularization parameter for user factors. Default is 0.1.</p>
</dd>
<dt><code>costq_l1</code></dt>
<dd>
<p>Numeric, L1 regularization parameter for item factors. Default is 0.</p>
</dd>
<dt><code>costq_l2</code></dt>
<dd>
<p>Numeric, L2 regularization parameter for item factors. Default is 0.1.</p>
</dd>
<dt><code>lrate</code></dt>
<dd>
<p>Numeric, the learning rate, which can be thought
of as the step size in gradient descent. Default is 0.1.</p>
</dd>
<dt><code>niter</code></dt>
<dd>
<p>Integer, the number of iterations. Default is 20.</p>
</dd>
<dt><code>nthread</code></dt>
<dd>
<p>Integer, the number of threads for parallel
computing. Default is 1.</p>
</dd>
<dt><code>nbin</code></dt>
<dd>
<p>Integer, the number of bins. Must be greater than <code>nthread</code>.
Default is 20.</p>
</dd>
<dt><code>nmf</code></dt>
<dd>
<p>Logical, whether to perform non-negative matrix factorization.
Default is <code>FALSE</code>.</p>
</dd>
<dt><code>verbose</code></dt>
<dd>
<p>Logical, whether to show detailed information. Default is
<code>TRUE</code>.</p>
</dd>
</dl>
<p>The <code>loss</code> option may take the following values:
</p>
<p>For real-valued matrix factorization,
</p>

<dl>
<dt><code>"l2"</code></dt>
<dd>
<p>Squared error (L2-norm)</p>
</dd>
<dt><code>"l1"</code></dt>
<dd>
<p>Absolute error (L1-norm)</p>
</dd>
<dt><code>"kl"</code></dt>
<dd>
<p>Generalized KL-divergence</p>
</dd>
</dl>
<p>For binary matrix factorization,
</p>

<dl>
<dt><code>"log"</code></dt>
<dd>
<p>Logarithmic error</p>
</dd>
<dt><code>"squared_hinge"</code></dt>
<dd>
<p>Squared hinge loss</p>
</dd>
<dt><code>"hinge"</code></dt>
<dd>
<p>Hinge loss</p>
</dd>
</dl>
<p>For one-class matrix factorization,
</p>

<dl>
<dt><code>"row_log"</code></dt>
<dd>
<p>Row-oriented pair-wise logarithmic loss</p>
</dd>
<dt><code>"col_log"</code></dt>
<dd>
<p>Column-oriented pair-wise logarithmic loss</p>
</dd>
</dl>
<h3>Author(s)</h3>

<p>Yixuan Qiu &lt;<a href="https://statr.me">https://statr.me</a>&gt;
</p>


<h3>References</h3>

<p>W.-S. Chin, Y. Zhuang, Y.-C. Juan, and C.-J. Lin.
A Fast Parallel Stochastic Gradient Method for Matrix Factorization in Shared Memory Systems.
ACM TIST, 2015.
</p>
<p>W.-S. Chin, Y. Zhuang, Y.-C. Juan, and C.-J. Lin.
A Learning-rate Schedule for Stochastic Gradient Methods to Matrix Factorization.
PAKDD, 2015.
</p>
<p>W.-S. Chin, B.-W. Yuan, M.-Y. Yang, Y. Zhuang, Y.-C. Juan, and C.-J. Lin.
LIBMF: A Library for Parallel Matrix Factorization in Shared-memory Systems.
Technical report, 2015.
</p>


<h3>See Also</h3>

<p><code>$tune()</code>, <code>$output()</code>, <code>$predict()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Training model from a data file
train_set = system.file("dat", "smalltrain.txt", package = "recosystem")
train_data = data_file(train_set)
r = Reco()
set.seed(123) # This is a randomized algorithm
# The model will be saved to a file
r$train(train_data, out_model = file.path(tempdir(), "model.txt"),
        opts = list(dim = 20, costp_l2 = 0.01, costq_l2 = 0.01, nthread = 1)
)

## Training model from data in memory
train_df = read.table(train_set, sep = " ", header = FALSE)
train_data = data_memory(train_df[, 1], train_df[, 2], rating = train_df[, 3])
set.seed(123)
# The model will be stored in memory
r$train(train_data, out_model = NULL,
        opts = list(dim = 20, costp_l2 = 0.01, costq_l2 = 0.01, nthread = 1)
)

## Training model from data in a sparse matrix
if(require(Matrix))
{
    mat = Matrix::sparseMatrix(i = train_df[, 1], j = train_df[, 2], x = train_df[, 3],
                               repr = "T", index1 = FALSE)
    train_data = data_matrix(mat)
    r$train(train_data, out_model = NULL,
            opts = list(dim = 20, costp_l2 = 0.01, costq_l2 = 0.01, nthread = 1))
}

</code></pre>


</div>