<div class="container">

<table style="width: 100%;"><tr>
<td>evaluateAlgorithmResults</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Convenience Function to generate all result plots and calculate the benchmark score</h2>

<h3>Description</h3>

<p>Convenience Function to generate all result plots and calculate the benchmark score
</p>


<h3>Usage</h3>

<pre><code class="language-R">evaluateAlgorithmResults(
  workingDir = "",
  algoNames = NULL,
  subset = "all",
  evalFolder = "Evaluation",
  withDirect = TRUE,
  withMean = TRUE,
  outline = TRUE,
  errorParam = c("zzDevAbs_Ov", "AbsPercError_Ov", "AbsError_Ov"),
  cutoffZ = 5,
  cols = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>workingDir</code></td>
<td>
<p>(character) specifying the working directory: Plots will be stored in 'workingDir/evalFolder' and results will be used from 'workingDir/Results/algoName/biomarker'</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>algoNames</code></td>
<td>
<p>(character) vector specifying all algorithms that should be part of the evaluation</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>subset</code></td>
<td>
<p>(character, numeric, or data.frame) to specify for which subset the algorithm should be evaluated. 
character options:    'all' (default) for all test sets,
a distribution type: 'normal', 'skewed', 'heavilySkewed', 'shifted';
a biomarker: 'Hb', 'Ca', 'FT4', 'AST', 'LACT', 'GGT', 'TSH', 'IgE', 'CRP', 'LDH'; 
'Runtime' for runtime analysis subset;                             
numeric option: number of test sets per biomarker, e.g. 10;
data.frame: customized subset of table with test set specifications</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>evalFolder</code></td>
<td>
<p>(character) specifying the name of the ouptut directory, Plots will be stored in workingDir/evalFolder, default: 'Evaluation'</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>withDirect</code></td>
<td>
<p>(logical) indicating whether the direct method should be simulated for comparison (default:TRUE)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>withMean</code></td>
<td>
<p>(logical) indicating whether the mean should be plotted as well (default: TRUE)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>outline</code></td>
<td>
<p>(logical) indicating whether outliers should be drawn (TRUE, default), or not (FALSE)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>errorParam</code></td>
<td>
<p>(character) specifying for which error parameter the data frame should be generated, choose between absolute z-score deviation ("zzDevAbs_Ov"), 
absolute percentage error ("AbsPercError_Ov"), and absolute error ("AbsError_Ov")</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cutoffZ</code></td>
<td>
<p>(integer) specifying if and if so which cutoff for the absolute z-score deviation should be used to 
classify results as implausible and exclude them from the overall benchmark score (default: 5)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cols</code></td>
<td>
<p>(character) vector specifying the colors used for the different algorithms</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>additional arguments to be passed to the method, e.g. 
"truncNormal" (logical) vector specifying if a normal distribution truncated at zero shall be assumed, can be either TRUE/FALSE or a vector with TRUE/FALSE for each algorithm;
"colDirect" (character) specifying the color used for the direct method, default: "grey" 
"ylab" (character) specifying the label for the y-axis</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>(data frame) containing the computed benchmark results
</p>


<h3>Author(s)</h3>

<p>Tatjana Ammer <a href="mailto:tatjana.ammer@roche.com">tatjana.ammer@roche.com</a>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## Not run: 
# Ensure that 'generateBiomarkerTestSets()' and 'evaluateBiomarkerTestSets() is called 
# with the same workingDir and for all mentioned algorithms before calling this function.

# first example, evaluation for several algorithms 
benchmarkScore &lt;- evaluateAlgorithmResults(workingDir=tempdir(), 
			algoNames=c("Hoffmann", "TML", "kosmic", "TMC", "refineR"))
# The function will create several plots saved in workingDir/Evaluation.

# second example, evaluation for only one algorithm and a defined subset
benchmarkScore &lt;- evaluateAlgorithmResults(workingDir = tempdir(), 
			algoNames = "refineR", subset = 'Ca')

# third example, saving the results in a different folder, and setting a different cutoff
# for the absolute z-score deviation
benchmarkScore &lt;- evaluateAlgorithmResults(workingDir = tempdir(), algoNames = "refineR", 
		subset = 'Ca', cutoffZ = 4, evalFolder = "Eval_Test")

## End(Not run)
 

</code></pre>


</div>