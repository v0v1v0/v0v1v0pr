<div class="container">

<table style="width: 100%;"><tr>
<td>RMTL-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>RMTL: Regularized Multi-Task Learning</h2>

<h3>Description</h3>

<p>This package provides an efficient implementation of regularized
multi-task learning (MTL) comprising 10 algorithms applicable for
regression, classification, joint feature selection, task clustering,
low-rank learning, sparse learning and network incorporation. All
algorithms are implemented based on the accelerated gradient descent
method and feature a complexity of O(1/k^2). Parallel computing is allowed to improve the efficiency. Sparse model structure
is induced by the solving the proximal operator.
</p>


<h3>Details</h3>

<p>This package provides 10 multi-task learning  algorithms (5
classification and 5 regression), which incorporate five 
regularization strategies for knowledge transferring among tasks. All
algorithms share the same framework:
</p>
<p style="text-align: center;"><code class="reqn">\min\limits_{W,C}
   \sum_{i}^{t}{L(W_i, C_i|X_i, Y_i)} + \lambda_1\Omega(W) + \lambda_2{||W||}_F^2</code>
</p>

<p>where <code class="reqn">L(\circ)</code> is the loss function (logistic loss for classification or least square loss for linear regression). 
<code class="reqn">\Omega(\circ)</code> is the cross-task regularization for knowledge transfer, and <code class="reqn">||W||_F^2</code> is used for improving the 
generalization. <code class="reqn">X=\{X_i= n_i \times p | i \in \{1,...,t\}\}</code> and <code class="reqn">Y=\{Y_i=n_i \times 1 | i \in \{1,...,t\}\}</code> are 
predictors matrices and responses of <code class="reqn">t</code> tasks respectively, while each task <code class="reqn">i</code> contains <code class="reqn">n_i</code> subjects and <code class="reqn">p</code> 
predictors. <code class="reqn">W=p \times t</code> is the coefficient matrix, where <code class="reqn">W_i</code>, the <code class="reqn">i</code>th column of <code class="reqn">W</code>, 
refers to the coefficient vector of task <code class="reqn">i</code>. 
</p>
<p>The function <code class="reqn">\Omega(W)</code> jointly modulates multi-task models(<code class="reqn">\{W_1, W_2, ..., W_t\}</code>) according to specific 
prior structure of <code class="reqn">W</code>. In this package, 5 common regularization methods are implemented to incorporate different priors, i.e.  
sparse structure (<code class="reqn">\Omega(W)=||W||_1</code>), joint feature selection (<code class="reqn">\Omega(W)=||W||_{2,1}</code>), low-rank structure
(<code class="reqn">\Omega(W)=||W||_*</code>), network-based relatedness across tasks (<code class="reqn">\Omega(W)=||WG||_F^2</code>) and task clustering
(<code class="reqn">\Omega(W)=tr(W^TW)-tr(F^TW^TWF)</code>). To call a specific method correctly, the corresponding "short name" has to be given. 
Follow the above sequence of methods, the short names are defined: <code>L21</code>, <code>Lasso</code>, <code>Trace</code>, <code>Graph</code> 
and <code>CMTL</code> 
</p>
<p>For all algorithms, we implemented an solver based on the accelerated
gradient descent method, which takes advantage of information from the
previous two iterations to calculate the current gradient and then
achieves an improved convergent rate. To solve the non-smooth and convex
regularizer, the proximal operator is applied. Moreover, backward
line search is used to determine the appropriate step-size in each
iteration. Overall, the solver achieves a complexity of
<code class="reqn">O(\frac{1}{k^2})</code> and is optimal among first-order gradient
descent methods.
</p>
<p>For the academic references of the implemented algorithms, the users are referred to the paper (doi:10.1093/bioinformatics/bty831) or 
the vignettes in the package.
</p>


</div>