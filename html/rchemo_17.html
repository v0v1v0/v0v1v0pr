<div class="container">

<table style="width: 100%;"><tr>
<td>plskern</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>PLSR algorithms</h2>

<h3>Description</h3>

<p>Algorithms fitting a PLS1 or PLS2 model between dependent variables <code class="reqn">X</code> and responses <code class="reqn">Y</code>.
</p>
<p>- <code>plskern</code>: "Improved kernel algorithm #1" proposed by Dayal and MacGregor (1997). This algorithm is stable and fast (Andersson 2009), and returns the same results as the NIPALS. 
</p>
<p>- <code>plsnipals</code>: NIPALS algorithm (e.g. Tenenhaus 1998, Wold 2002). In the function, the usual PLS2 NIPALS iterative is replaced by a direct calculation of the weights vector <code class="reqn">w</code> by SVD decomposition of matrix <code class="reqn">X'Y</code> (Hoskuldsson 1988 p.213).  
</p>
<p>- <code>plsrannar</code>: Kernel algorithm proposed by Rannar et al. (1994) for "wide" matrices, i.e.  with low number of rows and very large number of columns (p &gt;&gt; n; e.g. p = 20000). In such a situation, this algorithm is faster than the others (but it becomes much slower in other situations). If the algorithm converges, it returns the same results as the NIPALS (Note: discrepancies can be observed if too many PLS components are requested compared to the low number of observations).
</p>
<p>For weighted versions, see for instance Schaal et al. 2002, Siccard &amp; Sabatier 2006, Kim et al. 2011 and Lesnoff et al. 2020.
</p>
<p><b>Auxiliary functions</b>
</p>
<p><code>transform</code> Calculates the LVs for any new matrix <code class="reqn">X</code> from the model.
</p>
<p><code>summary</code> returns summary information for the model.
</p>
<p><code>coef</code> Calculates b-coefficients from the model.
</p>
<p><code>predict</code> Calculates the predictions for any new matrix <code class="reqn">X</code> from the model.
</p>


<h3>Usage</h3>

<pre><code class="language-R">
plskern(X, Y, weights = NULL, nlv, 
Xscaling = c("none", "pareto", "sd")[1], Yscaling = c("none", "pareto", "sd")[1])

plsnipals(X, Y, weights = NULL, nlv, 
Xscaling = c("none", "pareto", "sd")[1], Yscaling = c("none", "pareto", "sd")[1])

plsrannar(X, Y, weights = NULL, nlv, 
Xscaling = c("none", "pareto", "sd")[1], Yscaling = c("none", "pareto", "sd")[1])

## S3 method for class 'Plsr'
transform(object, X, ..., nlv = NULL)  

## S3 method for class 'Plsr'
summary(object, X, ...)  

## S3 method for class 'Plsr'
coef(object, ..., nlv = NULL) 

## S3 method for class 'Plsr'
predict(object, X, ..., nlv = NULL)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>For the main functions: Training X-data (<code class="reqn">n, p</code>). — For the auxiliary functions: Training X-data (<code class="reqn">n, p</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>Training Y-data (<code class="reqn">n, q</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>Weights (<code class="reqn">n, 1</code>) to apply to the training observations. Internally, weights are "normalized" to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlv</code></td>
<td>
<p>For the main functions: The number(s) of LVs to calculate. — For the auxiliary functions: The number(s) of LVs to consider.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Xscaling</code></td>
<td>
<p>X variable scaling among "none" (mean-centering only), "pareto" (mean-centering and pareto scaling), "sd" (mean-centering and unit variance scaling). If "pareto" or "sd", uncorrected standard deviation is used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Yscaling</code></td>
<td>
<p>Y variable scaling among "none" (mean-centering only), "pareto" (mean-centering and pareto scaling), "sd" (mean-centering and unit variance scaling). If "pareto" or "sd", uncorrected standard deviation is used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>For the auxiliary functions: A fitted model, output of a call to the main functions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>For the auxiliary functions: Optional arguments. Not used.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>For <code>plskern</code>, <code>plsnipals</code>, <code>plsrannar</code>: A list of outputs, such as
</p>
<table>
<tr style="vertical-align: top;">
<td><code>T</code></td>
<td>
<p>The X-score matrix (<code class="reqn">n, nlv</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>P</code></td>
<td>
<p>The X-loadings matrix (<code class="reqn">p, nlv</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>W</code></td>
<td>
<p>The X-loading weights matrix (<code class="reqn">p, nlv</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>C</code></td>
<td>
<p>The Y-loading weights matrix (C = t(Beta), where Beta is the scores regression coefficients matrix).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>R</code></td>
<td>
<p>The PLS projection matrix (<code class="reqn">p, nlv</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xmeans</code></td>
<td>
<p>The centering vector of <code class="reqn">X</code> (<code class="reqn">p, 1</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ymeans</code></td>
<td>
<p>The centering vector of <code class="reqn">Y</code> (<code class="reqn">q, 1</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xscales</code></td>
<td>
<p>The vector of <code class="reqn">X</code> variable standard deviations (<code class="reqn">p, 1</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>yscales</code></td>
<td>
<p>The vector of <code class="reqn">Y</code> variable standard deviations (<code class="reqn">q, 1</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>Weights applied to the training observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>TT</code></td>
<td>
<p>the X-score normalization factor.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>U</code></td>
<td>
<p>intermediate output.</p>
</td>
</tr>
</table>
<p>For <code>transform.Plsr</code>: X-scores matrix for new X-data.
</p>
<p>For <code>summary.Plsr</code>:
</p>
<table><tr style="vertical-align: top;">
<td><code>explvarx</code></td>
<td>
<p>matrix of explained variances.</p>
</td>
</tr></table>
<p>For <code>coef.Plsr</code>: 
</p>
<table>
<tr style="vertical-align: top;">
<td><code>int</code></td>
<td>
<p>matrix (1,nlv) with the intercepts</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>B</code></td>
<td>
<p>matrix (n,nlv) with the coefficients</p>
</td>
</tr>
</table>
<p>For <code>predict.Plsr</code>: 
</p>
<table><tr style="vertical-align: top;">
<td><code>pred</code></td>
<td>
<p>A list of matrices (<code class="reqn">m, q</code>) with the Y predicted values for the new X-data</p>
</td>
</tr></table>
<h3>References</h3>

<p>Andersson, M., 2009. A comparison of nine PLS1 algorithms. Journal of Chemometrics 23, 518-529.
</p>
<p>Dayal, B.S., MacGregor, J.F., 1997. Improved PLS algorithms. Journal of Chemometrics 11, 73-85.
</p>
<p>Hoskuldsson, A., 1988. PLS regression methods. Journal of Chemometrics 2, 211-228. https://doi.org/10.1002/cem.1180020306
</p>
<p>Kim, S., Kano, M., Nakagawa, H., Hasebe, S., 2011. Estimation of active pharmaceutical ingredients content using locally weighted partial least squares and statistical wavelength selection. Int. J. Pharm., 421, 269-274.
</p>
<p>Lesnoff, M., Metz, M., Roger, J.M., 2020. Comparison of locally weighted PLS strategies for regression and discrimination on agronomic NIR Data. Journal of Chemometrics. e3209. https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.3209
</p>
<p>Rannar, S., Lindgren, F., Geladi, P., Wold, S., 1994. A PLS kernel algorithm for data sets with many variables and fewer objects. Part 1: Theory and algorithm. Journal of Chemometrics 8, 111-125. https://doi.org/10.1002/cem.1180080204
</p>
<p>Schaal, S., Atkeson, C., Vijayamakumar, S. 2002. Scalable techniques from nonparametric statistics for the real time robot learning. Applied Intell., 17, 49-60.
</p>
<p>Sicard, E. Sabatier, R., 2006. Theoretical framework for local PLS1 regression and application to a rainfall data set. Comput. Stat. Data Anal., 51, 1393-1410.
</p>
<p>Tenenhaus, M., 1998. La régression PLS: théorie et pratique. Editions Technip, Paris, France.
</p>
<p>Wold, S., Sjostrom, M., Eriksson, l., 2001. PLS-regression: a basic tool for chemometrics. Chem. Int. Lab. Syst., 58, 109-130.
</p>


<h3>See Also</h3>

<p><code>plsr_plsda_allsteps</code> function to help determine the optimal number of latent variables, perform a permutation test, calculate model parameters and predict new observations.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
n &lt;- 6 ; p &lt;- 4
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(y1 = ytrain, y2 = 100 * ytrain)
m &lt;- 3
Xtest &lt;- Xtrain[1:m, , drop = FALSE] 
Ytest &lt;- Ytrain[1:m, , drop = FALSE] ; ytest &lt;- Ytest[1:m, 1]

nlv &lt;- 3
plskern(Xtrain, Ytrain, Xscaling = "sd", nlv = nlv)
plsnipals(Xtrain, Ytrain, Xscaling = "sd", nlv = nlv)
plsrannar(Xtrain, Ytrain, Xscaling = "sd", nlv = nlv)

plskern(Xtrain, Ytrain, Xscaling = "none", nlv = nlv)
plskern(Xtrain, Ytrain, nlv = nlv)$T
plskern(Xtrain, Ytrain, nlv = nlv, weights = 1:n)$T

fm &lt;- plskern(Xtrain, Ytrain, nlv = nlv)
coef(fm)
coef(fm, nlv = 0)
coef(fm, nlv = 1)

fm$T
transform(fm, Xtest)
transform(fm, Xtest, nlv = 1)

summary(fm, Xtrain)

predict(fm, Xtest)
predict(fm, Xtest, nlv = 0:3)

pred &lt;- predict(fm, Xtest)$pred
msep(pred, Ytest)

</code></pre>


</div>