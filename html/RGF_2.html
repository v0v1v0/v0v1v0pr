<div class="container">

<table style="width: 100%;"><tr>
<td>FastRGF_Regressor</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>A Fast Regularized Greedy Forest regressor</h2>

<h3>Description</h3>

<p>A Fast Regularized Greedy Forest regressor
</p>
<p>A Fast Regularized Greedy Forest regressor
</p>


<h3>Usage</h3>

<pre><code class="language-R"># init &lt;- FastRGF_Regressor$new(n_estimators = 500, max_depth = 6,
#                                      max_leaf = 50, tree_gain_ratio = 1.0,
#                                      min_samples_leaf = 5, l1 = 1.0,
#                                      l2 = 1000.0, opt_algorithm = "rgf",
#                                      learning_rate = 0.001, max_bin = NULL,
#                                      min_child_weight = 5.0, data_l2 = 2.0,
#                                      sparse_max_features = 80000,
#                                      sparse_min_occurences = 5,
#                                      n_jobs = 1, verbose = 0)
</code></pre>


<h3>Details</h3>

<p>the <em>fit</em> function builds a regressor from the training set (x, y).
</p>
<p>the <em>predict</em> function predicts the regression target for x.
</p>
<p>the <em>cleanup</em> function removes tempfiles used by this model. See the issue <em>https://github.com/RGF-team/rgf/issues/75</em>, which explains in which cases the <em>cleanup</em> function applies.
</p>
<p>the <em>get_params</em> function returns the parameters of the model.
</p>
<p>the <em>score</em> function returns the coefficient of determination ( R^2 ) for the predictions.
</p>


<h3>Methods</h3>


<dl>
<dt><code>FastRGF_Regressor$new(n_estimators = 500, max_depth = 6,
                                   max_leaf = 50, tree_gain_ratio = 1.0,
                                   min_samples_leaf = 5, l1 = 1.0,
                                   l2 = 1000.0, opt_algorithm = "rgf",
                                   learning_rate = 0.001, max_bin = NULL,
                                   min_child_weight = 5.0, data_l2 = 2.0,
                                   sparse_max_features = 80000,
                                   sparse_min_occurences = 5,
                                   n_jobs = 1, verbose = 0)</code></dt>
<dd></dd>
<dt><code>--------------</code></dt>
<dd></dd>
<dt><code>fit(x, y, sample_weight = NULL)</code></dt>
<dd></dd>
<dt><code>--------------</code></dt>
<dd></dd>
<dt><code>predict(x)</code></dt>
<dd></dd>
<dt><code>--------------</code></dt>
<dd></dd>
<dt><code>cleanup()</code></dt>
<dd></dd>
<dt><code>--------------</code></dt>
<dd></dd>
<dt><code>get_params(deep = TRUE)</code></dt>
<dd></dd>
<dt><code>--------------</code></dt>
<dd></dd>
<dt><code>score(x, y, sample_weight = NULL)</code></dt>
<dd></dd>
<dt><code>--------------</code></dt>
<dd></dd>
</dl>
<h3>Super class</h3>

<p><code>RGF::Internal_class</code> -&gt; <code>FastRGF_Regressor</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-FastRGF_Regressor-new"><code>FastRGF_Regressor$new()</code></a>
</p>
</li>
<li> <p><a href="#method-FastRGF_Regressor-clone"><code>FastRGF_Regressor$clone()</code></a>
</p>
</li>
</ul>
<details><summary>Inherited methods</summary><ul>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="cleanup"><a href="../../RGF/html/Internal_class.html#method-Internal_class-cleanup"><code>RGF::Internal_class$cleanup()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="dump_model"><a href="../../RGF/html/Internal_class.html#method-Internal_class-dump_model"><code>RGF::Internal_class$dump_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="feature_importances"><a href="../../RGF/html/Internal_class.html#method-Internal_class-feature_importances"><code>RGF::Internal_class$feature_importances()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="fit"><a href="../../RGF/html/Internal_class.html#method-Internal_class-fit"><code>RGF::Internal_class$fit()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="get_params"><a href="../../RGF/html/Internal_class.html#method-Internal_class-get_params"><code>RGF::Internal_class$get_params()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="predict"><a href="../../RGF/html/Internal_class.html#method-Internal_class-predict"><code>RGF::Internal_class$predict()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="predict_proba"><a href="../../RGF/html/Internal_class.html#method-Internal_class-predict_proba"><code>RGF::Internal_class$predict_proba()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="save_model"><a href="../../RGF/html/Internal_class.html#method-Internal_class-save_model"><code>RGF::Internal_class$save_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="score"><a href="../../RGF/html/Internal_class.html#method-Internal_class-score"><code>RGF::Internal_class$score()</code></a></span></li>
</ul></details><hr>
<a id="method-FastRGF_Regressor-new"></a>



<h4>Method <code>new()</code>
</h4>



<h5>Usage</h5>

<div class="r"><pre>FastRGF_Regressor$new(
  n_estimators = 500,
  max_depth = 6,
  max_leaf = 50,
  tree_gain_ratio = 1,
  min_samples_leaf = 5,
  l1 = 1,
  l2 = 1000,
  opt_algorithm = "rgf",
  learning_rate = 0.001,
  max_bin = NULL,
  min_child_weight = 5,
  data_l2 = 2,
  sparse_max_features = 80000,
  sparse_min_occurences = 5,
  n_jobs = 1,
  verbose = 0
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>n_estimators</code></dt>
<dd>
<p>an integer. The number of trees in the forest (Original name: forest.ntrees.)</p>
</dd>
<dt><code>max_depth</code></dt>
<dd>
<p>an integer. Maximum tree depth (Original name: dtree.max_level.)</p>
</dd>
<dt><code>max_leaf</code></dt>
<dd>
<p>an integer. Maximum number of leaf nodes in best-first search (Original name: dtree.max_nodes.)</p>
</dd>
<dt><code>tree_gain_ratio</code></dt>
<dd>
<p>a float. New tree is created when leaf-nodes gain &lt; this value * estimated gain of creating new tree (Original name: dtree.new_tree_gain_ratio.)</p>
</dd>
<dt><code>min_samples_leaf</code></dt>
<dd>
<p>an integer or float. Minimum number of training data points in each leaf node. If an integer, then consider min_samples_leaf as the minimum number. If a float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node (Original name: dtree.min_sample.)</p>
</dd>
<dt><code>l1</code></dt>
<dd>
<p>a float. Used to control the degree of L1 regularization (Original name: dtree.lamL1.)</p>
</dd>
<dt><code>l2</code></dt>
<dd>
<p>a float. Used to control the degree of L2 regularization (Original name: dtree.lamL2.)</p>
</dd>
<dt><code>opt_algorithm</code></dt>
<dd>
<p>a character string. Either <em>"rgf"</em> or <em>"epsilon-greedy"</em>. Optimization method for training forest (Original name: forest.opt.)</p>
</dd>
<dt><code>learning_rate</code></dt>
<dd>
<p>a float. Step size of epsilon-greedy boosting. Meant for being used with opt_algorithm = "epsilon-greedy" (Original name: forest.stepsize.)</p>
</dd>
<dt><code>max_bin</code></dt>
<dd>
<p>an integer or NULL. Maximum number of discretized values (bins). If NULL, 65000 is used for dense data and 200 for sparse data (Original name: discretize.(sparse/dense).max_buckets.)</p>
</dd>
<dt><code>min_child_weight</code></dt>
<dd>
<p>a float. Minimum sum of data weights for each discretized value (bin) (Original name: discretize.(sparse/dense).min_bucket_weights.)</p>
</dd>
<dt><code>data_l2</code></dt>
<dd>
<p>a float. Used to control the degree of L2 regularization for discretization (Original name: discretize.(sparse/dense).lamL2.)</p>
</dd>
<dt><code>sparse_max_features</code></dt>
<dd>
<p>an integer. Maximum number of selected features. Meant for being used with sparse data (Original name: discretize.sparse.max_features.)</p>
</dd>
<dt><code>sparse_min_occurences</code></dt>
<dd>
<p>an integer. Minimum number of occurrences for a feature to be selected. Meant for being used with sparse data (Original name: discretize.sparse.min_occrrences.)</p>
</dd>
<dt><code>n_jobs</code></dt>
<dd>
<p>an integer. The number of jobs to run in parallel for both fit and predict. If -1, all CPUs are used. If -2, all CPUs but one are used. If &lt; -1, (n_cpus + 1 + n_jobs) are used (Original name: set.nthreads.)</p>
</dd>
<dt><code>verbose</code></dt>
<dd>
<p>an integer. Controls the verbosity of the tree building process (Original name: set.verbose.)</p>
</dd>
</dl>
</div>


<hr>
<a id="method-FastRGF_Regressor-clone"></a>



<h4>Method <code>clone()</code>
</h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>FastRGF_Regressor$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt>
<dd>
<p>Whether to make a deep clone.</p>
</dd>
</dl>
</div>




<h3>References</h3>

<p><em>https://github.com/RGF-team/rgf/tree/master/python-package</em>, <em>Tong Zhang, FastRGF: Multi-core Implementation of Regularized Greedy Forest (https://github.com/RGF-team/rgf/tree/master/FastRGF)</em>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
try({
    if (reticulate::py_available(initialize = FALSE)) {
        if (reticulate::py_module_available("rgf.sklearn")) {

            library(RGF)

            set.seed(1)
            x = matrix(runif(100000), nrow = 100, ncol = 1000)

            y = runif(100)

            fast_RGF_regr = FastRGF_Regressor$new(max_leaf = 50)

            fast_RGF_regr$fit(x, y)

            preds = fast_RGF_regr$predict(x)
        }
    }
}, silent = TRUE)
</code></pre>


</div>