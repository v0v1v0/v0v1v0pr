<div class="container">

<table style="width: 100%;"><tr>
<td>rboost</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Generalized Boosted Modeling via Rank-Based Trees for Single Sample Classification with Gene Expression Profiles</h2>

<h3>Description</h3>

<p>The function fits generalized boosted models via Rank-Based Trees on both binary and multi-class problems. It converts continuous gene expression profiles into
ranked gene pairs, for which the variable importance indices are computed and adopted for dimension reduction.   The boosting implementation was directly imported from the <span class="pkg">gbm</span>
package.  For technical details, see the
vignette: <code>utils::browseVignettes("gbm")</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">rboost(
  formula,
  data,
  dimreduce = TRUE,
  datrank = TRUE,
  distribution = "multinomial",
  weights,
  ntree = 100,
  nodedepth = 3,
  nodesize = 5,
  shrinkage = 0.05,
  bag.fraction = 0.5,
  train.fraction = 1,
  cv.folds = 5,
  keep.data = TRUE,
  verbose = TRUE,
  class.stratify.cv = TRUE,
  n.cores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>Object of class 'formula' describing the model to fit.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>Data frame containing the y-outcome and x-variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dimreduce</code></td>
<td>
<p>Dimension reduction via variable importance weighted forests. <code>FALSE</code>
means no dimension reduction; <code>TRUE</code> means reducing 75% variables before binary rank conversion and then fitting a weighted forest; a numeric value x% between 0 and 1 means reducing x% variables before binary rank conversion and then fitting a weighted forest.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>datrank</code></td>
<td>
<p> If using ranked raw data for fitting the dimension reduction model. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>distribution</code></td>
<td>
<p>Either a character string specifying the name of the
distribution to use: if the response has only 2 unique values,
<code>bernoulli</code> is assumed; otherwise, if the response is a factor, <code>multinomial</code> is
assumed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>an optional vector of weights to be used in the fitting
process. It must be positive but does not need to be normalized.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ntree</code></td>
<td>
<p>Integer specifying the total number of trees to fit. This is
equivalent to the number of iterations and the number of basis functions in
the additive expansion, which matches <code>n.tree</code> in the <code>gbm</code> package.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nodedepth</code></td>
<td>
<p>Integer specifying the maximum depth of each tree. A value of 1
implies an additive model. This matches <code>interaction.depth</code> in the <code>gbm</code> package.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nodesize</code></td>
<td>
<p>Integer specifying the minimum number of observations
in the terminal nodes of the trees, which matches <code>n.minobsinnode</code> in the <code>gbm</code> package.. Note that this is the actual number of
observations, not the total weight.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>shrinkage</code></td>
<td>
<p>a shrinkage parameter applied to each tree in the
expansion. Also known as the learning rate or step-size reduction; 0.001 to
0.1 usually work, but a smaller learning rate typically requires more trees.
Default is 0.05.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bag.fraction</code></td>
<td>
<p>the fraction of the training set observations randomly
selected to propose the next tree in the expansion. This introduces
randomnesses into the model fit. If <code>bag.fraction</code> &lt; 1 then running the
same model twice will result in similar but different fits. <code>gbm</code> uses
the R random number generator so <code>set.seed</code> can ensure that the model
can be reconstructed. Preferably, the user can save the returned
<code>gbm.object</code> using <code>save</code>. Default is 0.5.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>train.fraction</code></td>
<td>
<p>The first <code>train.fraction * nrows(data)</code>
observations are used to fit the <code>gbm</code> and the remaining observations are used for
computing out-of-sample estimates of the loss function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cv.folds</code></td>
<td>
<p>Number of cross-validation folds to perform. If
<code>cv.folds</code>&gt;1 then <code>gbm</code>, in addition to the usual fit, will
perform cross-validation and calculate an estimate of generalization error
returned in <code>cv.error</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>keep.data</code></td>
<td>
<p>a logical variable indicating whether to keep the data and
an index of the data stored with the object. Keeping the data and index
makes subsequent calls to <code>gbm.more</code> faster at the cost of
storing an extra copy of the dataset.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Logical indicating whether or not to print out progress and
performance indicators (<code>TRUE</code>). If this option is left unspecified for
<code>gbm.more</code>, then it uses <code>verbose</code> from <code>object</code>. Default is
<code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>class.stratify.cv</code></td>
<td>
<p>Logical indicating whether or not the
cross-validation should be stratified by class. The purpose of stratifying the
cross-validation is to help avoid situations in which training sets do
not contain all classes.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.cores</code></td>
<td>
<p>The number of CPU cores to use. The cross-validation loop
will attempt to send different CV folds off to different cores. If
<code>n.cores</code> is not specified by the user, it is guessed using the
<code>detectCores</code> function in the <code>parallel</code> package. Note that the
documentation for <code>detectCores</code> makes clear that it is not failsafe and
could return a spurious number of available cores.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>fit</code></td>
<td>
<p>A vector containing the fitted
values on the scale of regression function (e.g. log-odds scale for
bernoulli).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>train.error</code></td>
<td>
<p>A vector of length
equal to the number of fitted trees containing the value of the loss
function for each boosting iteration evaluated on the training data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>valid.error</code></td>
<td>
<p>A vector of length equal to the number of fitted trees
containing the value of the loss function for each boosting iteration
evaluated on the validation data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cv.error</code></td>
<td>
<p>If <code>cv.folds</code> &lt; 2 this
component is <code>NULL</code>. Otherwise, this component is a vector of length equal to
the number of fitted trees containing a cross-validated estimate of the loss
function for each boosting iteration.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>oobag.improve</code></td>
<td>
<p>A vector of
length equal to the number of fitted trees containing an out-of-bag estimate
of the marginal reduction in the expected value of the loss function. The
out-of-bag estimate uses only the training data and is useful for estimating
the optimal number of boosting iterations. See <code>gbm.perf</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cv.fitted</code></td>
<td>
<p>If cross-validation was performed, the cross-validation
predicted values on the scale of the linear predictor. That is, the fitted
values from the i-th CV-fold, for the model having been trained on the data
in all other folds.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Ruijie Yin (Maintainer,&lt;ruijieyin428@gmail.com&gt;), Chen Ye and Min Lu
</p>


<h3>References</h3>

<p>Lu M. Yin R. and Chen X.S. Ensemble Methods of Rank-Based Trees for Single Sample Classification with Gene Expression Profiles. Journal of Translational Medicine. 22, 140 (2024). doi: 10.1186/s12967-024-04940-2
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(tnbc)
obj &lt;- rboost(subtype~., data = tnbc[,c(1:10,337)])
obj
</code></pre>


</div>