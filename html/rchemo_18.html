<div class="container">

<table style="width: 100%;"><tr>
<td>rr</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Linear Ridge Regression</h2>

<h3>Description</h3>

<p>Fitting linear ridge regression models (RR) (Hoerl &amp; Kennard 1970, Hastie &amp; Tibshirani 2004, Hastie et al 2009, Cule &amp; De Iorio 2012) by SVD factorization.
</p>


<h3>Usage</h3>

<pre><code class="language-R">
rr(X, Y, weights = NULL, lb = 1e-2)

## S3 method for class 'Rr'
coef(object, ..., lb = NULL)  

## S3 method for class 'Rr'
predict(object, X, ..., lb = NULL)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>For the main function: Training X-data (<code class="reqn">n, p</code>). — For the auxiliary functions: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>Training Y-data (<code class="reqn">n, q</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>Weights (<code class="reqn">n, 1</code>) to apply to the training observations. Internally, weights are "normalized" to sum to 1. Default to <code>NULL</code> (weights are set to <code class="reqn">1 / n</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lb</code></td>
<td>
<p>A value of regularization parameter <code class="reqn">lambda</code>. If <code>lb = 0</code>, a pseudo-inverse is used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>For the auxiliary functions: A fitted model, output of a call to the main function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>— For the auxiliary functions: Optional arguments. Not used.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>For <code>rr</code>:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>V</code></td>
<td>
<p>eigenvector matrix of the correlation matrix (n,n).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>TtDY</code></td>
<td>
<p>intermediate output.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sv</code></td>
<td>
<p>singular values of the matrix <code class="reqn">(1,n)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lb</code></td>
<td>
<p>value of regularization parameter <code class="reqn">lambda</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xmeans</code></td>
<td>
<p>the centering vector of X <code class="reqn">(p,1)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ymeans</code></td>
<td>
<p>the centering vector of Y <code class="reqn">(q,1)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>the weights vector of X-variables <code class="reqn">(p,1)</code>.</p>
</td>
</tr>
</table>
<p>For <code>coef.Rr</code>:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>int</code></td>
<td>
<p>matrix (1,nlv) with the intercepts</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>B</code></td>
<td>
<p>matrix (n,nlv) with the coefficients</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>df</code></td>
<td>
<p>model complexity (number of degrees of freedom)</p>
</td>
</tr>
</table>
<p>For <code>predict.Rr</code>:
</p>
<table><tr style="vertical-align: top;">
<td><code>pred</code></td>
<td>
<p>A list of matrices (<code class="reqn">m, q</code>) with the Y predicted values for the new X-data</p>
</td>
</tr></table>
<h3>References</h3>

<p>Cule, E., De Iorio, M., 2012. A semi-automatic method to guide the choice of ridge parameter in ridge regression. arXiv:1205.0686.
</p>
<p>Hastie, T., Tibshirani, R., 2004. Efficient quadratic regularization for expression arrays. Biostatistics 5, 329-340. https://doi.org/10.1093/biostatistics/kxh010
</p>
<p>Hastie, T., Tibshirani, R., Friedman, J., 2009. The elements of statistical learning: data mining, inference, and prediction, 2nd ed. Springer, New York.
</p>
<p>Hoerl, A.E., Kennard, R.W., 1970. Ridge Regression: Biased Estimation for Nonorthogonal Problems. Technometrics 12, 55-67. https://doi.org/10.1080/00401706.1970.10488634
</p>
<p>Wu, W., Massart, D.L., de Jong, S., 1997. The kernel PCA algorithms for wide data. Part I: Theory and algorithms. Chemometrics and Intelligent Laboratory Systems 36, 165-172. https://doi.org/10.1016/S0169-7439(97)00010-5
</p>


<h3>Examples</h3>

<pre><code class="language-R">
n &lt;- 6 ; p &lt;- 4
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(y1 = ytrain, y2 = 100 * ytrain)
m &lt;- 3
Xtest &lt;- Xtrain[1:m, , drop = FALSE] 
Ytest &lt;- Ytrain[1:m, , drop = FALSE] ; ytest &lt;- Ytest[1:m, 1]

lb &lt;- .1
fm &lt;- rr(Xtrain, Ytrain, lb = lb)
coef(fm)
coef(fm, lb = .8)
predict(fm, Xtest)
predict(fm, Xtest, lb = c(0.1, .8))

pred &lt;- predict(fm, Xtest)$pred
msep(pred, Ytest)

</code></pre>


</div>