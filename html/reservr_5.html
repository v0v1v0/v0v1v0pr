<div class="container">

<table style="width: 100%;"><tr>
<td>callback_adaptive_lr</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Keras Callback for adaptive learning rate with weight restoration</h2>

<h3>Description</h3>

<p>Provides a keras callback similar to <code>keras3::callback_reduce_lr_on_plateau()</code> but which also restores the weights
to the best seen so far whenever a learning rate reduction occurs, and with slightly more restrictive improvement
detection.
</p>


<h3>Usage</h3>

<pre><code class="language-R">callback_adaptive_lr(
  monitor = "val_loss",
  factor = 0.1,
  patience = 10L,
  verbose = 0L,
  mode = c("auto", "min", "max"),
  delta_abs = 1e-04,
  delta_rel = 0,
  cooldown = 0L,
  min_lr = 0,
  restore_weights = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>monitor</code></td>
<td>
<p>quantity to be monitored.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>factor</code></td>
<td>
<p>factor by which the learning rate will be reduced. <code>new_lr = old_lr * factor</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>patience</code></td>
<td>
<p>number of epochs with no significant improvement after which the learning rate will be reduced.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>integer. Set to 1 to receive update messages.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mode</code></td>
<td>
<p>Optimisation mode. "auto" detects the mode from the name of <code>monitor</code>. "min" monitors for decreasing
metrics. "max" monitors for increasing metrics.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>delta_abs</code></td>
<td>
<p>Minimum absolute metric improvement per epoch. The learning rate will be reduced if the average
improvement is less than <code>delta_abs</code> per epoch for <code>patience</code> epochs.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>delta_rel</code></td>
<td>
<p>Minimum relative metric improvement per epoch. The learning rate will be reduced if the average
improvement is less than <code style="white-space: pre;">⁠|metric| * delta_rel⁠</code> per epoch for <code>patience</code> epochs.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cooldown</code></td>
<td>
<p>number of epochs to wait before resuming normal operation after learning rate has been reduced.
The minimum number of epochs between two learning rate reductions is <code>patience + cooldown</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_lr</code></td>
<td>
<p>lower bound for the learning rate. If a learning rate reduction would lower the learning rate below
<code>min_lr</code>, it will be clipped at <code>min_lr</code> instead and no further reductions will be performed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>restore_weights</code></td>
<td>
<p>Bool. If TRUE, the best weights will be restored at each learning rate reduction.
This is very useful if the metric oscillates.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Note that while <code>keras3::callback_reduce_lr_on_plateau()</code> automatically logs the learning rate as a metric 'lr',
this is currently impossible from R.
Thus, if you want to also log the learning rate, you should add <code>keras3::callback_reduce_lr_on_plateau()</code> with a high
<code>min_lr</code> to effectively disable the callback but still monitor the learning rate.
</p>


<h3>Value</h3>

<p>A <code>KerasCallback</code> suitable for passing to <code>keras3::fit()</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">dist &lt;- dist_exponential()
group &lt;- sample(c(0, 1), size = 100, replace = TRUE)
x &lt;- dist$sample(100, with_params = list(rate = group + 1))
global_fit &lt;- fit(dist, x)

if (interactive()) {
  library(keras3)
  l_in &lt;- layer_input(shape = 1L)
  mod &lt;- tf_compile_model(
    inputs = list(l_in),
    intermediate_output = l_in,
    dist = dist,
    optimizer = optimizer_adam(),
    censoring = FALSE,
    truncation = FALSE
  )
  tf_initialise_model(mod, global_fit$params)
  fit_history &lt;- fit(
    mod,
    x = as_tensor(group, config_floatx()),
    y = as_trunc_obs(x),
    epochs = 20L,
    callbacks = list(
      callback_adaptive_lr("loss", factor = 0.5, patience = 2L, verbose = 1L, min_lr = 1.0e-4),
      callback_reduce_lr_on_plateau("loss", min_lr = 1.0) # to track lr
    )
  )

  plot(fit_history)

  predicted_means &lt;- predict(mod, data = as_tensor(c(0, 1), config_floatx()))
}

</code></pre>


</div>