<div class="container">

<table style="width: 100%;"><tr>
<td>CrossValidationSSL</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Cross-validation in semi-supervised setting</h2>

<h3>Description</h3>

<p>Cross-validation for semi-supervised learning, in which the dataset is split in three parts: labeled training object, unlabeled training object and validation objects. This can be used to evaluate different approaches to semi-supervised classification under the assumption the labels are missing at random. Different cross-validation schemes are implemented. See below for details.
</p>


<h3>Usage</h3>

<pre><code class="language-R">CrossValidationSSL(X, y, ...)

## S3 method for class 'list'
CrossValidationSSL(X, y, ..., verbose = FALSE, mc.cores = 1)

## S3 method for class 'matrix'
CrossValidationSSL(X, y, classifiers, measures = list(Error
  = measure_error), k = 10, repeats = 1, verbose = FALSE,
  leaveout = "test", n_labeled = 10, prop_unlabeled = 0.5, time = TRUE,
  pre_scale = FALSE, pre_pca = FALSE, n_min = 1, low_level_cores = 1,
  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>design matrix of the labeled objects</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>vector with labels</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>arguments passed to underlying functions</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>logical; Controls the verbosity of the output</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mc.cores</code></td>
<td>
<p>integer; Number of cores to be used</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>classifiers</code></td>
<td>
<p>list; Classifiers to crossvalidate</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>measures</code></td>
<td>
<p>named list of functions giving the measures to be used</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>integer; Number of folds in the cross-validation</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>repeats</code></td>
<td>
<p>integer; Number of repeated assignments to folds</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>leaveout</code></td>
<td>
<p>either "labeled" or "test", see details</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_labeled</code></td>
<td>
<p>Number of labeled examples, used in both leaveout modes</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prop_unlabeled</code></td>
<td>
<p>numeric; proportion of unlabeled objects</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>time</code></td>
<td>
<p>logical; Whether execution time should be saved.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pre_scale</code></td>
<td>
<p>logical; Whether the features should be scaled before the dataset is used</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pre_pca</code></td>
<td>
<p>logical; Whether the features should be preprocessed using a PCA step</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_min</code></td>
<td>
<p>integer; Minimum number of labeled objects per class</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>low_level_cores</code></td>
<td>
<p>integer; Number of cores to use compute repeats of the learning curve</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The input to this function can be either: a dataset in the form of a feature matrix and factor containing the labels, a dataset in the form of a formula and data.frame or a named list of these two options.
There are two main modes in which the cross-validation can be carried out, controlled by the <code>leaveout</code> parameter. 
When leaveout is "labeled", the folds are formed by non-overlapping labeled training sets of a user specified size. 
Each of these folds is used as a labeled set, while the rest of the objects are split into the an unlabeled and the test set, controlled by <code>prop_unlabeled</code> parameter. Note that objects can be used multiple times for testing, when training on a different fold, while other objects may never used for testing.
</p>
<p>The "test" option of <code>leaveout</code>, on the other hand, uses the folds as the test sets. This means every object will be used as a test object exactly once. The remaining objects in each training iteration are split randomly into a labeled and an unlabeled part, where the number of the labeled objects is controlled by the user through the n_labeled parameter.
</p>


<h3>Examples</h3>

<pre><code class="language-R">X &lt;- model.matrix(Species~.-1,data=iris)
y &lt;- iris$Species

classifiers &lt;- list("LS"=function(X,y,X_u,y_u) {
  LeastSquaresClassifier(X,y,lambda=0)}, 
  "EM"=function(X,y,X_u,y_u) {
    SelfLearning(X,y,X_u,
                 method=LeastSquaresClassifier)}
)

measures &lt;- list("Accuracy" =  measure_accuracy,
                 "Loss" = measure_losstest,
                 "Loss labeled" = measure_losslab,
                 "Loss Lab+Unlab" = measure_losstrain
)

# Cross-validation making sure test folds are non-overlapping
cvresults1 &lt;- CrossValidationSSL(X,y, 
                                 classifiers=classifiers, 
                                 measures=measures,
                                 leaveout="test", k=10,
                                 repeats = 2,n_labeled = 10)
print(cvresults1)
plot(cvresults1)

# Cross-validation making sure labeled sets are non-overlapping
cvresults2 &lt;- CrossValidationSSL(X,y, 
                                 classifiers=classifiers, 
                                 measures=measures,
                                 leaveout="labeled", k=10,
                                 repeats = 2,n_labeled = 10,
                                 prop_unlabeled=0.5)
print(cvresults2)
plot(cvresults2)

</code></pre>


</div>