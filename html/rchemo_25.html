<div class="container">

<table style="width: 100%;"><tr>
<td>dfplsr_cg</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Degrees of freedom of Univariate PLSR Models</h2>

<h3>Description</h3>

<p>Computation of the model complexity <code class="reqn">df</code> (number of degrees of freedom) of univariate PLSR models (with intercept). See Lesnoff et al. 2021 for an illustration.
</p>
<p>(1) Estimation from the CGLSR algorithm (Hansen, 1998).
</p>
<p>- <code>dfplsr_cov</code>
</p>
<p>(2) Monte Carlo estimation (Ye, 1998 and Efron, 2004). Details in relation with the functions are given in Lesnoff et al. 2021.
</p>
<p>- <code>dfplsr_cov</code>: The covariances are computed  by parametric bootstrap (Efron, 2004, Eq. 2.16). The residual variance <code class="reqn">sigma^2</code> is estimated from a low-biased model.
</p>
<p>- <code>dfplsr_div</code>: The divergencies <code class="reqn">dy_fit/dy</code> are computed by perturbation analysis(Ye, 1998 and Efron, 2004). This is a Stein unbiased risk estimation (SURE) of <code class="reqn">df</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">
dfplsr_cg(X, y, nlv, reorth = TRUE)

dfplsr_cov(
    X, y, nlv, algo = NULL,
    maxlv = 50, B = 30, print = FALSE, ...)

dfplsr_div(
    X, y, nlv, algo = NULL,
    eps = 1e-2, B = 30, print = FALSE, ...) 

</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>A <code class="reqn">n x p</code> matrix or data frame of training observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>A vector of length <code class="reqn">n</code> of training responses.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlv</code></td>
<td>
<p>The maximal number of latent variables (LVs) to consider in the model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reorth</code></td>
<td>
<p>For <code>dfplsr_cg</code>: Logical. If <code>TRUE</code>, a Gram-Schmidt reorthogonalization of the normal equation residual vectors is done.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>algo</code></td>
<td>
<p>a PLS algorithm. Default to  <code>NULL</code> (<code>plskern</code> is used).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxlv</code></td>
<td>
<p>For <code>dfplsr_cov</code>: dDmension of the PLSR model (nb. LVs) used for parametric bootstrap.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>For <code>dfplsr_div</code>: The <code class="reqn">epsilon</code> quantity used for scaling the perturbation analysis.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>B</code></td>
<td>
<p>For <code>dfplsr_cov</code>: Number of bootstrap replications. For <code>dfplsr_div</code>: number of observations in the data receiving perturbation (the maximum is <code class="reqn">n</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>print</code></td>
<td>
<p>Logical. If <code>TRUE</code>, fitting information are printed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Optionnal arguments to pass in the function defined in <code>algo</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Missing values are not allowed.
</p>
<p>The example below reproduces the numerical illustration given by Kramer &amp; Sugiyama 2011 on the Ozone data (Fig. 1, center).
The <code class="reqn">pls.model</code> function from the R package "plsdof" v0.2-9 (Kramer &amp; Braun 2019) is used for <code class="reqn">df</code> calculations (<code class="reqn">df.kramer</code>), and automatically scales the X matrix before PLS. The example scales also X for consistency when using the other functions.
</p>
<p>For the Monte Carlo estimations, <code>B</code> Should be increased for more stability
</p>


<h3>Value</h3>

<p>A list of outputs :
</p>
<table>
<tr style="vertical-align: top;">
<td><code>df</code></td>
<td>
<p>vector with the model complexity for the models with <code class="reqn">a = 0, 1, ..., nlv</code> components.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cov</code></td>
<td>
<p>For <code>dfplsr_cov</code>: vector with covariances, computed  by parametric bootstrap.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Efron, B., 2004. The Estimation of Prediction Error. Journal of the American Statistical Association 99,
619-632. https://doi.org/10.1198/016214504000000692
</p>
<p>Hastie, T., Tibshirani, R.J., 1990. Generalized Additive Models, Monographs on statistics and applied
probablity. Chapman and Hall/CRC, New York, USA.
</p>
<p>Hastie, T., Tibshirani, R., Friedman, J., 2009. The elements of statistical learning: data mining,
inference, and prediction, 2nd ed. Springer, NewYork.
</p>
<p>Hastie, T., Tibshirani, R., Wainwright, M., 2015. Statistical Learning with Sparsity: The Lasso and
Generalizations. CRC Press
</p>
<p>Kramer, N., Braun, M.L., 2007. Kernelizing PLS, degrees of freedom, and efficient model selection, in: Proceedings of the 24th International Conference on Machine Learning, ICML 07. Association for Computing Machinery, New York, NY, USA, pp. 441-448. https://doi.org/10.1145/1273496.1273552
</p>
<p>Kramer, N., Sugiyama, M., 2011. The Degrees of Freedom of Partial Least Squares Regression. Journal of the American Statistical Association 106, 697-705. https://doi.org/10.1198/jasa.2011.tm10107
</p>
<p>Kramer, N., Braun, M. L. 2019. plsdof: Degrees of Freedom and Statistical Inference for Partial Least Squares Regression. R package version 0.2-9. https://cran.r-project.org
</p>
<p>Lesnoff, M., Roger, J.M., Rutledge, D.N., 2021. Monte Carlo methods for estimating Mallow's Cp and AIC criteria for PLSR models. Illustration on agronomic spectroscopic NIR data. Journal of Chemometrics, 35(10), e3369. https://doi.org/10.1002/cem.3369
</p>
<p>Stein, C.M., 1981. Estimation of the Mean of a Multivariate Normal
Distribution. The Annals of Statistics 9, 1135-1151.
</p>
<p>Ye, J., 1998. On Measuring and Correcting the Effects of Data Mining and Model Selection. Journal of
the American Statistical Association 93, 120-131. https://doi.org/10.1080/01621459.1998.10474094
</p>
<p>Zou, H., Hastie, T., Tibshirani, R., 2007. On the degrees of freedom of the lasso. The Annals of
Statistics 35, 2173-2192. https://doi.org/10.1214/009053607000000127
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## EXAMPLE 1

data(ozone)

z &lt;- ozone$X
u &lt;- which(!is.na(rowSums(z)))
X &lt;- z[u, -4]
y &lt;- z[u, 4]
dim(X)

Xs &lt;- scale(X)

nlv &lt;- 12
res &lt;- dfplsr_cg(Xs, y, nlv = nlv)

df.kramer &lt;- c(1.000000, 3.712373, 6.456417, 11.633565, 12.156760, 11.715101, 12.349716,
  12.192682, 13.000000, 13.000000, 13.000000, 13.000000, 13.000000)

znlv &lt;- 0:nlv
plot(znlv, res$df, type = "l", col = "red",
     ylim = c(0, 15),
     xlab = "Nb components", ylab = "df")
lines(znlv, znlv + 1, col = "grey40")
points(znlv, df.kramer, pch = 16)
abline(h = 1, lty = 2, col = "grey")
legend("bottomright", legend=c("dfplsr_cg","Naive df","df.kramer"), col=c("red","grey40","black"),
lty=c(1,1,0), pch=c(NA,NA,16), bty="n")

## EXAMPLE 2

data(ozone)

z &lt;- ozone$X
u &lt;- which(!is.na(rowSums(z)))
X &lt;- z[u, -4]
y &lt;- z[u, 4]
dim(X)

Xs &lt;- scale(X)

nlv &lt;- 12
B &lt;- 50 
u &lt;- dfplsr_cov(Xs, y, nlv = nlv, B = B)
v &lt;- dfplsr_div(Xs, y, nlv = nlv, B = B)

df.kramer &lt;- c(1.000000, 3.712373, 6.456417, 11.633565, 12.156760, 11.715101, 12.349716,
  12.192682, 13.000000, 13.000000, 13.000000, 13.000000, 13.000000)

znlv &lt;- 0:nlv
plot(znlv, u$df, type = "l", col = "red",
     ylim = c(0, 15),
     xlab = "Nb components", ylab = "df")
lines(znlv, v$df, col = "blue")                 
lines(znlv, znlv + 1, col = "grey40")
points(znlv, df.kramer, pch = 16)
abline(h = 1, lty = 2, col = "grey")
legend("bottomright", legend=c("dfplsr_cov","dfplsr_div","Naive df","df.kramer"), 
col=c("blue","red","grey40","black"),
lty=c(1,1,1,0), pch=c(NA,NA,NA,16), bty="n")

</code></pre>


</div>