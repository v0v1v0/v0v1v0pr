<div class="container">

<table style="width: 100%;"><tr>
<td>knnr</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>KNN-R</h2>

<h3>Description</h3>

<p>KNN weighted regression. For each new observation to predict, a number of <code class="reqn">k</code> nearest neighbors is selected and the prediction is calculated by the average (eventually weighted) of the response <code class="reqn">Y</code> over this neighborhood.
</p>


<h3>Usage</h3>

<pre><code class="language-R">
knnr(X, Y,
    nlvdis, diss = c("eucl", "mahal"),
    h, k)

## S3 method for class 'Knnr'
predict(object, X, ...)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>For the main function: Training X-data (<code class="reqn">n, p</code>). — For the auxiliary functions: New X-data (<code class="reqn">m, p</code>) to consider.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>Training Y-data (<code class="reqn">n, q</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlvdis</code></td>
<td>
<p>The number of LVs to consider in the global PLS used for the dimension reduction before calculating the dissimilarities. If <code>nlvdis = 0</code>, there is no dimension reduction. (see details)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>diss</code></td>
<td>
<p>The type of dissimilarity used for defining the neighbors. Possible values are "eucl" (default; Euclidean distance), "mahal" (Mahalanobis distance), or "correlation". Correlation dissimilarities are calculated by sqrt(.5 * (1 - rho)).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>h</code></td>
<td>
<p>A scale scalar defining the shape of the weight function. Lower is <code class="reqn">h</code>, sharper is the function. See <code>wdist</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>The number of nearest neighbors to select for each observation to predict.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>— For the auxiliary functions: A fitted model, output of a call to the main function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>— For the auxiliary functions: Optional arguments. Not used.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>In function <code>knnr</code>, the dissimilarities used for computing the neighborhood and the weights can be calculated from the original X-data or after a dimension reduction (argument <code>nlvdis</code>). In the last case, global PLS scores are computed from <code class="reqn">(X, Y)</code> and the dissimilarities are calculated on these scores. For high dimension X-data, the dimension reduction is in general required for using the Mahalanobis distance.   
</p>


<h3>Value</h3>

<p>For <code>knnr</code>:list with input arguments.
</p>
<p>For <code>predict.Knnr</code>:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>pred</code></td>
<td>
<p>prediction calculated for each observation by the average (eventually weighted) of the response <code class="reqn">Y</code> over its neighborhood.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>listnn</code></td>
<td>
<p>list with the neighbors used for each observation to be predicted</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>listd</code></td>
<td>
<p>list with the distances to the neighbors used for each observation to be predicted</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>listw</code></td>
<td>
<p>list with the weights attributed to the neighbors used for each observation to be predicted</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
n &lt;- 30 ; p &lt;- 10
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(ytrain, 100 * ytrain)
m &lt;- 4
Xtest &lt;- matrix(rnorm(m * p), ncol = p)
ytest &lt;- rnorm(m)
Ytest &lt;- cbind(ytest, 10 * ytest)

nlvdis &lt;- 5 ; diss &lt;- "mahal"
h &lt;- 2 ; k &lt;- 10
fm &lt;- knnr(
    Xtrain, Ytrain, 
    nlvdis = nlvdis, diss = diss,
    h = h, k = k)
res &lt;- predict(fm, Xtest)
names(res)
res$pred
msep(res$pred, Ytest)

</code></pre>


</div>