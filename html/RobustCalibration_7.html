<div class="container">

<table style="width: 100%;"><tr>
<td>Log_marginal_post</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>

Natural Logorithm of the posterior. </h2>

<h3>Description</h3>


<p>This function compute the natural Logorithm of the posterior assuming the GaSP or S-GaSP models for the discrepancy function.
</p>


<h3>Usage</h3>

<pre><code class="language-R">Log_marginal_post(param, L_cur, output,  p_theta,  p_x, X, have_mean, CL, a, b,  cm_obs
,S_2_f,num_obs_all)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>param</code></td>
<td>


<p>Current parameters in the MCMC. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>L_cur</code></td>
<td>


<p>Cholesky decomposition of the covariance matrix. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>output</code></td>
<td>

<p>Experimental observations.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>p_theta</code></td>
<td>

<p>Number of calibration parameters.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>p_x</code></td>
<td>

<p>Number of range parameters.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>

<p>Number of mean discrepancy parameters.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>have_mean</code></td>
<td>

<p>Whether the mean discrepancy is zero or not.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>CL</code></td>
<td>

<p>Prior parameter in the jointly robust prior.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>a</code></td>
<td>

<p>Prior parameter in the jointly robust prior.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>b</code></td>
<td>

<p>Prior parameter in the jointly robust prior.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cm_obs</code></td>
<td>

<p>Outputs from the mathematical model.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>S_2_f</code></td>
<td>

<p>Variance of the data. This term is useful when there are repeated experiments. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num_obs_all</code></td>
<td>

<p>Total number of observations. If there is no repeated experiment, this is equal to the number of observable inputs. 
</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Natural logorithm of the posterior assuming the GaSP or S-GaSP models for the discrepancy function. </p>


<h3>Author(s)</h3>


<p>Mengyang Gu [aut, cre]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>A. O'Hagan and M. C. Kennedy (2001), <em>Bayesian calibration of computer models</em>, <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology</em>, <b>63</b>, 425-464.
</p>
<p>Mengyang Gu. (2016). Robust Uncertainty Quantification and Scalable Computation for Computer Models with Massive Output. Ph.D. thesis. Duke University.
</p>
<p>M. Gu and L. Wang (2017) <em>Scaled Gaussian Stochastic Process for Computer Model Calibration and Prediction</em>. arXiv preprint arXiv:1707.08215.
</p>
<p>M. Gu (2018) <em>Jointly Robust Prior for Gaussian Stochastic Process in Emulation, Calibration and Variable Selection
</em>. arXiv preprint arXiv:1804.09329.
</p>


</div>