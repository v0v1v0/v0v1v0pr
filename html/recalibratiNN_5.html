<div class="container">

<table style="width: 100%;"><tr>
<td>PIT_global</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Obtain the PIT-values of a Model</h2>

<h3>Description</h3>

<p>A function to calculate the Probability Integral Transform (PIT) values for any fitted model
that assumes a normal distribution of the output.
</p>


<h3>Usage</h3>

<pre><code class="language-R">PIT_global(ycal, yhat, mse)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>ycal</code></td>
<td>
<p>Numeric vector representing the true observations (y-values) of the response variable from the calibration dataset.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>yhat</code></td>
<td>
<p>Numeric vector of predicted y-values on the calibration dataset.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mse</code></td>
<td>
<p>Mean Squared Error calculated from the calibration dataset.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function is designed to work with models that is, even implicitly, assuming normal distribution of the response variable.
This includes, but is not limited to, linear models created using <code>lm()</code> or neural networks utilizing Mean Squared Error as the loss function.
The OLS method is used to minimized residuals in these models. This mathematical optimization will also yield a probabilistic optimization
when normal distribution of the response variable is assumed, since OLS and maximum likelihood estimation are equivalent under normality.
Therefore, in order to render a probabilistic interpretation of the predictions, the model is intrinsically assuming a normal distribution of the response variable.
</p>


<h3>Value</h3>

<p>Returns a numeric vector of PIT-values.
</p>


<h3>Examples</h3>

<pre><code class="language-R">n &lt;- 10000
split &lt;- 0.8

# generating heterocedastic data
mu &lt;- function(x1){
10 + 5*x1^2
}

sigma_v &lt;- function(x1){
30*x1
}


x &lt;- runif(n, 1, 10)
y &lt;- rnorm(n, mu(x), sigma_v(x))

x_train &lt;- x[1:(n*split)]
y_train &lt;- y[1:(n*split)]

x_cal &lt;- x[(n*split+1):n]
y_cal &lt;- y[(n*split+1):n]

model &lt;- lm(y_train ~ x_train)

y_hat &lt;- predict(model, newdata=data.frame(x_train=x_cal))

MSE_cal &lt;- mean((y_hat - y_cal)^2)

PIT_global(ycal=y_cal, yhat=y_hat, mse=MSE_cal)

</code></pre>


</div>