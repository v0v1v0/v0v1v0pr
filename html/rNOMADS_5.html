<div class="container">

<table style="width: 100%;"><tr>
<td>CrawlModels</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Get Available Model Runs
</h2>

<h3>Description</h3>

<p>This function determine which instances of a given model are available for download.
</p>


<h3>Usage</h3>

<pre><code class="language-R">CrawlModels(abbrev = NULL, model.url = NULL, depth = NULL, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>abbrev</code></td>
<td>

<p>The model abbreviation, see <code>NOMADSRealTimeList</code>.
Defaults to <code>NULL</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model.url</code></td>
<td>

<p>A URL to use instead of using the abbreviations in <code>NOMADSRealTimeList</code>.
Defaults to <code>NULL</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>depth</code></td>
<td>

<p>How many model instances to return.
This avoids having to download the entire model list (sometimes several hundred) if only the first few instances are required.
Defaults to <code>NULL</code>, which returns everything.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>

<p>Print out each link as it is discovered.
Defaults to <code>TRUE</code>.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function calls <code>WebCrawler</code>, a recursive algorithm that discovers each link available in the URL provided.
It then searches each link in turn, and follows those links until it reaches a dead end.
At that point, it returns the URL.
For the model pages on the NOMADS web site, each dead end is a model instance that can be examined using <code>ParseModelPage</code> or have data retrieved from it using <code>GribGrab</code>.
</p>


<h3>Value</h3>

<table><tr style="vertical-align: top;">
<td><code>urls.out</code></td>
<td>
<p>A list of web page addresses, each of which corresponds to a model instance.</p>
</td>
</tr></table>
<h3>Note</h3>

<p>It is a good idea to set <code>depth</code> to a small number rather than leave it at the default value.
Some models (such as the Global Forecast System) have a large number of instances, and crawling each one can take a lot of time.
I recommend <code>depth = 2</code>, since the first URL may not have an active model on it yet if the model is still being uploaded to the server.
In that case,the first URL will contain no data, and the second URL can be used instead.
</p>


<h3>Author(s)</h3>

<p>Daniel C. Bowman <a href="mailto:danny.c.bowman@gmail.com">danny.c.bowman@gmail.com</a>
</p>


<h3>See Also</h3>

<p><code>WebCrawler</code>, <code>ParseModelPage</code>, <code>NOMADSRealTimeList</code>, <code>GribGrab</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
#Get the latest 5 instances
#for the Global Forecast System 0.5 degree model

## Not run: urls.out &lt;- CrawlModels(abbrev = "gfs_0p50", depth = 5)

</code></pre>


</div>