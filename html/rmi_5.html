<div class="container">

<table style="width: 100%;"><tr>
<td>lnn_mi</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Local Nearest Neighbor (LNN) MI Estimator</h2>

<h3>Description</h3>

<p>Local Nearest Neighbor (LNN) mutual information estimator by Gao et al. 2017. This estimator uses the LNN entropy (<code>lnn_entropy</code>) estimator into the mutual information identity.
</p>


<h3>Usage</h3>

<pre><code class="language-R">lnn_mi(data, splits, k = 5, tr = 30)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>Matrix of sample observations, each row is an observation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>splits</code></td>
<td>
<p>A vector that describes which sets of columns in <code>data</code> to compute the mutual information between. For example, to compute mutual information between two variables use <code>splits = c(1,1)</code>. To compute <em>redundancy</em> among multiple random variables use <code>splits = rep(1,ncol(data))</code>. To compute the mutual information between two random vector list the dimensions of each vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>Order of the local kNN bandwidth selection.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tr</code></td>
<td>
<p>Order of truncation (number of neighbors to include in the local density estimation).</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Gao, W., Oh, S., &amp; Viswanath, P. (2017). Density functional estimators with k-nearest neighbor bandwidths. IEEE International Symposium on Information Theory - Proceedings, 1, 1351â€“1355.
</p>


<h3>Examples</h3>

<pre><code class="language-R">set.seed(123)
x &lt;- rnorm(1000)
y &lt;- x + rnorm(1000)
lnn_mi(cbind(x,y),c(1,1))

</code></pre>


</div>