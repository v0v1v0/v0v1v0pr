<div class="container">

<table style="width: 100%;"><tr>
<td>MLcalc</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Calculate Efficient Maximum Likelihood (ML) point-estimates for a Linear Model that
are either Unbiased (OLS) or Most Likely to be Optimally Biased under Normal-distribution
theory.</h2>

<h3>Description</h3>

<p>Compute MSE risk-optimal point-estimates of Beta-Coefficients and their Relative MSE risks.
Much of the code for this function is identical to that of eff.ridge(), which computes
multiple points along the "Efficient" Shrinkage Path. MLcalc() restricts attention to
only two points: [1] the Unbiased OLS (BLUE) vector and [2] the Most Likely to be Optimally
Biased [Minimum MSE Risk] vector of estimates.</p>


<h3>Usage</h3>

<pre><code class="language-R">  MLcalc(form, data, rscale = 1)</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>form</code></td>
<td>
<p>A regression formula [y~x1+x2+...] suitable for use with lm().</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>Data frame containing observations on all variables in the formula.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rscale</code></td>
<td>
<p>One of three possible choices (0, 1 or 2) for "rescaling" of variables
(after being "centered") to remove all "non-essential" ill-conditioning:  0 implies no
rescaling; 1 implies divide each variable by its standard error; 2 implies rescale as
in option 1 but re-express answers as in option 0.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Ill-conditioned and/or nearly multi-collinear regression models are unlikely to
produce Ordinary Least Squares (OLS) regression coefficient estimates that are very
close, numerically, to their unknown true values.  Specifically, OLS estimates can then
tend to have "wrong" numerical signs and/or unreasonable relative magnitudes, while
shrunken (generalized ridge) estimates chosen to Maximize their Likelihood of reducing
Mean Squared Error (MSE) Risk (expected squared-error loss) can be more stable
numerically.  On the other hand, because only OLS estimates are guaranteed to be minimax
when risk is Matrix Valued (truly multivariate), no guarantee of an expected reduction in
MSE Risk is necessarily associated with "Optimal" Generalized Ridge Regression shrinkage. 
</p>


<h3>Value</h3>

<p> An output list object of class MLcalc:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>Name of the data.frame object specified as the second argument.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>form</code></td>
<td>
<p>The regression formula specified as the first argument.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>p</code></td>
<td>
<p>Number of regression predictor variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n</code></td>
<td>
<p>Number of complete observations after removal of all missing values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>r2</code></td>
<td>
<p>Numerical value of R-square goodness-of-fit statistic.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>s2</code></td>
<td>
<p>Numerical value of the residual mean square estimate of error.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prinstat</code></td>
<td>
<p>Listing of principal statistics.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gmat</code></td>
<td>
<p>Orthogonal Matrix of Direction Cosines for Principal Axes [1:p, 1:p].</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>Numerical shrinkage-ridge regression coefficient estimates [1:2, 1:p].</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rmse</code></td>
<td>
<p>Numerical MSE risk estimates for fitted coefficients [1:2, 1:p].</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dMSE</code></td>
<td>
<p>Numerical delta-factors for shrinking OLS components [1:p].</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ys</code></td>
<td>
<p>Numerical rescaling factor for y-outcome variable [1, 1].</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xs</code></td>
<td>
<p>Numerical rescaling factors for given x-variables [1:p].</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Bob Obenchain &lt;wizbob@att.net&gt;</p>


<h3>References</h3>

<p>Thompson JR. (1968) Some shrinkage techniques for estimating the mean. <em>Journal
of the American Statistical Association</em> <b>63</b>, 113-122. (The “cubic” estimator.)
</p>
<p>Obenchain RL. (2021) <em>The Efficient Shrinkage Path: Maximum Likelihood of Minimum MSE
Risk.</em> <a href="https://arxiv.org/abs/2103.05161">https://arxiv.org/abs/2103.05161</a>
</p>
<p>Obenchain RL. (2022) Efficient Generalized Ridge Regression. <em>Open Statistics</em>
<b>3</b>: 1-18. <a href="https://doi.org/10.1515/stat-2022-0108">doi:10.1515/stat-2022-0108</a>
</p>
<p>Obenchain RL. (2022) <em>RXshrink_in_R.PDF</em> RXshrink package vignette-like document,
Version 2.1. <a href="http://localcontrolstatistics.org">http://localcontrolstatistics.org</a>
</p>


<h3>See Also</h3>

<p><code>eff.ridge</code>, <code>MLboot</code>, <code>eff.aug</code></p>


</div>