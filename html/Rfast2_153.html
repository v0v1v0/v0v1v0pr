<div class="container">

<table style="width: 100%;"><tr>
<td>Hellinger distance based univariate regression for proportions</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Hellinger distance based univariate regression for proportions
</h2>

<h3>Description</h3>

<p>Hellinger distance based univariate regression for proportions.
</p>


<h3>Usage</h3>

<pre><code class="language-R">prophelling.reg(y, x, cov = FALSE, tol = 1e-07, maxiters = 100) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>

<p>The dependent variable, a numerical vector with percentages. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>A numerical matrix with the indendent variables. We add, internally, the first column of ones.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cov</code></td>
<td>

<p>Should the sandwich covariance matrix and the standard errors be returned? If yes, set this equal to TRUE.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxiters</code></td>
<td>

<p>The max number of iterations that can take place in each regression. 
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>We minimise the Jensen-Shannon divergence instead of the ordinarily used divergence, the Kullback-Leibler. 
Both of them fall under the <code class="reqn">\phi</code>-divergence class models and hance this one produces asympottically 
normal regression coefficients as well.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>be</code></td>
<td>

<p>The regression coefficients.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seb</code></td>
<td>

<p>The sandwich standard errors of the beta coefficients, if the input argument argument was set to TRUE.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>covb</code></td>
<td>

<p>The sandwich covariance matrix of the beta coefficients, if the input argument argument was set to TRUE.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>js</code></td>
<td>

<p>The final Jensen-Shannon divergence.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>H</code></td>
<td>

<p>The final Hellinger distance.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iters</code></td>
<td>

<p>The number of iterations required by Newton-Raphson.
</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tsagris, Michail (2015). A novel, divergence based, regression for compositional data. 
Proceedings of the 28th Panhellenic Statistics Conference, 15-18/4/2015, Athens, Greece.
https://arxiv.org/pdf/1511.07600.pdf 
</p>


<h3>See Also</h3>

<p><code> propols.reg, simplex.mle, kumar.mle </code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">y &lt;- rbeta(150, 3, 4)
x &lt;- iris
a &lt;- prophelling.reg(y, x)
</code></pre>


</div>