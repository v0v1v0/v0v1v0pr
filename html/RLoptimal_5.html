<div class="container">

<table style="width: 100%;"><tr>
<td>rl_config_set</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Configuration of Reinforcement Learning</h2>

<h3>Description</h3>

<p>Mainly settings for the arguments of the training() function.
Not compatible with the new API stack introduced in Ray 2.10.0.
</p>


<h3>Usage</h3>

<pre><code class="language-R">rl_config_set(
  iter = 1000L,
  save_start_iter = NULL,
  save_every_iter = NULL,
  cores = 4L,
  gamma = 1,
  lr = 5e-05,
  train_batch_size = 10000L,
  model = rl_dnn_config(),
  sgd_minibatch_size = 200L,
  num_sgd_iter = 20L,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>iter</code></td>
<td>
<p>A positive integer value. Number of iterations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>save_start_iter, save_every_iter</code></td>
<td>
<p>An integer value. Save checkpoints every
'save_every_iter' iterations starting from 'save_start_iter' or later.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cores</code></td>
<td>
<p>A positive integer value. Number of CPU cores used for learning.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>A positive numeric value. Discount factor of the Markov decision
process. Default is 1.0 (not discount).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lr</code></td>
<td>
<p>A positive numeric value. Learning rate (default 5e-5). You can set
a learning schedule instead of a learning rate.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>train_batch_size</code></td>
<td>
<p>A positive integer value. Training batch size.
Deprecated on the new API stack.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>A list. Arguments passed into the policy model. See
rl_dnn_config for details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sgd_minibatch_size</code></td>
<td>
<p>A positive integer value. Total SGD batch size
across all devices for SGD. Deprecated on the new API stack.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num_sgd_iter</code></td>
<td>
<p>A positive integer value. Number of SGD iterations in
each outer loop.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Other settings for training(). See the arguments of the training()
function in the source code of RLlib.
https://github.com/ray-project/ray/blob/master/rllib/algorithms/algorithm_config.py
https://github.com/ray-project/ray/blob/master/rllib/algorithms/ppo/ppo.py</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A list of reinforcement learning configuration parameters
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
allocation_rule &lt;- learn_allocation_rule(
  models, 
  N_total = 150, N_ini = rep(10, 5), N_block = 10, Delta = 1.3,
  outcome_type = "continuous", sd_normal = sqrt(4.5), 
  seed = 123, 
  # We change `iter` to 200 and `cores` for reinforcement learning to 2
  rl_config = rl_config_set(iter = 200, cores = 2), 
  alpha = 0.025
)
## End(Not run) 

</code></pre>


</div>