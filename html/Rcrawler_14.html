<div class="container">

<table style="width: 100%;"><tr>
<td>RobotParser</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>RobotParser fetch and parse robots.txt</h2>

<h3>Description</h3>

<p>This function fetch and parse robots.txt file of the website which is specified in the first argument and return the list of correspending rules .
</p>


<h3>Usage</h3>

<pre><code class="language-R">RobotParser(website, useragent)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>website</code></td>
<td>
<p>character, url of the website which rules have to be extracted  .</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>useragent</code></td>
<td>
<p>character, the useragent of the crawler</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>return a list of three elements, the first is a character vector of Disallowed directories, the third is a Boolean value which is TRUE if the user agent of the crawler is blocked.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
#RobotParser("http://www.glofile.com","AgentX")
#Return robot.txt rules and check whether AgentX is blocked or not.


</code></pre>


</div>