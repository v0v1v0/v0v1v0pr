<div class="container">

<table style="width: 100%;"><tr>
<td>RcppHMM-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Overview of Package RcppHMM
</h2>

<h3>Description</h3>

<p>This package can model observations based on hidden Markov models. The observations can be considered to be emitted by a multinomial distribution, A mixture of Gaussians or a mixture of Poissons. It can be used for inference, parameter estimation and simulation.  
</p>


<h3>Details</h3>

<p>The package can be used to represent a discrete-time hidden Markov model. The states can generate categorical (labeled), continuous or discrete observations. The hidden state transition and observations can be randomly generated based on fixed parameters. Also, the inference methods can be used to evaluate sequences or decode the hidden states that generated the observations. Finally, the model parameters can be estimated by a single or multiple observed sequences.  
</p>


<h3>Author(s)</h3>

<p>Roberto A. Cardenas-Ovando, Julieta Noguez and Claudia Rangel-Escareno
</p>
<p>Maintainer: Roberto A. Cardenas-Ovando &lt;robalecarova@gmail.com&gt;
</p>


<h3>References</h3>

<p>Bilmes, J.E. (1998). A Gentle Tutorial of the EM Algorithm and its Application to Parameter Estimation for Gaussian Mixture and Hidden Markov Models. <em>International Computer Science Institute</em>.
</p>
<p>Ibe, O. (2009). Markov processes for stochastic modeling. <em>Oxford</em>.
</p>
<p>Rabiner, L.R. (1989). A tutorial on hidden Markov models and selected applications in speech recognition. <em>Proceedings of the IEEE</em>.
</p>
<p>Rabiner L.; Juang, B.H. (1993) Fundamentals of Speech Recognition. <em>Prentice Hall Signal Processing Series</em>.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Multinomial case
# Set the model parameters to be estimated
n &lt;- c("First","Second")
m &lt;- c("A","T","C","G")
A &lt;- matrix(c(0.8,0.2,
              0.1,0.9),
            nrow = 2,
            byrow = TRUE)

B &lt;- matrix(c(0.2, 0.2, 0.3, 0.3,
              0.4, 0.4, 0.1, 0.1),
            nrow = 2,
            byrow = TRUE)
            
Pi &lt;- c(0.5, 0.5)


params &lt;- list( "Model" = "HMM",
                "StateNames" = n,
                "ObservationNames" = m,
                "A" = A,
                "B" = B,
                "Pi" = Pi)

# Model parameters validation

HMM &lt;- verifyModel(params)

# Data simulation
# Multiple sequences

set.seed(100)
length &lt;- 100
seqs &lt;- 100
observationSequences&lt;- c()
for(i in 1:seqs){
  Y &lt;- generateObservations(HMM , length)$Y
  observationSequences &lt;- rbind(observationSequences , Y)
}

# New model random initialization

set.seed(1000)
newModel &lt;- initHMM(2,4) 
n = c("X1","X2")
m = c("A","T","C","G")

# Change model names

newModel &lt;- setNames(newModel,
                        list( "StateNames" = n,
                              "ObservationNames" = m) )

# Model parameters estimation

newModel &lt;- learnEM(newModel,
        observationSequences,
        iter=300, 
        delta = 1E-8,
        pseudo = 0,
        print = TRUE)

# New sequence simulation to compare the new model
# Data simulation

# Single sequence
Y &lt;- generateObservations(HMM , length)$Y

# Evaluation

evaluation(newModel, Y, "f")
evaluation(newModel, Y, "b")

# Hidden state decoding

hiddenStatesViterbi &lt;- viterbi(newModel, Y)
hiddenStatesFB &lt;- forwardBackward( newModel, Y)
</code></pre>


</div>