<div class="container">

<table style="width: 100%;"><tr>
<td>SosDiscRobust</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Robust and sparse multigroup classification by the optimal scoring approach
</h2>

<h3>Description</h3>

<p>Robust and sparse multigroup classification by the optimal scoring approach is robust against outliers, provides a low-dimensional and sparse representation of the predictors and is also applicable if the number of variables exeeds the number of observations.
</p>


<h3>Usage</h3>

<pre><code class="language-R">SosDiscRobust(x, ...)
## Default S3 method:
SosDiscRobust(x, grouping, prior=proportions, 
    lambda, Q=length(unique(grouping))-1, alpha=0.5, maxit=100, 
    tol = 1.0e-4, trace=FALSE, ...)
## S3 method for class 'formula'
SosDiscRobust(formula, data = NULL, ..., subset, na.action)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>A formula of the form <code>y~x</code>, it describes the response
and the predictors. The formula can be more complicated, such as
<code>y~log(x)+z</code> etc (see <code>formula</code> for more details).
The response should
be a factor representing the response variable, or any vector
that can be coerced to such (such as a logical variable).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>An optional data frame (or similar: see
<code>model.frame</code>) containing the variables in the
formula <code>formula</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>subset</code></td>
<td>
<p>An optional vector used to select rows (observations) of the
data matrix <code>x</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na.action</code></td>
<td>
<p>A function which indicates what should happen
when the data contain <code>NA</code>s.  The default is set by
the <code>na.action</code> setting of <code>options</code>, and is
<code>na.fail</code> if that is unset. The default is <code>na.omit</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A matrix or data frame containing the explanatory variables (training set); 
colnames of x have to be provided. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>grouping</code></td>
<td>
<p>Grouping variable:  a factor specifying the class for each observation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prior</code></td>
<td>
<p>Prior probabilities, a vector of positive numbers that sum up to 1;
default to the class proportions for the training set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>A non-negative tuning parameter for L1 norm penalty introducing sparsity on the 
optimal scoring coefficients <code class="reqn">\boldsymbol{\beta}_h</code> (see Details). 
If the number of variables exceeds the number of observations <code>lambda</code> has to be positive.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Q</code></td>
<td>
<p>Number of optimal scoring coefficient vectors; <code>Q</code> has to be smaller than the number of groups. 
Defaults to number of groups - 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>Robustness parameter used in sparseLTS (for initial estimation, see Details). Default <code>alpha=0.5</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p>Number of iterations for the estimation of optimal scoring coefficients and case weights. Default <code>maxit=100</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>Tolerance for convergence of the normed weighted change in the residual sum of squares
for the estiamtion of optimal scoring coefficeints. Default is <code>tol=1.0e-4</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trace</code></td>
<td>
<p>Whether to print intermediate results. Default is <code>trace = FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Arguments passed to or from other methods.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The sparse optimal scoring problem (Clemmensen et al, 2011):
for <code class="reqn">h=1,....,Q</code>
</p>
<p style="text-align: center;"><code class="reqn">
\min_{\beta_h,\theta_h} \frac{1}{n} \|Y \theta_h - X \beta_h \|_2^2   + \lambda \|\beta_h\|_1
</code>
</p>

<p>subject to
</p>
<p style="text-align: center;"><code class="reqn">
\frac{1}{n} \theta_h^T Y^T Y\theta_h=1, \quad \theta_h^T Y^T Y \theta_l=0 \quad \forall l&lt;h,
</code>
</p>

<p>where <code class="reqn">X</code> deontes the robustly centered and scaled input matrix <code>x</code> (or alternativly the predictors from <code>formular</code>) and <code class="reqn">Y</code> is an dummy matrix coding die classmemberships from <code>grouping</code>.
</p>
<p>For each <code class="reqn">h</code> this problem can be solved interatively for <code class="reqn">\beta_h</code> and <code class="reqn">\theta_h</code>. In order to obtain robust estimates, <code class="reqn">\beta_h</code> is estimated with reweighted sparse least trimmed squares regression (Alfons et al, 2013) and <code class="reqn">\theta_h</code> with least absolut deviation regression in the first two iterations. To speed up the following repetitions an iterative down-weighting of observations with large residuals is combined with the iterative estimation of the optimal scoring coefficients with their classical estimates.
</p>
<p>The classification model is estimated on the low dimensional sparse subspace <code class="reqn">X[\beta_1,...,\beta_Q]</code> with robust LDA (<code>Linda</code>).
</p>


<h3>Value</h3>

<p>An S4 object of class <code>SosDiscRobust-class</code> which is a subclass of of the 
virtual class <code>SosDisc-class</code>. 
</p>


<h3>Author(s)</h3>

 
<p>Irene Ortner <a href="mailto:irene.ortner@applied-statistics.at">irene.ortner@applied-statistics.at</a> and Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> 
</p>


<h3>References</h3>

<p>Clemmensen L, Hastie T, Witten D &amp; Ersboll B (2011),
Sparse discriminant analysis.
<em>Technometrics</em>, <b>53</b>(4), 406–413. 
</p>
<p>Alfons A, Croux C &amp; Gelper S (2013),
Sparse least trimmed squares regression for analysing high-dimensional large data sets.
<em>The Annals of Applied Statistics</em>, <b>7</b>(1), 226–248.
</p>
<p>Ortner I, Filzmoser P &amp; Croux C (2020),
Robust and sparse multigroup classification by the optimal scoring approach.
Data Mining and Knowledge Discovery <b>34</b>, 723–741.
<a href="https://doi.org/10.1007/s10618-019-00666-8">doi:10.1007/s10618-019-00666-8</a>.     
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## EXAMPLE 1 ######################################
data(olitos)
grind &lt;- which(colnames(olitos)=="grp")

set.seed(5008642)
mod &lt;- SosDiscRobust(grp~., data=olitos, lambda=0.3, maxIte=30, Q=3, tol=1e-2)

pred &lt;- predict(mod, newdata=olitos[,-grind])

summary(mod)
plot(mod, ind=c(1:3))


## EXAMPLE 2 ######################################
##

## Not run: 
library(sparseLDA)
data(penicilliumYES)

## for demonstration only:
set.seed(5008642)
X &lt;- penicilliumYES$X[, sample(1:ncol(penicilliumYES$X), 100)]

## takes a subsample of the variables
## to have quicker computation time

colnames(X) &lt;- paste0("V",1:ncol(X))
y &lt;- as.factor(c(rep(1,12), rep(2,12), rep(3,12)))

set.seed(5008642)
mod &lt;- SosDiscRobust(X, y, lambda=1, maxit=5, Q=2, tol=1e-2)

summary(mod)
plot(mod)

## End(Not run)

</code></pre>


</div>