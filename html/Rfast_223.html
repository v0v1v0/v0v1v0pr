<div class="container">

<table style="width: 100%;"><tr>
<td>k nearest neighbours algorithm (k-NN)</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
k nearest neighbours algorithm (k-NN)
</h2>

<h3>Description</h3>

<p>k nearest neighbours algorithm (k-NN). 
</p>


<h3>Usage</h3>

<pre><code class="language-R">knn(xnew, y, x, k, dist.type = "euclidean", type = "C", method = "average", 
freq.option = 0, mem.eff = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>xnew</code></td>
<td>

<p>The new data, new predictor variable values. A matrix with numerical data. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>

<p>A vector with the response variable, whose values for the new data we wish to predict. This can be 
numerical data, factor or discrete, 0, 1, ... The latter two cases are for classification.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>The dataset. A matrix with numerical data.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>

<p>The number of nearest neighbours to use. The number can either be a single value or a vector with multiple values.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dist.type</code></td>
<td>

<p>The type of distance to be used. Either \"euclidean\" or \"manhattan\". 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>

<p>If your response variable \"y\" is numerical data, then this should be \"R\" (regression). If \"y\" is in general 
categorical, factor or discrete set this argument to \"C\" (classification).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>

<p>In case you have regression (type = \"R\") you want a way to summarise the prediction. If you want to take the 
average of the reponses of the k closest observations, type \"average\". For the median, type \"median\" and for the 
harmonic mean, type \"harmonic\".
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>freq.option</code></td>
<td>

<p>If classification (type = \"C\") and ties occur in the prediction, more than one class has the same number 
of k nearest neighbours, in which case there are two strategies available: Option 0 selects the first most frequent encountered. Option 1 randomly selects the most frequent value, in the case that there are duplicates. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mem.eff</code></td>
<td>

<p>Boolean value indicating a conservative or not use of memory. Lower usage of memory/Having this option on will lead to a slight 
decrease in execution speed and should ideally be on when the amount of memory in demand might be a concern.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The concept behind k-NN is simple. Suppose we have a matrix with predictor variables and a vector with the 
response variable (numerical or categorical). When a new vector with observations (predictor variables) is 
available, its corresponding response value, numerical or category is to be predicted. Instead of using a 
model, parametric or not, one can use this ad hoc algorithm. 
</p>
<p>The k smallest distances between the new predictor variables and the existing ones are calculated. In the 
case of regression, the average, median or harmonic mean of the corresponding respone values of these closest
predictor values are calculated. In the case of classification, i.e. categorical response value, a voting rule 
is applied. The most frequent group (response value) is where the new observation is to be allocated. 
</p>


<h3>Value</h3>

<p>A matrix whose number of columns is equal to the size of k. If in the input you provided there is just one value of
k, then a matrix with one column is returned containing the predicted values. If more than one value was supplied,
the matrix will contain the predicted values for every value of k. 
</p>


<h3>Author(s)</h3>

<p>Marios Dimitriadis
</p>
<p>R implementation and documentation: Marios Dimitriadis &lt;kmdimitriadis@gmail.com&gt;
</p>


<h3>References</h3>

<p>Cover TM and Hart PE (1967). Nearest neighbor pattern classification. IEEE Transactions on 
Information Theory. 13(1):21-27.
</p>
<p>Friedman J., Hastie T. and Tibshirani R. (2017). The elements of statistical learning. 
New York: Springer.
</p>
<p>http://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf 	
</p>
<p>http://statlink.tripod.com/id3.html
</p>


<h3>See Also</h3>

<p><code>knn.cv, dirknn, logistic_only, fs.reg, cor.fsreg </code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Simulate a dataset with continuous data
x &lt;- as.matrix(iris[, 1:4])
y &lt;- as.numeric(iris[, 5])
id &lt;- sample(1:150, 120)
mod &lt;- knn(x[-id, ], y[id], x[id, ], k = c(4, 5, 6), type = "C", mem.eff = FALSE)
mod # Predicted values of y for 3 values of k. 
res&lt;-table(mod[, 1], y[-id] )  # Confusion matrix for k = 4
res&lt;-table(mod[, 2], y[-id] )  # Confusion matrix for k = 5
res&lt;-table(mod[, 3], y[-id] )  # Confusion matrix for k = 6
</code></pre>


</div>