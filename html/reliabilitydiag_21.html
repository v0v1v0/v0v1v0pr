<div class="container">

<table style="width: 100%;"><tr>
<td>summary.reliabilitydiag</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Decomposing scores into miscalibration, discrimination and uncertainty</h2>

<h3>Description</h3>

<p>An object of class <code>reliabilitydiag</code> contains the observations, the
original forecasts, and recalibrated forecasts given by isotonic regression.
The function <code>summary.reliabilitydiag</code> calculates quantitative measures
of predictive performance, miscalibration, discrimination,
and uncertainty, for each of the prediction methods in relation to their
recalibrated version.
</p>


<h3>Usage</h3>

<pre><code class="language-R">## S3 method for class 'reliabilitydiag'
summary(object, ..., score = "brier")
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>an object inheriting from the class <code>'reliabilitydiag'</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>score</code></td>
<td>
<p>currently only "brier" or a vectorized scoring function,
that is, <code>function(observation, prediction)</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Predictive performance is measured by the mean score of the original
forecast values, denoted by <code class="reqn">S</code>.
</p>
<p>Uncertainty, denoted by <code class="reqn">UNC</code>, is the mean score of a constant
prediction at the value of the average observation.
It is the highest possible mean score of a calibrated prediction method.
</p>
<p>Discrimination, denoted by <code class="reqn">DSC</code>, is <code class="reqn">UNC</code> minus the mean score
of the PAV-recalibrated forecast values.
A small value indicates a low information content (low signal) in the
original forecast values.
</p>
<p>Miscalibration, denoted by <code class="reqn">MCB</code>, is <code class="reqn">S</code> minus the mean score
of the PAV-recalibrated forecast values.
A high value indicates that predictive performance of the prediction method
can be improved by recalibration.
</p>
<p>These measures are related by the following equation,
</p>
<p style="text-align: center;"><code class="reqn">S = MCB - DSC + UNC.</code>
</p>

<p>Score decompositions of this type have been studied extensively, but the
optimality of the PAV solution ensures that <code class="reqn">MCB</code> is nonnegative,
regardless of the chosen (admissible) scoring function.
This is a unique property achieved by choosing PAV-recalibration.
</p>
<p>If deviating from the Brier score as performance metric, make sure to choose
a proper scoring rule for binary events, or equivalently,
a scoring function with outcome space {0, 1} that is consistent for the
expectation functional.
</p>


<h3>Value</h3>

<p>A <code>'summary.reliability'</code> object, which is also a
tibble (see <code>tibble::tibble()</code>) with columns:
</p>

<table>
<tr>
<td style="text-align: left;">
   <code>forecast</code> </td>
<td style="text-align: left;"> the name of the prediction method.</td>
</tr>
<tr>
<td style="text-align: left;">
   <code>mean_score</code> </td>
<td style="text-align: left;"> the mean score of the original
     forecast values.</td>
</tr>
<tr>
<td style="text-align: left;">
   <code>miscalibration</code> </td>
<td style="text-align: left;"> a measure of miscalibration
     (<em>how reliable is the prediction method?</em>),
      smaller is better.</td>
</tr>
<tr>
<td style="text-align: left;">
   <code>discrimination</code> </td>
<td style="text-align: left;"> a measure of discrimination
     (<em>how variable are the recalibrated predictions?</em>),
     larger is better.</td>
</tr>
<tr>
<td style="text-align: left;">
   <code>uncertainty</code> </td>
<td style="text-align: left;"> the mean score of a constant prediction at the
     value of the average observation.
 </td>
</tr>
</table>
<h3>Examples</h3>

<pre><code class="language-R">data("precip_Niamey_2016", package = "reliabilitydiag")
r &lt;- reliabilitydiag(
  precip_Niamey_2016[c("Logistic", "EMOS", "ENS", "EPC")],
  y = precip_Niamey_2016$obs,
  region.level = NA
)
summary(r)
summary(r, score = function(y, x) (x - y)^2)

</code></pre>


</div>