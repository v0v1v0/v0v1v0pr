<div class="container">

<table style="width: 100%;"><tr>
<td>meanCenter</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>meanCenter</h2>

<h3>Description</h3>

<p>meanCenter selectively centers or standarizes variables in a regression model.
</p>


<h3>Usage</h3>

<pre><code class="language-R">meanCenter(
  model,
  centerOnlyInteractors = TRUE,
  centerDV = FALSE,
  standardize = FALSE,
  terms = NULL
)

## Default S3 method:
meanCenter(
  model,
  centerOnlyInteractors = TRUE,
  centerDV = FALSE,
  standardize = FALSE,
  terms = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>a fitted regression model (presumably from lm)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>centerOnlyInteractors</code></td>
<td>
<p>Default TRUE. If FALSE, all numeric
predictors in the regression data frame are centered before
the regression is conducted.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>centerDV</code></td>
<td>
<p>Default FALSE. Should the dependent variable be
centered? Do not set this option to TRUE unless the dependent
variable is a numeric variable. Otherwise, it is an error.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>standardize</code></td>
<td>
<p>Default FALSE. Instead of simply mean-centering
the variables, should they also be "standardized" by first
mean-centering and then dividing by the estimated standard
deviation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>terms</code></td>
<td>
<p>Optional. A vector of variable names to be
centered. Supplying this argument will stop meanCenter from
searching for interaction terms that might need to be
centered.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Works with "lm" class objects, objects estimated by <code>glm()</code>. This
centers some or all of the the predictors and then re-fits the
original model with the new variables. This is a convenience to
researchers who are often urged to center their predictors.  This
is sometimes suggested as a way to ameliorate multi-collinearity
in models that include interaction terms (Aiken and West, 1991;
Cohen, et al 2002). Mean-centering may enhance interpretation of
the regression intercept, but it actually does not help with
multicollinearity.  (Echambadi and Hess, 2007). This function
facilitates comparison of mean-centered models with others by
calculating centered variables.  The defaults will cause a
regression's numeric interactive variables to be mean
centered. Variations on the arguments are discussed in details.
</p>
<p>Suppose the user's formula that fits the original model is
<code>m1 &lt;- lm(y ~ x1*x2 + x3 + x4, data = dat)</code>. The fitted model
will include estimates for predictors <code>x1</code>, <code>x2</code>,
<code>x1:x2</code>, <code>x3</code> and <code>x4</code>. By default,
<code>meanCenter(m1)</code> scans the output to see if there are
interaction terms of the form <code>x1:x2</code>. If so, then x1 and x2
are replaced by centered versions (m1-mean(m1)) and
(m2-mean(m2)). The model is re-estimated with those new variables.
model (the main effect and the interaction). The resulting thing
is "just another regression model", which can be analyzed or
plotted like any R regression object.
</p>
<p>The user can claim control over which variables are centered in
several ways. Most directly, by specifying a vector of variable
names, the user can claim direct control. For example, the
argument <code>terms=c("x1","x2","x3")</code> would cause 3 predictors
to be centered. If one wants all predictors to be centered, the
argument <code>centerOnlyInteractors</code> should be set to
FALSE. Please note, this WILL NOT center factor variables. But it
will find all numeric predictors and center them.
</p>
<p>The dependent variable will not be centered, unless the user
explicitly requests it by setting centerDV = TRUE.
</p>
<p>As an additional convenience to the user, the argument
<code>standardize = TRUE</code> can be used.  This will divide each
centered variable by its observed standard deviation. For people
who like standardized regression, I suggest this is a better
approach than the <code>standardize</code> function (which is brain-dead
in the style of SPSS). meanCenter with <code>standardize = TRUE</code>
will only try to standardize the numeric predictors.
</p>
<p>To be completely clear, I believe mean-centering is not helpful
with the multicollinearity problem. It doesn't help, it doesn't
hurt.  Only a misunderstanding leads its proponents to claim
otherwise. This is emphasized in the vignette "rockchalk" that is
distributed with this package.
</p>


<h3>Value</h3>

<p>A regression model of the same type as the input model,
with attributes representing the names of the centered variables.
</p>


<h3>Author(s)</h3>

<p>Paul E. Johnson <a href="mailto:pauljohn@ku.edu">pauljohn@ku.edu</a>
</p>


<h3>References</h3>

<p>Aiken, L. S. and West, S.G. (1991). Multiple
Regression: Testing and Interpreting Interactions. Newbury
Park, Calif: Sage Publications.
</p>
<p>Cohen, J., Cohen, P., West, S. G., and Aiken, L. S. (2002). Applied
Multiple Regression/Correlation Analysis for the Behavioral Sciences
(Third.). Routledge Academic.
</p>
<p>Echambadi, R., and Hess, J. D. (2007). Mean-Centering Does Not Alleviate
Collinearity Problems in Moderated Multiple Regression Models.
Marketing Science, 26(3), 438-445.
</p>


<h3>See Also</h3>

<p><code>standardize</code>
<code>residualCenter</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
library(rockchalk)
N &lt;- 100
dat &lt;- genCorrelatedData(N = N, means = c(100, 200), sds = c(20, 30),
                         rho = 0.4, stde = 10)
dat$x3 &lt;- rnorm(100, m = 40, s = 4)

m1 &lt;- lm(y ~ x1 * x2 + x3, data = dat)
summary(m1)
mcDiagnose(m1)

m1c &lt;- meanCenter(m1)
summary(m1c)
mcDiagnose(m1c)

m2 &lt;- lm(y ~ x1 * x2 + x3, data = dat)
summary(m2)
mcDiagnose(m2)

m2c &lt;- meanCenter(m2, standardize = TRUE)
summary(m2c)
mcDiagnose(m2c)

m2c2 &lt;- meanCenter(m2, centerOnlyInteractors = FALSE)
summary(m2c2)

m2c3 &lt;- meanCenter(m2, centerOnlyInteractors = FALSE, centerDV = TRUE)
summary(m2c3)

dat &lt;- genCorrelatedData(N = N, means = c(100, 200), sds = c(20, 30),
                         rho = 0.4, stde = 10)
dat$x3 &lt;- rnorm(100, m = 40, s = 4)
dat$x3 &lt;- gl(4, 25, labels = c("none", "some", "much", "total"))

m3 &lt;- lm(y ~ x1 * x2 + x3, data = dat)
summary(m3)
## visualize, for fun
plotPlane(m3, "x1", "x2")

m3c1 &lt;- meanCenter(m3)
summary(m3c1)

## Not exactly the same as a "standardized" regression because the
## interactive variables are centered in the model frame,
## and the term "x1:x2" is never centered again.
m3c2 &lt;- meanCenter(m3, centerDV = TRUE,
                   centerOnlyInteractors = FALSE, standardize = TRUE)
summary(m3c2)

m3st &lt;- standardize(m3)
summary(m3st)

## Make a bigger dataset to see effects better
N &lt;- 500
dat &lt;- genCorrelatedData(N = N, means = c(200,200), sds = c(60,30),
                         rho = 0.2, stde = 10)
dat$x3 &lt;- rnorm(100, m = 40, s = 4)
dat$x3 &lt;- gl(4, 25, labels = c("none", "some", "much", "total"))
dat$y2 &lt;- with(dat,
               0.4 - 0.15 * x1 + 0.04 * x1^2 -
               drop(contrasts(dat$x3)[dat$x3, ] %*% c(-1.9, 0, 5.1))  +
               1000* rnorm(nrow(dat)))
dat$y2 &lt;- drop(dat$y2)

m4literal &lt;- lm(y2 ~ x1 + I(x1*x1) + x2 + x3, data = dat)
summary(m4literal)
plotCurves(m4literal, plotx="x1")
## Superficially, there is multicollinearity (omit the intercept)
cor(model.matrix(m4literal)[ -1 , -1 ])

m4literalmc &lt;- meanCenter(m4literal, terms = "x1")
summary(m4literalmc)

m4literalmcs &lt;- meanCenter(m4literal, terms = "x1", standardize = TRUE)
summary(m4literalmcs)

m4 &lt;- lm(y2 ~ poly(x1, 2, raw = TRUE) + x2 + x3, data = dat)
summary(m4)
plotCurves(m4, plotx="x1")

m4mc1 &lt;- meanCenter(m4, terms = "x1")
summary(m4mc1)

m4mc2 &lt;- meanCenter(m4, terms = "x1", standardize = TRUE)
summary(m4mc2)

m4mc3 &lt;- meanCenter(m4, terms = "x1", centerDV = TRUE, standardize = TRUE)
summary(m4mc3)
</code></pre>


</div>