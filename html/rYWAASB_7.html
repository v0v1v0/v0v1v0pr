<div class="container">

<table style="width: 100%;"><tr>
<td>PCA_biplot</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>The PCA biplot with loadings</h2>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#stable"><img src="../help/figures/lifecycle-stable.svg" alt="[Stable]"></a>
</p>

<ul><li> <p><code>PCA_biplot()</code> creates the PCA (Principal Component
Analysis) biplot with loadings for the new index <code>rYWAASB</code>
for simultaneous selection of genotypes by trait and WAASB index.
It shows <code>rYWAASB</code>, <code>rWAASB</code> and <code>rWAASBY</code> indices (r: ranked) in a
biplot, simultaneously for a better differentiation of genotypes.
In PCA biplots controlling the color of variable using their
contrib i.e. contributions and cos2 takes place.
</p>
</li></ul>
<h3>Usage</h3>

<pre><code class="language-R">PCA_biplot(datap)
</code></pre>


<h3>Arguments</h3>

<table><tr style="vertical-align: top;">
<td><code>datap</code></td>
<td>
<p>The data set</p>
</td>
</tr></table>
<h3>Details</h3>

<p><em>PCA</em> is a machine learning method and dimension
reduction technique.
It is utilized to simplify large data sets by extracting
a smaller set that preserves significant patterns and
trends(1).
According to Johnson and Wichern (2007), a PCA explains
the var-covar structure of a set of variables
<script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
\(X_1, X_2, ..., X_p\) with a less <code>linear</code>
combinations of such variables. Moreover the common
objective of PCA is 1) data reduction and 2) interpretation.
</p>
<p><em>Biplot and PCA</em>:
The biplot is a method used to visually represent
both the rows and columns of a data table. It involves
approximating the table using a two-dimensional matrix
product, with the aim of creating a plane that represents
the rows and columns.
The techniques used in a biplot typically involve an eigen
decomposition, similar to the one used in PCA. It is common
for the biplot to be conducted using mean-centered and scaled
data(2).
</p>
<p><em>Algebra of PCA:</em>
As Johnson and Wichern (2007) stated(3), if the random vector
\(\mathbf{X'} = {X_1, X_2,...,X_p }\) have the
covariance matrix \(\sum\) with eigenvalues
\(
\lambda_1 \ge \lambda_2 \ge ... \ge \lambda_p \ge 0\).
</p>
<p>Regarding the linear combinations:
\[Y_1 = a'_1X = a_{11}X_1 + a_{12}X_2 + ... + a_{1P}X_p \]
\[Y_2 = a'_2X = a_{21}X_1 + a_{22}X_2 + ... + a_{2p}X_p\]
\[...\]
\[Y_p = a'_pX = a_{p1}X_1 + a_{p2}X_2 + ... + a_{pp}X_p\]
</p>
<p>where \(Var(Y_i) = \mathbf{a'_i\sum{a_i}}\) ,
i = 1, 2, ..., p
\(Cov(Y_i, Y_k) = \mathbf{a'_i\sum{a_k}}\) ,
i, k = 1, 2, ..., p
</p>
<p>The principal components refer to the uncorrelated
linear combinations
\(Y_1, Y_2, ..., Y_p\) which aim to have the
largest possible variances.
</p>
<p>For the random vector \(\mathbf{X'}=\left [ X_1, X_2, ...,
X_p \right ]\),
if \(\mathbf{\sum}\) be the associated covariance matrix, then
\(\mathbf{\sum}\) have the eigenvalue-eigenvector pairs
\((\lambda_1, e_1), (\lambda_2, e_2), ..., (\lambda_p, e_p)\),
and as said \(\lambda_1 \ge \lambda_2 \ge ... \ge \lambda_p \ge 0\).
</p>
<p>Then the \(\it{i}\)th principal component is as follows:
\[Y_i = \mathbf{e'_iX} = e_{i1}X_1 + e_{i2}X_2 + ... + e_{ip}X_p,
i = 1, 2, ..., p\], where \(Var(Y_i) =\mathbf(e'_i\sum{e_i}) = \lambda_i,
i = 1, 2, ..., p\)
\(Cov(Y_i, Y_k) = \mathbf{e'_i\sum e_i = 0, i \not\equiv k}\), and:
\(\sigma_{11} + \sigma_{22} + ... + \sigma_{pp} =
\sum_{i=1}^p{Var(X_i)} = \lambda_1 +
\lambda_2 + ... + \lambda_p = \sum_{i=1}^p{Var(Y_i)}\).
</p>
<p>Interestingly, Total population variance = \(\sigma_{11} + \sigma_{22}
+ ... + \sigma_{pp} = \lambda_1 + \lambda_2 + ... + \lambda_{p}\).
</p>
<p>Another issues that are significant in PCA analysis are:
</p>

<ol>
<li>
<p> The proportion of total variance due to (explained by)
the \(\mathit{k}\)th principal component:
\[\frac{\lambda_k}{(\lambda_1 + \lambda_2 + ... +
\lambda_p)},  k=1, 2, ..., p\]
</p>
</li>
<li>
<p> The correlation coefficients between the components \(Y_i\)
and the variables \(X_k\) is as follows:
\(\rho_{Y_i, X_k} = \frac{e_{ik}\sqrt{\lambda_i}}{\sqrt{\sigma_{kk}}}\),
i,k = 1, 2, ..., p
</p>
</li>
</ol>
<p>Please note that PCA can be performed on <code>Covariance</code> or
<code style="white-space: pre;">⁠correlation matrices⁠</code>.
And before PCA the data should be centered, generally.
</p>


<h3>Value</h3>

<p>Returns a a list of dataframes
</p>


<h3>Author(s)</h3>


<p>Ali Arminian <a href="mailto:abeyran@gmail.com">abeyran@gmail.com</a>

</p>


<h3>References</h3>

<p>(1) <a href="https://builtin.com">https://builtin.com</a>
</p>
<p>(2) <a href="https://pca4ds.github.io/biplot-and-pca.html">https://pca4ds.github.io/biplot-and-pca.html</a>.
</p>
<p>(3) Johnson, R.A. and Wichern, D.W. 2007. Applied
Multivariate Statistical Analysis. Pearson Prentice
Hall. 773 p.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
data(maize)
PCA_biplot(maize)

</code></pre>


</div>