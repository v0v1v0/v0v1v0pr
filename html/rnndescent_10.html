<div class="container">

<table style="width: 100%;"><tr>
<td>random_knn</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Find nearest neighbors by random selection</h2>

<h3>Description</h3>

<p>Create a neighbor graph by randomly selecting neighbors. This is not a useful
nearest neighbor method on its own, but can be used with other methods which
require initialization, such as <code>nnd_knn()</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">random_knn(
  data,
  k,
  metric = "euclidean",
  use_alt_metric = TRUE,
  order_by_distance = TRUE,
  n_threads = 0,
  verbose = FALSE,
  obs = "R"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>Matrix of <code>n</code> items to generate random neighbors for, with
observations in the rows and features in the columns. Optionally, input can
be passed with observations in the columns, by setting <code>obs = "C"</code>, which
should be more efficient. Possible formats are <code>base::data.frame()</code>,
<code>base::matrix()</code> or <code>Matrix::sparseMatrix()</code>. Sparse matrices should be in
<code>dgCMatrix</code> format. Dataframes will be converted to <code>numerical</code> matrix
format internally, so if your data columns are <code>logical</code> and intended to be
used with the specialized binary <code>metric</code>s, you should convert it to a
logical matrix first (otherwise you will get the slower dense numerical
version).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>Number of nearest neighbors to return.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metric</code></td>
<td>
<p>Type of distance calculation to use. One of:
</p>

<ul>
<li> <p><code>"braycurtis"</code>
</p>
</li>
<li> <p><code>"canberra"</code>
</p>
</li>
<li> <p><code>"chebyshev"</code>
</p>
</li>
<li> <p><code>"correlation"</code> (1 minus the Pearson correlation)
</p>
</li>
<li> <p><code>"cosine"</code>
</p>
</li>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"euclidean"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"hellinger"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"jensenshannon"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"sqeuclidean"</code> (squared Euclidean)
</p>
</li>
<li> <p><code>"manhattan"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"spearmanr"</code> (1 minus the Spearman rank correlation)
</p>
</li>
<li> <p><code>"symmetrickl"</code> (symmetric Kullback-Leibler divergence)
</p>
</li>
<li> <p><code>"tsss"</code> (Triangle Area Similarity-Sector Area Similarity or TS-SS
metric)
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li>
</ul>
<p>For non-sparse data, the following variants are available with
preprocessing: this trades memory for a potential speed up during the
distance calculation. Some minor numerical differences should be expected
compared to the non-preprocessed versions:
</p>

<ul>
<li> <p><code>"cosine-preprocess"</code>: <code>cosine</code> with preprocessing.
</p>
</li>
<li> <p><code>"correlation-preprocess"</code>: <code>correlation</code> with preprocessing.
</p>
</li>
</ul>
<p>For non-sparse binary data passed as a <code>logical</code> matrix, the following
metrics have specialized variants which should be substantially faster than
the non-binary variants (in other cases the logical data will be treated as
a dense numeric vector of 0s and 1s):
</p>

<ul>
<li> <p><code>"dice"</code>
</p>
</li>
<li> <p><code>"hamming"</code>
</p>
</li>
<li> <p><code>"jaccard"</code>
</p>
</li>
<li> <p><code>"kulsinski"</code>
</p>
</li>
<li> <p><code>"matching"</code>
</p>
</li>
<li> <p><code>"rogerstanimoto"</code>
</p>
</li>
<li> <p><code>"russellrao"</code>
</p>
</li>
<li> <p><code>"sokalmichener"</code>
</p>
</li>
<li> <p><code>"sokalsneath"</code>
</p>
</li>
<li> <p><code>"yule"</code>
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use_alt_metric</code></td>
<td>
<p>If <code>TRUE</code>, use faster metrics that maintain the
ordering of distances internally (e.g. squared Euclidean distances if using
<code>metric = "euclidean"</code>), then apply a correction at the end. Probably
the only reason to set this to <code>FALSE</code> is if you suspect that some
sort of numeric issue is occurring with your data in the alternative code
path.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>order_by_distance</code></td>
<td>
<p>If <code>TRUE</code> (the default), then results for each
item are returned by increasing distance. If you don't need the results
sorted, e.g. you are going to pass the results as initialization to another
routine like <code>nnd_knn()</code>, set this to <code>FALSE</code> to save a small amount of
computational time.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_threads</code></td>
<td>
<p>Number of threads to use.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>If <code>TRUE</code>, log information to the console.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>obs</code></td>
<td>
<p>set to <code>"C"</code> to indicate that the input <code>data</code> orientation stores
each observation as a column. The default <code>"R"</code> means that observations are
stored in each row. Storing the data by row is usually more convenient, but
internally your data will be converted to column storage. Passing it
already column-oriented will save some memory and (a small amount of) CPU
usage.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>a random neighbor graph as a list containing:
</p>

<ul>
<li> <p><code>idx</code> an n by k matrix containing the nearest neighbor indices.
</p>
</li>
<li> <p><code>dist</code> an n by k matrix containing the nearest neighbor distances.
</p>
</li>
</ul>
<h3>Examples</h3>

<pre><code class="language-R"># Find 4 random neighbors and calculate their Euclidean distance
# If you pass a data frame, non-numeric columns are removed
iris_nn &lt;- random_knn(iris, k = 4, metric = "euclidean")

# Manhattan (l1) distance
iris_nn &lt;- random_knn(iris, k = 4, metric = "manhattan")

# Multi-threading: you can choose the number of threads to use: in real
# usage, you will want to set n_threads to at least 2
iris_nn &lt;- random_knn(iris, k = 4, metric = "manhattan", n_threads = 1)

# Use verbose flag to see information about progress
iris_nn &lt;- random_knn(iris, k = 4, metric = "euclidean", verbose = TRUE)

# These results can be improved by nearest neighbors descent. You don't need
# to specify k here because this is worked out from the initial input
iris_nn &lt;- nnd_knn(iris, init = iris_nn, metric = "euclidean", verbose = TRUE)
</code></pre>


</div>