<div class="container">

<table style="width: 100%;"><tr>
<td>Backward selection with the F test or the partial correlation coefficient</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
backward selection with the F test or the partial correlation coefficient
</h2>

<h3>Description</h3>

<p>backward selection with the F test or the partial correlation coefficient.
</p>


<h3>Usage</h3>

<pre><code class="language-R">lm.bsreg(y, x, alpha = 0.05, type = "F") 
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>

<p>The dependent variable, a numerical vector with numbers. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>A numerical matrix with the indendent variables. We add, internally, the first column of ones.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>

<p>If you want to perform the usual F (or t) test set this equal to "F". 
For the test based on the partial correlation set this equal to "cor".
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>

<p>The type of backward selection to be used, "F" stands for F-test, where "cor" stands for partial correlation.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>It performs backward selection with the F test or the partial correlation coefficient. 
For the linear regression model, the Wald test is equivalent to the partial F test. 
So, instead of performing many regression models with single term deletions
we perform one regression model with all variables and compute their Wald test effectively. Note, that this 
is true, only if the design matrix "x" contains the vectors of ones and in our case this must be, strictly, 
the first column. The second option is to compute the p-value of the partial correlation. 
</p>


<h3>Value</h3>

<p>A matrix with two columns. The removed variables and their associated pvalue.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Hastie T., Tibshirani R. and Friedman J. (2008). The Elements of Statistical Learning (2nd Ed.), Springer. 
</p>


<h3>See Also</h3>

<p><code> lm.drop1, mmpc2, gee.reg, pc.sel </code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">y &lt;- rnorm(150)
x &lt;- as.matrix(iris[, 1:4])
a &lt;- lm(y ~., data.frame(x) )
lm.bsreg(y, x)
</code></pre>


</div>