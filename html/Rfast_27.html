<div class="container">

<table style="width: 100%;"><tr>
<td>BIC (using partial correlation) forward regression</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
BIC (using partial correlation) forward regression
</h2>

<h3>Description</h3>

<p>BIC (using partial correlation) forward regression.
</p>


<h3>Usage</h3>

<pre><code class="language-R">bic.corfsreg(y, x, tol = 2) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>

<p>A numerical vector.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>A matrix with data, the predictor variables. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>

<p>If the BIC difference between two successive models is less than the tolerance value, the variable will not enter the model.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The forward regression tries one by one the variables using the F-test, basically partial F-test every time for 
the latest variable. This is the same as testing the significance of the coefficient of this latest enetered 
variable. Alternatively the correlation can be used and this case the partial correlation coefficient. There is 
a direct relationship between the t-test statistic and the partial correlation coefficient. Now, instead of 
having to calculate the test statistic, we calculate the partial correlation coefficient. The largest partial correlation
indicates the candidate variable to enter the model. If the BIC of the regression model with that variable included, reduces, 
less than "tol" from the previous model without this variable, the variable enters. 
</p>


<h3>Value</h3>

<p>A matrix with two columns, the index of the selected variable(s) and the BIC of each model. The first line is always 
0 and the BIC of the model with no predictor variables. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris &lt;mtsagris@uoc.gr&gt; and Manos Papadakis &lt;papadakm95@gmail.com&gt;.
</p>


<h3>References</h3>

<p>Draper, N.R. and Smith H. (1988). Applied regression analysis. New York, Wiley, 3rd edition.
</p>


<h3>See Also</h3>

<p><code> cor.fsreg, score.glms, univglms, logistic_only,  
poisson_only, regression
</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## 200 variables, hence 200 univariate regressions are to be fitted
x &lt;- matrix( rnorm(200 * 200), ncol = 200 )
y &lt;- rnorm(200)
a1 &lt;- bic.corfsreg(y, x)
a2 &lt;- cor.fsreg(y, x)
x &lt;- NULL
</code></pre>


</div>