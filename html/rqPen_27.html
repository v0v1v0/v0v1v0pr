<div class="container">

<table style="width: 100%;"><tr>
<td>rq.gq.pen</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Title Quantile regression estimation and consistent variable selection across multiple quantiles</h2>

<h3>Description</h3>

<p>Uses the group lasso penalty across the quantiles to provide consistent selection across all, K, modeled quantiles. Let <code class="reqn">\beta^q</code>
be the coefficients for the kth quantiles, <code class="reqn">\beta_j</code> be the Q-dimensional vector of the jth coefficient for each quantile, and
<code class="reqn">\rho_\tau(u)</code> is the quantile loss function. The method minimizes
</p>
<p style="text-align: center;"><code class="reqn">\sum_{q=1}^Q \frac{1}{n} \sum_{i=1}^n m_i \rho_\tau(y_i-x_i^\top\beta^q) + \lambda \sum_{j=1}^p ||\beta_j||_{2,w}  .</code>
</p>

<p>Uses a Huber approximation in the fitting of model, as presented in Sherwood and Li (2022). Where,
</p>
<p style="text-align: center;"><code class="reqn">||\beta_j||_{2,w} = \sqrt{\sum_{k=1}^K w_kv_j\beta_{kj}^2},</code>
</p>
<p> where <code class="reqn">w_k</code> is a quantile weight 
that can be specified by <code>tau.penalty.factor</code>, <code class="reqn">v_j</code> is a predictor weight that can be assigned by <code>penalty.factor</code>, 
and <code class="reqn">m_i</code> is an observation weight that can be set by <code>weights</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">rq.gq.pen(
  x,
  y,
  tau,
  lambda = NULL,
  nlambda = 100,
  eps = ifelse(nrow(x) &lt; ncol(x), 0.01, 0.001),
  weights = NULL,
  penalty.factor = NULL,
  scalex = TRUE,
  tau.penalty.factor = NULL,
  gmma = 0.2,
  max.iter = 200,
  lambda.discard = TRUE,
  converge.eps = 1e-04,
  beta0 = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>covariate matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>a univariate response variable</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tau</code></td>
<td>
<p>a sequence of quantiles to be modeled, must be of at least length 3.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>shrinkage parameter. Default is NULL, and the algorithm provides a solution path.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlambda</code></td>
<td>
<p>Number of lambda values to be considered.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>If not pre-specified the lambda vector will be from lambda_max to lambda_max times eps</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>observation weights. Default is NULL, which means equal weights.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty.factor</code></td>
<td>
<p>weights for the shrinkage parameter for each covariate. Default is equal weight.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scalex</code></td>
<td>
<p>Whether x should be scaled before fitting the model. Coefficients are returned on the original scale.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tau.penalty.factor</code></td>
<td>
<p>weights for different quantiles. Default is equal weight.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gmma</code></td>
<td>
<p>tuning parameter for the Huber loss</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max.iter</code></td>
<td>
<p>maximum number of iteration. Default is 200.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.discard</code></td>
<td>
<p>Default is TRUE, meaning that the solution path stops if the relative deviance changes sufficiently small. It usually happens near the end of solution path. However, the program returns at least 70 models along the solution path.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>converge.eps</code></td>
<td>
<p>The epsilon level convergence. Default is 1e-4.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta0</code></td>
<td>
<p>Initial estimates. Default is NULL, and the algorithm starts with the intercepts being the quantiles of response variable and other coefficients being zeros.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>An rq.pen.seq object. 
</p>

<dl>
<dt>models: </dt>
<dd>
<p> A list of each model fit for each tau and a combination.</p>
</dd>
<dt>n:</dt>
<dd>
<p> Sample size.</p>
</dd>
<dt>p:</dt>
<dd>
<p> Number of predictors.</p>
</dd>
<dt>alg:</dt>
<dd>
<p> Algorithm used. Options are "huber" or any method implemented in rq(), such as "br". </p>
</dd>
<dt>tau:</dt>
<dd>
<p> Quantiles modeled.</p>
</dd>
<dt>a:</dt>
<dd>
<p> Tuning parameters a used.</p>
</dd>
<dt>modelsInfo:</dt>
<dd>
<p> Information about the quantile and a value for each model.</p>
</dd>
<dt>lambda:</dt>
<dd>
<p> Lambda values used for all models. If a model has fewer coefficients than lambda, say k. Then it used the first k values of lambda. Setting lambda.discard to TRUE will gurantee all values use the same lambdas, but may increase computational time noticeably and for little gain.</p>
</dd>
<dt>penalty:</dt>
<dd>
<p> Penalty used.</p>
</dd>
<dt>call:</dt>
<dd>
<p> Original call.</p>
</dd>
</dl>
<p>Each model in the models list has the following values. 
</p>

<dl>
<dt>coefficients:</dt>
<dd>
<p> Coefficients for each value of lambda.</p>
</dd>
<dt>rho:</dt>
<dd>
<p> The unpenalized objective function for each value of lambda.</p>
</dd>
<dt>PenRho:</dt>
<dd>
<p> The penalized objective function for each value of lambda.</p>
</dd>
<dt>nzero:</dt>
<dd>
<p> The number of nonzero coefficients for each value of lambda.</p>
</dd>
<dt>tau:</dt>
<dd>
<p> Quantile of the model.</p>
</dd>
<dt>a:</dt>
<dd>
<p> Value of a for the penalized loss function.</p>
</dd>
</dl>
<h3>Author(s)</h3>

<p>Shaobo Li and Ben Sherwood, <a href="mailto:ben.sherwood@ku.edu">ben.sherwood@ku.edu</a>
</p>


<h3>References</h3>

<p>Wang M, Kang X, Liang J, Wang K, Wu Y (2024).
“Heteroscedasticity identification and variable selection via multiple quantile regression.”
<em>Journal of Statistical Computation and Simulation</em>, <b>94</b>(2), 297-314.
</p>
<p>Sherwood B, Li S (2022).
“Quantile regression feature selection and estimation with grouped variables using Huber approximation.”
<em>Statistics and Computing</em>, <b>32</b>(5), 75.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run:  
n&lt;- 200
p&lt;- 10
X&lt;- matrix(rnorm(n*p),n,p)
y&lt;- -2+X[,1]+0.5*X[,2]-X[,3]-0.5*X[,7]+X[,8]-0.2*X[,9]+rt(n,2)
taus &lt;- seq(0.1, 0.9, 0.2)
fit&lt;- rq.gq.pen(X, y, taus)
#use IC to select best model, see rq.gq.pen.cv() for a cross-validation approach
qfit &lt;- qic.select(fit)

## End(Not run)
</code></pre>


</div>