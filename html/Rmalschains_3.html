<div class="container">

<table style="width: 100%;"><tr>
<td>malschains.control</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Sets and initializes the main parameters of the algorithm</h2>

<h3>Description</h3>

<p>This is a function that initializes and sets the parameters of the algorithm. 
It generates a list of parameters, to be used with the <code>malschains</code> function.
</p>


<h3>Usage</h3>

<pre><code class="language-R">malschains.control(
  popsize = 50,
  ls = "cmaes",
  istep = 500,
  effort = 0.5,
  alpha = 0.5,
  optimum = -Inf,
  threshold = 1e-08,
  lsOnly = FALSE,
  lsParam1 = 0,
  lsParam2 = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>popsize</code></td>
<td>
<p>The population size of the evolutionary algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ls</code></td>
<td>
<p>The local search method. Currently implementend are <code>cmaes</code>, <code>sw</code>, <code>simplex</code>, and <code>ssw</code>.
Usually, the <code>cmaes</code> local search strategy will give good results. However, it does not scale well with the problem size. 
So, if performance is needed, the <code>sw</code> strategy is a better choice. If the problem is high-dimensional, the <code>ssw</code> strategy is promising,
which selects randomly 20% of the variables for optimization.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>istep</code></td>
<td>
<p>The number of iterations of the local search. I.e., if the local search is started or re-started on an individual, it will be executed for an 
<code>istep</code> number of iterations. 
This parameter depends on the local search used. For <code>cmaes</code>, usually an <code>istep</code> of 300 is
a good choice. For the other local search methods, an <code>istep</code> of 100 performs better.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>effort</code></td>
<td>
<p>A value between 0 and 1 which gives the ratio between the amount of evaluations that are used for the local search and
for the evolutionary algorithm, respectively. A higher effort means more evaluations for the evolutionary algorithm. So, if exploration 
of the search space is more important than finding local optima, <code>effort</code> is to be chosen higher.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>The alpha parameter from crossover BLX-alpha. A lower value (&lt; 0.3) reduces diversity, a higher value increases diversity.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>optimum</code></td>
<td>
<p>The optimum to achieve. The default is zero, as in many minimization problems a value of zero can be considered optimal.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>threshold</code></td>
<td>
<p>A threshold which defines for the local search how much improvement is considered as no improvement. If this value is chosen
too low (zero), then the local search will usually always try to improve on the best individual, even if it is already located very close to a local optimum.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lsOnly</code></td>
<td>
<p>Apply only the local search algorithm, and not MA-LS-Chains</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lsParam1</code></td>
<td>
<p>First (optional) parameter. Currently, if local search is cmaes, this is the parameter popsize/lambda of cmaes. If it is not set or not positive, cmaes will calculate this automatically using a heuristic.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lsParam2</code></td>
<td>
<p>Second (optional) parameter. Currently, if local search is cmaes, this is the parameter parentssize/mu of cmaes. If it is not set, not positive, or not smaller lambda, cmaes will calculate this automatically using a heuristic.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>returns a list with the parameter names and values of the supplied parameters.
</p>


<h3>References</h3>

<p>Molina, D., Lozano, M., Sánchez, A.M., Herrera, F.
Memetic algorithms based on local search chains for large scale continuous optimisation problems: MA-SSW-Chains
(2011) Soft Computing, 15 (11), pp. 2201-2220. 
</p>
<p>Molina, D., Lozano, M., García-Martínez, C., Herrera, F.
Memetic algorithms for continuous optimisation based on local search chains
(2010) Evolutionary Computation, 18 (1), pp. 27-63.
</p>


</div>