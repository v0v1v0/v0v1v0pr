<div class="container">

<table style="width: 100%;"><tr>
<td>knnest,meany,vary,loclin,predict.knn,preprocessx,kmin,parvsnonparplot,nonparvsxplot,l1,l2,kNN,bestKperPoint</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>k-NN Nonparametric Regression and Classification</h2>

<h3>Description</h3>

<p>Full set of tools for k-NN regression and classification, including both
for direct usage and as tools for assessing the fit of parametric
models.
</p>


<h3>Usage</h3>

<pre><code class="language-R">kNN(x,y,newx=x,kmax,scaleX=TRUE,PCAcomps=0,expandVars=NULL,expandVals=NULL,
   smoothingFtn=mean,allK=FALSE,leave1out=FALSE, classif=FALSE,
   startAt1=TRUE,saveNhbrs=FALSE,savedNhbrs=NULL)
knnest(y,xdata,k,nearf=meany)
preprocessx(x,kmax,xval=FALSE)
meany(nearIdxs,x,y,predpt) 
mediany(nearIdxs,x,y,predpt) 
vary(nearIdxs,x,y,predpt) 
loclin(nearIdxs,x,y,predpt) 
## S3 method for class 'knn'
predict(object,...)
kmin(y,xdata,lossftn=l2,nk=5,nearf=meany) 

parvsnonparplot(lmout,knnout,cex=1.0) 
nonparvsxplot(knnout,lmout=NULL) 
nonparvarplot(knnout,returnPts=FALSE)
l2(y,muhat)
l1(y,muhat)
MAPE(yhat,y)
bestKperPoint(x,y,maxK,lossFtn="MAPE",classif=FALSE)
kNNallK(x,y,newx=x,kmax,scaleX=TRUE,PCAcomps=0,
   expandVars=NULL,expandVals=NULL,smoothingFtn=mean,
   allK=FALSE,leave1out=FALSE,classif=FALSE,startAt1=TRUE)
kNNxv(x,y,k,scaleX=TRUE,PCAcomps=0,smoothingFtn=mean,
   nSubSam=500)
knnest(y,xdata,k,nearf=meany)
loclogit(nearIdxs,x,y,predpt)
mediany(nearIdxs,x,y,predpt) 
exploreExpVars(xtrn, ytrn, xtst, ytst, k, eVar, maxEVal, lossFtn, 
    eValIncr = 0.05, classif = FALSE, leave1out = FALSE) 
plotExpVars(xtrn,ytrn,xtst,ytst,k,eVars,maxEVal,lossFtn,
   ylim,eValIncr=0.05,classif=FALSE,leave1out=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>nearf</code></td>
<td>
<p>Function to be applied to a neighborhood.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ylim</code></td>
<td>
<p>Range of Y values for plot.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lossFtn</code></td>
<td>
<p>Loss function for plot.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eVar</code></td>
<td>
<p>Variable to be expanded.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eVars</code></td>
<td>
<p>Variables to be expanded.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxEVal</code></td>
<td>
<p>Maximum expansion value.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eValIncr</code></td>
<td>
<p>Increment in range of expansion value.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xtrn</code></td>
<td>
<p>Training set for X.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ytrn</code></td>
<td>
<p>Training set for Y.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xtst</code></td>
<td>
<p>Test set for X.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ytst</code></td>
<td>
<p>Test set for Y.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nearIdxs</code></td>
<td>
<p>Indices of the neighbors.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nSubSam</code></td>
<td>
<p>Number of folds.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>"X" data, predictors, one row per data point, in the training
set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Response variable data in the training set. Vector or matrix,
the latter case for vector-valued response, e.g. multiclass
classification.  In that case, can be a vector, either (0,1,2,...,)
or (1,2,3,...), which automatically is converted into a matrix of
dummies.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>newx</code></td>
<td>
<p>New data points to be predicted.  If NULL in <code>kNN</code>,
compute regression functions estimates on <code>x</code> and save for
future prediction with <code>predict.kNN</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scaleX</code></td>
<td>
<p>If TRUE, call <code>scale</code> on <code>x</code> and <code>newx</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>PCAcomps</code></td>
<td>
<p>If positive, transform <code>x</code> and <code>newx</code> by
PCA, using the top <code>PCAcomps</code> principal components.  Disabled.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>expandVars</code></td>
<td>
<p>Indices of columns in <code>x</code> to expand.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>expandVals</code></td>
<td>
<p>The corresponding expansion values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>smoothingFtn</code></td>
<td>
<p>Function to apply to the "Y" values in the 
set of nearest neighbors.  Built-in choices are <code>meany</code>,
<code>mediany</code>, <code>vary</code> and <code>loclin</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>allK</code></td>
<td>
<p>If TRUE, find regression estimates for all <code>k</code>
through <code>kmax</code>.  Currently disabled.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>leave1out</code></td>
<td>
<p>If TRUE, omit the 1-nearest neighbor from analysis</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>classif</code></td>
<td>
<p>If TRUE, compute the predicted class labels, not just
the regression function values</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>startAt1</code></td>
<td>
<p>If TRUE, class labels start at 1, else 0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>Number of nearest neighbors</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>saveNhbrs</code></td>
<td>
<p>If TRUE, place output of <code>FNN::get.knnx</code> 
into <code>nhbrs</code> of component in return value</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>savedNhbrs</code></td>
<td>
<p>If non-NULL, this is the <code>nhbrs</code> component 
in the return value of a previous call; <code>newx</code> must be the
same in both calls</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Needed for consistency with generic.  See Details below for
'arguments.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xdata</code></td>
<td>
<p>X and associated neighbor indices. Output of
<code>preprocessx</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>Output of <code>knnest</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>predpt</code></td>
<td>
<p>One point on which to predict, as a vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kmax</code></td>
<td>
<p>Maximal number of nearest neighbors to find.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxK</code></td>
<td>
<p>Maximal number of nearest neighbors to find.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xval</code></td>
<td>
<p>Cross-validation flag. If TRUE, then the set of nearest 
neighbors of a point will not include the point itself.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lossftn</code></td>
<td>
<p>Loss function to be used in cross-validation
determination of "best" <code>k</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nk</code></td>
<td>
<p>Number of values of <code>k</code> to try in cross-validation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lmout</code></td>
<td>
<p>Output of <code>lm</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>knnout</code></td>
<td>
<p>Output of <code>knnest</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cex</code></td>
<td>
<p>R parameter to control dot size in plot.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>muhat</code></td>
<td>
<p>Vector of estimated regression function values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>yhat</code></td>
<td>
<p>Vector of estimated regression function values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>returnPts</code></td>
<td>
<p>If TRUE, return matrix of plotted points.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The <code>kNN</code> function is the main tool here; <code>knnest</code> is being
deprecated.  (Note too <code>qeKNN</code>, a wrapper for <code>kNN</code>; more
on this below.)  Here are the capabilities:
</p>
<p>In its most basic form, the function will input training data and
output predictions for new cases <code>newx</code>.  By default this is
done for a single value of the number <code>k</code> of nearest neighbors,
but by setting <code>allK</code> to TRUE, the user can request that it be
done for all <code>k</code> through the specified maximum.
</p>
<p>In the second form, <code>newx</code> is set to NULL in the call to
<code>kNN</code>.  No predictions are made; instead, the regression function 
is estimated on all data points in <code>x</code>, which are saved in the return 
value.  Future new cases can then be predicted from this saved object, 
via <code>predict.kNN</code> (called via the generic <code>predict</code>).  
The call form is <code>predict(knnout,newx,newxK</code>, with a 
default value of 1 for <code>newxK</code>.  
</p>
<p>In this second form, the closest <code>k</code> points to the <code>newx</code> in
<code>x</code> are determined as usual, but instead of averaging their Y
values, the average is taken over the fitted regression estimates at
those points.  In this manner, there is almost no computational cost
in the prediction stage.  
</p>
<p>The second form is intended more for production use, so that neighbor
distances need not be repeatedly recomputed.
</p>
<p>Nearest-neighbor computation can be time-consuming.  If more than one
value of <code>k</code> is anticipated, for the same <code>x</code>, <code>y</code> and
<code>newx</code>, first run with the largest anticipated value of
<code>k</code>, with <code>saveNhbrs</code> set to TRUE.  Then for other values
of <code>k</code>, set <code>savedNhbrs</code> to the <code>nhbrs</code> component in
the return value of the first call.
</p>
<p>In addition, a novel feature allows the user to weight some
predictors more than others.  This is done by scaling the given
predictor up or down, according to a specified value.  Normally, this
should be done with <code>scaleX = TRUE</code>, which applies
<code>scale()</code> to the data.  In other words, first we create a "level
playing field" in which all predictors have standard deviation 1.0,
then scale some of them up or down.
</p>
<p>Alternatives are provided to calculating the mean Y in the given
neighborhood, such as the median and the variance, the latter of
possible use in dealing with heterogeneity in linear models.
</p>
<p>Another choice of note is to allow local-linear smoothing, by
setting <code>smoothingFtn</code> to <code>loclin</code>.  Here the value of the
regression function at a point is predicted from a linear fit to the
point's neighbors.  This may be especially helpful to counteract bias
near the edges of the data.  As in any regression fit, the number of
predictors should be considerably less than the number of neighbors.
</p>
<p>Custom functions for smoothing can easily be written, say following
the pattern of <code>loclin</code>.
</p>
<p>The main alternative to <code>kNN</code> is <code>qeKNN</code> in the qe* ("quick
and easy") series. It is more convenient, e.g. allowing factor
inputs, but less flexible.
</p>
<p>The functions <code>ovaknntrn</code> and <code>ovaknnpred</code> are multiclass
wrappers for <code>knnest</code> and <code>knnpred</code>, thus also deprecated.
Here <code>y</code> is coded 0,1,...,<code>m</code>-1 for the <code>m</code> classes.
</p>
<p>The tools here can be useful for fit assessment of parametric models.
The <code>parvsnonparplot</code> function plots fitted values of
parameteric model vs. kNN fitted, <code>nonparvsxplot</code> k-NN fitted
values against each predictor, one by one.
</p>
<p>The functions <code>l2</code> and <code>l1</code> are used to define L2 and L1
loss.
</p>




<h3>Author(s)</h3>

<p>Norm Matloff
</p>


<h3>Examples</h3>

<pre><code class="language-R">
x &lt;- rbind(c(1,0),c(2,5),c(0,5),c(3,3),c(6,3))
y &lt;- c(8,3,10,11,4)
newx &lt;- c(0,0)

kNN(x,y,newx,2,scaleX=FALSE)
# $whichClosest
#      [,1] [,2]
# [1,]    1    4
# $regests
# [1] 9.5

kNN(x,y,newx,3,scaleX=FALSE,smoothingFtn=loclin)$regests
# 7.307692

knnout &lt;- kNN(x,y,newx,2,scaleX=FALSE)
knnout
# $whichClosest
#      [,1] [,2]
# [1,]    1    4
# ...

## Not run: 
data(mlb) 
mlb &lt;- mlb[,c(4,6,5)]  # height, age, weight
# fit, then predict 75", age 21, and 72", age 32
knnout &lt;- kNN(mlb[,1:2],mlb[,3],rbind(c(75,21),c(72,32)),25) 
knnout$regests
# [1] 202.72 195.72

# fit now, predict later
knnout &lt;- kNN(mlb[,1:2],mlb[,3],NULL,25) 
predict(knnout,c(70,28)) 
# [1] 186.48

data(peDumms) 
names(peDumms) 
ped &lt;- peDumms[,c(1,20,22:27,29,31,32)] 
names(ped) 

# fit, and predict income of a 35-year-old man, MS degree, occupation 101,
# worked 50 weeks, using 25 nearest neighbors
kNN(ped[,-10],ped[,10],c(35,1,0,0,1,0,0,0,1,50),25) $regests
# [1] 67540

# fit, and predict occupation 101 for a 35-year-old man, MS degree, 
# wage $55K, worked 50 weeks, using 25 nearest neighbors
z &lt;- kNN(ped[,-c(4:8)],ped[,4],c(35,1,0,1,55,50),25,classif=TRUE)
z$regests
# [1] 0.16  16
z$ypreds
# [1] 0  class 0, i.e. not occupation 101; round(0.24) = 0, 
# computed by user request, classif = TRUE

# the y argument must be either a vector (2-class setting) or a matrix
# (multiclass setting)
occs &lt;- as.matrix(ped[, 4:8])
z &lt;- kNN(ped[,-c(4:8)],occs,c(35,1,0,1,72000,50),25,classif=TRUE)
z$ypreds
# [1] 3   occupation 3, i.e. 102, is predicted

# predict occupation in general; let's bring occ.141 back in (was
# excluded as a predictor due to redundancy)
names(peDumms)
#  [1] "age"     "cit.1"   "cit.2"   "cit.3"   "cit.4"   "cit.5"   "educ.1" 
#  [8] "educ.2"  "educ.3"  "educ.4"  "educ.5"  "educ.6"  "educ.7"  "educ.8" 
# [15] "educ.9"  "educ.10" "educ.11" "educ.12" "educ.13" "educ.14" "educ.15"
# [22] "educ.16" "occ.100" "occ.101" "occ.102" "occ.106" "occ.140" "occ.141"
# [29] "sex.1"   "sex.2"   "wageinc" "wkswrkd" "yrentry"
occs &lt;- as.matrix(peDumms[,23:28])  
z &lt;- kNN(ped[,-c(4:8)],occs,c(35,1,0,1,72000,50),25,classif=TRUE)
z$ypreds
# [1] 3   prediction is occ.102

# try weight age 0.5, wkswrked 1.5; use leave1out to avoid overfit
knnout &lt;- kNN(ped[,-10],ped[,10],ped[,-10],25,leave1out=TRUE)
mean(abs(knnout$regests - ped[,10]))
# [1] 25341.6

# use of the weighted distance feature; deweight age by a factor of 0.5,
# put increased weight on weeks worked, factor of 1.5
knnout &lt;- kNN(ped[,-10],ped[,10],ped[,-10],25,
   expandVars=c(1,10),expandVals=c(0.5,1.5),leave1out=TRUE)
mean(abs(knnout$regests - ped[,10]))
# [1] 25196.61




## End(Not run)

</code></pre>


</div>