<div class="container">

<table style="width: 100%;"><tr>
<td>replayExperience</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Performs experience replay</h2>

<h3>Description</h3>

<p>Performs experience replay. Experience replay allows reinforcement learning agents to remember and reuse experiences from the past.
The algorithm requires input data in the form of sample sequences consisting of states, actions and rewards.
The result of the learning process is a state-action table Q that allows one to infer the best possible action in each state.
</p>


<h3>Usage</h3>

<pre><code class="language-R">replayExperience(D, Q, control, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>D</code></td>
<td>
<p>A <code>dataframe</code> containing the input data for reinforcement learning.
Each row represents a state transition tuple <code>(s,a,r,s_new)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Q</code></td>
<td>
<p>Existing state-action table of type <code>hash</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>control</code></td>
<td>
<p>Control parameters defining the behavior of the agent.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional parameters passed to function.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Returns an object of class <code>hash</code> that contains the learned Q-table.
</p>


<h3>References</h3>

<p>Lin (1992). "Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching", Machine Learning (8:3), pp. 293–321.
</p>
<p>Watkins (1992). "Q-learning". Machine Learning (8:3), pp. 279–292.
</p>


<h3>See Also</h3>

<p><code>ReinforcementLearning</code>
</p>


</div>