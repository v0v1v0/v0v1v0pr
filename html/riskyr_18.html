<div class="container">

<table style="width: 100%;"><tr>
<td>comp_accu_freq</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Compute accuracy metrics of current classification results.</h2>

<h3>Description</h3>

<p><code>comp_accu_freq</code> computes a list of current accuracy metrics
from the 4 essential frequencies (<code>hi</code>,
<code>mi</code>, <code>fa</code>, <code>cr</code>)
that constitute the current confusion matrix and
are contained in <code>freq</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">comp_accu_freq(hi = freq$hi, mi = freq$mi, fa = freq$fa, cr = freq$cr, w = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>hi</code></td>
<td>
<p>The number of hits <code>hi</code> (or true positives).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mi</code></td>
<td>
<p>The number of misses <code>mi</code> (or false negatives).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fa</code></td>
<td>
<p>The number of false alarms <code>fa</code> (or false positives).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cr</code></td>
<td>
<p>The number of correct rejections <code>cr</code> (or true negatives).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w</code></td>
<td>
<p>The weighting parameter <code>w</code> (from 0 to 1)
for computing weighted accuracy <code>wacc</code>.
Default: <code>w = .50</code> (i.e., yielding balanced accuracy <code>bacc</code>).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Currently computed accuracy metrics include:
</p>

<ol>
<li> <p><code>acc</code>: Overall accuracy as the proportion (or probability)
of correctly classifying cases or of <code>dec_cor</code> cases:
</p>
<p><code>acc = dec_cor/N = (hi + cr)/(hi + mi + fa + cr)</code>
</p>
<p>Values range from 0 (no correct prediction) to 1 (perfect prediction).
</p>
</li>
<li> <p><code>wacc</code>: Weighted accuracy, as a weighted average of the
sensitivity <code>sens</code> (aka. hit rate <code>HR</code>, <code>TPR</code>,
<code>power</code> or <code>recall</code>)
and the the specificity <code>spec</code> (aka. <code>TNR</code>)
in which <code>sens</code> is multiplied by a weighting parameter <code>w</code>
(ranging from 0 to 1) and <code>spec</code> is multiplied by
<code>w</code>'s complement <code>(1 - w)</code>:
</p>
<p><code>wacc = (w * sens) + ((1 - w) * spec)</code>
</p>
<p>If <code>w = .50</code>, <code>wacc</code> becomes <em>balanced</em> accuracy <code>bacc</code>.
</p>
</li>
<li> <p><code>mcc</code>: The Matthews correlation coefficient (with values ranging from -1 to +1):
</p>
<p><code>mcc = ((hi * cr) - (fa * mi)) / sqrt((hi + fa) * (hi + mi) * (cr + fa) * (cr + mi))</code>
</p>
<p>A value of <code>mcc = 0</code> implies random performance; <code>mcc = 1</code> implies perfect performance.
</p>
<p>See <a href="https://en.wikipedia.org/wiki/Matthews_correlation_coefficient">Wikipedia: Matthews correlation coefficient</a>
for additional information.
</p>
</li>
<li> <p><code>f1s</code>: The harmonic mean of the positive predictive value <code>PPV</code>
(aka. <code>precision</code>)
and the sensitivity <code>sens</code> (aka. hit rate <code>HR</code>,
<code>TPR</code>, <code>power</code> or <code>recall</code>):
</p>
<p><code>f1s =  2 * (PPV * sens) / (PPV + sens)</code>
</p>
<p>See <a href="https://en.wikipedia.org/wiki/F1_score">Wikipedia: F1 score</a> for additional information.
</p>
</li>
</ol>
<p>Notes:
</p>

<ul>
<li>
<p> Accuracy metrics describe the <em>correspondence</em> of decisions (or predictions) to actual conditions (or truth).
</p>
<p>There are several possible interpretations of accuracy:
</p>

<ol>
<li>
<p> as <em>probabilities</em> (i.e., <code>acc</code> being the proportion of correct classifications,
or the ratio <code>dec_cor</code>/<code>N</code>),
</p>
</li>
<li>
<p> as <em>frequencies</em> (e.g., as classifying a population of <code>N</code>
individuals into cases of <code>dec_cor</code> vs. <code>dec_err</code>),
</p>
</li>
<li>
<p> as <em>correlations</em> (e.g., see <code>mcc</code> in <code>accu</code>).
</p>
</li>
</ol>
</li>
<li>
<p> Computing exact accuracy values based on probabilities (by <code>comp_accu_prob</code>) may differ from
accuracy values computed from (possibly rounded) frequencies (by <code>comp_accu_freq</code>).
</p>
<p>When frequencies are rounded to integers (see the default of <code>round = TRUE</code>
in <code>comp_freq</code> and <code>comp_freq_prob</code>) the accuracy metrics computed by
<code>comp_accu_freq</code> correspond to these rounded values.
Use <code>comp_accu_prob</code> to obtain exact accuracy metrics from probabilities.
</p>
</li>
</ul>
<h3>Value</h3>

<p>A list <code>accu</code> containing current accuracy metrics.
</p>


<h3>References</h3>

<p>Consult <a href="https://en.wikipedia.org/wiki/Confusion_matrix">Wikipedia: Confusion matrix</a>
for additional information.
</p>


<h3>See Also</h3>

<p><code>accu</code> for all accuracy metrics;
<code>comp_accu_prob</code> computes exact accuracy metrics from probabilities;
<code>num</code> for basic numeric parameters;
<code>freq</code> for current frequency information;
<code>txt</code> for current text settings;
<code>pal</code> for current color settings;
<code>popu</code> for a table of the current population.
</p>
<p>Other metrics: 
<code>accu</code>,
<code>acc</code>,
<code>comp_accu_prob()</code>,
<code>comp_acc()</code>,
<code>comp_err()</code>,
<code>err</code>
</p>
<p>Other functions computing probabilities: 
<code>comp_FDR()</code>,
<code>comp_FOR()</code>,
<code>comp_NPV()</code>,
<code>comp_PPV()</code>,
<code>comp_accu_prob()</code>,
<code>comp_acc()</code>,
<code>comp_comp_pair()</code>,
<code>comp_complement()</code>,
<code>comp_complete_prob_set()</code>,
<code>comp_err()</code>,
<code>comp_fart()</code>,
<code>comp_mirt()</code>,
<code>comp_ppod()</code>,
<code>comp_prob_freq()</code>,
<code>comp_prob()</code>,
<code>comp_sens()</code>,
<code>comp_spec()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">comp_accu_freq()  # =&gt; accuracy metrics for freq of current scenario
comp_accu_freq(hi = 1, mi = 2, fa = 3, cr = 4)  # medium accuracy, but cr &gt; hi

# Extreme cases:
comp_accu_freq(hi = 1, mi = 1, fa = 1, cr = 1)  # random performance
comp_accu_freq(hi = 0, mi = 0, fa = 1, cr = 1)  # random performance: wacc and f1s are NaN
comp_accu_freq(hi = 1, mi = 0, fa = 0, cr = 1)  # perfect accuracy/optimal performance
comp_accu_freq(hi = 0, mi = 1, fa = 1, cr = 0)  # zero accuracy/worst performance, but see f1s
comp_accu_freq(hi = 1, mi = 0, fa = 0, cr = 0)  # perfect accuracy, but see wacc and mcc

# Effects of w:
comp_accu_freq(hi = 3, mi = 2, fa = 1, cr = 4, w = 1/2)  # equal weights to sens and spec
comp_accu_freq(hi = 3, mi = 2, fa = 1, cr = 4, w = 2/3)  # more weight to sens
comp_accu_freq(hi = 3, mi = 2, fa = 1, cr = 4, w = 1/3)  # more weight to spec

## Contrasting comp_accu_freq and comp_accu_prob:
# (a) comp_accu_freq (based on rounded frequencies):
freq1 &lt;- comp_freq(N = 10, prev = 1/3, sens = 2/3, spec = 3/4)   # =&gt; hi = 2, mi = 1, fa = 2, cr = 5
accu1 &lt;- comp_accu_freq(freq1$hi, freq1$mi, freq1$fa, freq1$cr)  # =&gt; accu1 (based on rounded freq).
# accu1
#
# (b) comp_accu_prob (based on probabilities):
accu2 &lt;- comp_accu_prob(prev = 1/3, sens = 2/3, spec = 3/4)      # =&gt; exact accu (based on prob).
# accu2
all.equal(accu1, accu2)  # =&gt; 4 differences!
#
# (c) comp_accu_freq (exact values, i.e., without rounding):
freq3 &lt;- comp_freq(N = 10, prev = 1/3, sens = 2/3, spec = 3/4, round = FALSE)
accu3 &lt;- comp_accu_freq(freq3$hi, freq3$mi, freq3$fa, freq3$cr)  # =&gt; accu3 (based on EXACT freq).
# accu3
all.equal(accu2, accu3)  # =&gt; TRUE (qed).


</code></pre>


</div>