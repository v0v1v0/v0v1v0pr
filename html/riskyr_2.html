<div class="container">

<table style="width: 100%;"><tr>
<td>accu</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>A list containing current accuracy information.</h2>

<h3>Description</h3>

<p><code>accu</code> contains current accuracy information
returned by the corresponding generating function
<code>comp_accu_prob</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">accu
</code></pre>


<h3>Format</h3>

<p>An object of class <code>list</code> of length 5.
</p>


<h3>Details</h3>

<p>Current metrics include:
</p>

<ol>
<li> <p><code>acc</code>: Overall accuracy as the probability (or proportion)
of correctly classifying cases or of <code>dec_cor</code> cases:
</p>
<p>See <code>acc</code> for definition and explanations.
</p>
<p><code>acc</code> values range from 0 (no correct prediction) to 1 (perfect prediction).
</p>
</li>
<li> <p><code>wacc</code>: Weighted accuracy, as a weighted average of the
sensitivity <code>sens</code> (aka. hit rate <code>HR</code>, <code>TPR</code>,
<code>power</code> or <code>recall</code>)
and the the specificity <code>spec</code> (aka. <code>TNR</code>)
in which <code>sens</code> is multiplied by a weighting parameter <code>w</code>
(ranging from 0 to 1) and <code>spec</code> is multiplied by
<code>w</code>'s complement <code>(1 - w)</code>:
</p>
<p><code>wacc = (w * sens) + ((1 - w) * spec)</code>
</p>
<p>If <code>w = .50</code>, <code>wacc</code> becomes <em>balanced</em> accuracy <code>bacc</code>.
</p>
</li>
<li> <p><code>mcc</code>: The Matthews correlation coefficient (with values ranging from -1 to +1):
</p>
<p><code>mcc = ((hi * cr) - (fa * mi)) / sqrt((hi + fa) * (hi + mi) * (cr + fa) * (cr + mi))</code>
</p>
<p>A value of <code>mcc = 0</code> implies random performance; <code>mcc = 1</code> implies perfect performance.
</p>
<p>See <a href="https://en.wikipedia.org/wiki/Matthews_correlation_coefficient">Wikipedia: Matthews correlation coefficient</a>
for additional information.
</p>
</li>
<li> <p><code>f1s</code>: The harmonic mean of the positive predictive value <code>PPV</code>
(aka. <code>precision</code>)
and the sensitivity <code>sens</code> (aka. hit rate <code>HR</code>,
<code>TPR</code>, <code>power</code> or <code>recall</code>):
</p>
<p><code>f1s =  2 * (PPV * sens) / (PPV + sens)</code>
</p>
<p>See <a href="https://en.wikipedia.org/wiki/F1_score">Wikipedia: F1 score</a> for additional information.
</p>
</li>
</ol>
<p>Notes:
</p>

<ul>
<li>
<p> Accuracy metrics describe the <em>correspondence</em> of decisions (or predictions) to actual conditions (or truth).
</p>
<p>There are several possible interpretations of accuracy:
</p>

<ol>
<li>
<p> as <em>probabilities</em> (i.e., <code>acc</code> being the probability or proportion
of correct classifications, or the ratio <code>dec_cor</code>/<code>N</code>),
</p>
</li>
<li>
<p> as <em>frequencies</em> (e.g., as classifying a population of <code>N</code>
individuals into cases of <code>dec_cor</code> vs. <code>dec_err</code>),
</p>
</li>
<li>
<p> as <em>correlations</em> (e.g., see <code>mcc</code> in <code>accu</code>).
</p>
</li>
</ol>
</li>
<li>
<p> Computing exact accuracy values based on probabilities (by <code>comp_accu_prob</code>) may differ from
accuracy values computed from (possibly rounded) frequencies (by <code>comp_accu_freq</code>).
</p>
<p>When frequencies are rounded to integers (see the default of <code>round = TRUE</code>
in <code>comp_freq</code> and <code>comp_freq_prob</code>) the accuracy metrics computed by
<code>comp_accu_freq</code> correspond to these rounded values.
Use <code>comp_accu_prob</code> to obtain exact accuracy metrics from probabilities.
</p>
</li>
</ul>
<h3>See Also</h3>

<p>The corresponding generating function <code>comp_accu_prob</code> computes exact accuracy metrics from probabilities;
<code>acc</code> defines accuracy as a probability;
<code>comp_accu_freq</code> computes accuracy metrics from frequencies;
<code>num</code> for basic numeric parameters;
<code>freq</code> for current frequency information;
<code>prob</code> for current probability information;
<code>txt</code> for current text settings.
</p>
<p>Other lists containing current scenario information: 
<code>freq</code>,
<code>num</code>,
<code>pal_bwp</code>,
<code>pal_bw</code>,
<code>pal_kn</code>,
<code>pal_mbw</code>,
<code>pal_mod</code>,
<code>pal_org</code>,
<code>pal_rgb</code>,
<code>pal_unikn</code>,
<code>pal_vir</code>,
<code>pal</code>,
<code>prob</code>,
<code>txt_TF</code>,
<code>txt_org</code>,
<code>txt</code>
</p>
<p>Other metrics: 
<code>acc</code>,
<code>comp_accu_freq()</code>,
<code>comp_accu_prob()</code>,
<code>comp_acc()</code>,
<code>comp_err()</code>,
<code>err</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">accu &lt;- comp_accu_prob()  # =&gt; compute exact accuracy metrics (from probabilities)
accu                      # =&gt; current accuracy information

## Contrasting comp_accu_freq and comp_accu_prob:
# (a) comp_accu_freq (based on rounded frequencies):
freq1 &lt;- comp_freq(N = 10, prev = 1/3, sens = 2/3, spec = 3/4)   # =&gt; rounded frequencies!
accu1 &lt;- comp_accu_freq(freq1$hi, freq1$mi, freq1$fa, freq1$cr)  # =&gt; accu1 (based on rounded freq).
# accu1
#
# (b) comp_accu_prob (based on probabilities):
accu2 &lt;- comp_accu_prob(prev = 1/3, sens = 2/3, spec = 3/4)      # =&gt; exact accu (based on prob).
# accu2
all.equal(accu1, accu2)  # =&gt; 4 differences!
#
# (c) comp_accu_freq (exact values, i.e., without rounding):
freq3 &lt;- comp_freq(N = 10, prev = 1/3, sens = 2/3, spec = 3/4, round = FALSE)
accu3 &lt;- comp_accu_freq(freq3$hi, freq3$mi, freq3$fa, freq3$cr)  # =&gt; accu3 (based on EXACT freq).
# accu3
all.equal(accu2, accu3)  # =&gt; TRUE (qed).


</code></pre>


</div>