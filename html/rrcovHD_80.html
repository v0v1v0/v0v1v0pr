<div class="container">

<table style="width: 100%;"><tr>
<td>SosDisc-class</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Class <code>"SosDisc"</code> - virtual base class for all classic and robust SosDisc 
classes representing the results of the robust and sparse multigroup classification 
by the optimal scoring approach
</h2>

<h3>Description</h3>

<p>Robust and sparse multigroup classification by the optimal scoring approach.
The class <code>SosDisc</code> searves as a base class for deriving all other 
classes representing the results of the robust and sparse multigroup classification 
by the optimal scoring approach.
</p>


<h3>Details</h3>

<p>The sparse optimal scoring problem (Clemmensen et al, 2011):
for <code class="reqn">h=1,....,Q</code>
</p>
<p style="text-align: center;"><code class="reqn">
\min_{\beta_h,\theta_h} \frac{1}{n} \|Y \theta_h - X \beta_h \|_2^2   + \lambda \|\beta_h\|_1
</code>
</p>

<p>subject to
</p>
<p style="text-align: center;"><code class="reqn">
\frac{1}{n} \theta_h^T Y^T Y\theta_h=1, \quad \theta_h^T Y^T Y \theta_l=0 \quad \forall l&lt;h.
</code>
</p>

<p>where <code class="reqn">X</code> deontes the robustly centered and scaled input matrix <code>x</code> (or alternativly the predictors from <code>formular</code>) and <code class="reqn">Y</code> is an dummy matrix coding die classmemberships from <code>grouping</code>.
</p>
<p>For each <code class="reqn">h</code> this problem can be solved interatively for <code class="reqn">\beta_h</code> and <code class="reqn">\theta_h</code>. In order to obtain robust estimates, <code class="reqn">\beta_h</code> is estimated with reweighted sparse least trimmed squares regression (Alfons et al, 2013) and <code class="reqn">\theta_h</code> with least absolut deviation regression in the first two iterations. To speed up the following repetitions an iterative down-weighting of observations with large residuals is combined with the iterative estimation of the optimal scoring coefficients with their classical estimates.
</p>
<p>The classification model is estimated on the low dimensional sparse subspace <code class="reqn">X[\beta_1,...,\beta_Q]</code> with robust LDA (<code>Linda</code>).
</p>


<h3>Objects from the Class</h3>

<p>A virtual Class: No objects may be created from it.</p>


<h3>Slots</h3>


<dl>
<dt>
<code>call</code>:</dt>
<dd>
<p>The (matched) function call.</p>
</dd>
<dt>
<code>prior</code>:</dt>
<dd>
<p>Prior probabilities; same as input parameter.</p>
</dd>
<dt>
<code>counts</code>:</dt>
<dd>
<p>Number of observations in each class.</p>
</dd>
<dt>
<code>beta</code>:</dt>
<dd>
<p>Object of class <code>"matrix"</code>: Q coefficient vectors of the predictor matrix from optimal scoring (see Details); 
rows corespond to variables listed in <code>varnames</code>.</p>
</dd>
<dt>
<code>theta</code>:</dt>
<dd>
<p>Object of class <code>"matrix"</code>: Q coefficient vectors of the dummy matrix for class coding from optimal scoring (see Details).</p>
</dd>
<dt>
<code>lambda</code>:</dt>
<dd>
<p>Non-negative tuning paramer from L1 norm penaly; same as input parameter</p>
</dd>
<dt>
<code>varnames</code>:</dt>
<dd>
<p>Character vector: Names of included predictor variables 
(variables where at least one beta coefficient is non-zero).</p>
</dd>
<dt>
<code>center</code>:</dt>
<dd>
<p>Centering vector of the input predictors (coordinate wise median).</p>
</dd>
<dt>
<code>scale</code>:</dt>
<dd>
<p>Scaling vector of the input predictors (mad).</p>
</dd>
<dt>
<code>fit</code>:</dt>
<dd>
<p>Object of class <code>"Linda"</code>: Linda model (robust LDA model) estimated in the low dimensional subspace <code class="reqn">X[\beta_1,...,\beta_Q]</code> (see Details)</p>
</dd>
<dt>
<code>mahadist2</code>:</dt>
<dd>
<p>These will go later to Linda object: squared robust Mahalanobis distance 
(calculated with estimates from Linda, with common covariance structure of all groups) 
of each observation to its group center in the low dimensional subspace <code class="reqn">X[\beta_1,...,\beta_Q]</code> (see Details).</p>
</dd>
<dt>
<code>wlinda</code>:</dt>
<dd>
<p>These will go later to Linda object: 0-1 weights derived from <code>mahadist2</code>;
observations where the squred robust Mahalanobis distance is larger than the 0.975 quantile 
of the chi-square distribution with Q degrees of freedom resive weight zero.</p>
</dd>
<dt>
<code>X</code>:</dt>
<dd>
<p>The training data set (same as the input parameter <code>x</code> of the constructor function)</p>
</dd>
<dt>
<code>grp</code>:</dt>
<dd>
<p>Grouping variable: a factor specifying the class for each observation (same as the input parameter <code>grouping</code>)</p>
</dd>   
</dl>
<h3>Methods</h3>


<dl>
<dt>predict</dt>
<dd>
<p><code>signature(object = "SosDisc")</code>: calculates prediction using the results in 
<code>object</code>. An optional data frame or matrix in which to look for variables with which 
to predict. If omitted, the training data set is used. If the original fit used a formula or 
a data frame or a matrix with column names, newdata must contain columns with the 
same names. </p>
</dd>
<dt>show</dt>
<dd>
<p><code>signature(object = "SosDisc")</code>: prints the results </p>
</dd>
<dt>summary</dt>
<dd>
<p><code>signature(object = "SosDisc")</code>: prints summary information </p>
</dd>
</dl>
<h3>Author(s)</h3>

 
<p>Irene Ortner <a href="mailto:irene.ortner@applied-statistics.at">irene.ortner@applied-statistics.at</a> and Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> 
</p>


<h3>References</h3>

<p>Clemmensen L, Hastie T, Witten D &amp; Ersboll B (2012),
Sparse discriminant analysis.
<em>Technometrics</em>, <b>53</b>(4), 406–413. 
</p>
<p>Ortner I, Filzmoser P &amp; Croux C (2020),
Robust and sparse multigroup classification by the optimal scoring approach.
Data Mining and Knowledge Discovery <b>34</b>, 723–741.
<a href="https://doi.org/10.1007/s10618-019-00666-8">doi:10.1007/s10618-019-00666-8</a>.     
</p>


<h3>Examples</h3>

<pre><code class="language-R">    showClass("SosDisc")
</code></pre>


</div>