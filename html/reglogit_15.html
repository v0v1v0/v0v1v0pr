<div class="container">

<table style="width: 100%;"><tr>
<td>predict.reglogit</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Prediction for regularized (polychotomous) logistic regression models
</h2>

<h3>Description</h3>

<p>Sampling from the posterior predictive distribution of a regularized (multinomial)
logistic regression fit, including entropy information for variability assessment
</p>


<h3>Usage</h3>

<pre><code class="language-R">## S3 method for class 'reglogit'
predict(object, XX, burnin = round(0.1 * nrow(object$beta)), ...)
## S3 method for class 'regmlogit'
predict(object, XX, burnin = round(0.1 * dim(object$beta)[1]), ...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>

<p>a <code>"reglogit"</code>-class object or a <code>"regmlogit"</code>-class object, depending
on whether binary or polychotomous methods were used for fitting
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>XX</code></td>
<td>

<p>a <code>matrix</code> of predictive locations where <code>ncol(XX) == object$ncol(XX)</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>burnin</code></td>
<td>

<p>a scalar positive <code>integer</code> indicate the number of samples of <code>object$beta</code>
to discard as burn-in;  the default is 10% of the number of samples
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p> For compatibility with generic <code>predict</code> method; not used</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Applies the logit transformation (<code>reglogit</code>) or multinomial logit (<code>regmlogit</code>)
to convert samples of the linear predictor at <code>XX</code> into a samples from a predictive
posterior probability distribution.  The raw probabilties, averages (posterior means), 
entropies, and posterior mean casses (arg-max of the average probabilities) are returned.
</p>


<h3>Value</h3>

<p>The output is a <code>list</code> with components explained below. For
<code>predict.regmlogit</code> everyhing (except entropy) is expanded by one
dimension into an <code>array</code> or <code>matrix</code> as appropriate.
</p>
<table>
<tr style="vertical-align: top;">
<td><code>p </code></td>
<td>
<p> a <code>nrow(XX) x (T-burnin)</code> sized <code>matrix</code> of
probabilities (of class 1) from the posterior predictive distribution. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mp </code></td>
<td>
<p> a vector of average probablities calculated over the rows of <code>p</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pc </code></td>
<td>
<p> class labels formed by rouding (or arg max for <code>predict.regmlogit</code>) 
the values in <code>mp</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ent </code></td>
<td>
<p> The posterior mean entropy given the probabilities in <code>mp</code> </p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>R.B. Gramacy, N.G. Polson. “Simulation-based regularized
logistic regression”. (2012) Bayesian Analysis, 7(3), p567-590; 
arXiv:1005.3430; <a href="https://arxiv.org/abs/1005.3430">https://arxiv.org/abs/1005.3430</a>
</p>
<p>C. Holmes, K. Held (2006). “Bayesian Auxilliary Variable Models for
Binary and Multinomial Regression”. Bayesian Analysis, 1(1), p145-168. 
</p>


<h3>See Also</h3>

<p><code>reglogit</code> and <code>regmlogit</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## see reglogit for a full example of binary classifiction complete with
## sampling from the posterior predictive distribution.  

## the example here is for polychotomous classification and prediction

## Not run: 
library(plgp)
x &lt;- seq(-2, 2, length=40)
X &lt;- expand.grid(x, x)
C &lt;- exp2d.C(X)
xx &lt;- seq(-2, 2, length=100)
XX &lt;- expand.grid(xx, xx)
CC &lt;- exp2d.C(XX)

## build cubically-expanded design matrix (with interactions)
Xe &lt;- cbind(X, X[,1]^2, X[,2]^2, X[,1]*X[,2],
            X[,1]^3, X[,2]^3, X[,1]^2*X[,2], X[,2]^2*X[,1],
            (X[,1]*X[,2])^2)

## perform MCMC
T &lt;- 1000
out &lt;- regmlogit(T, C, Xe, nu=6, normalize=TRUE)

## create predictive (cubically-expanded) design matrix
XX &lt;- as.matrix(XX)
XXe &lt;- cbind(XX, XX[,1]^2, XX[,2]^2, XX[,1]*XX[,2],
             XX[,1]^3, XX[,2]^3, XX[,1]^2*XX[,2], XX[,2]^2*XX[,1],
             (XX[,1]*XX[,2])^2)

## predict class labels
p &lt;- predict(out, XXe)

## make an image of the predictive surface
cols &lt;- c(gray(0.85), gray(0.625), gray(0.4))
par(mfrow=c(1,3))
image(xx, xx, matrix(CC, ncol=length(xx)), col=cols, main="truth")
image(xx, xx, matrix(p$c, ncol=length(xx)), col=cols, main="predicted")
image(xx, xx, matrix(p$ent, ncol=length(xx)), col=heat.colors(128),
      main="entropy")

## End(Not run)
</code></pre>


</div>