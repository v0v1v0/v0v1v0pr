<div class="container">

<table style="width: 100%;"><tr>
<td>cglsr</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>CG Least Squares Models</h2>

<h3>Description</h3>

<p>Conjugate gradient algorithm (CG) for the normal equations (CGLS algorithm 7.4.1, Bjorck 1996, p.289)
</p>


<h3>Usage</h3>

<pre><code class="language-R">
cglsr(X, y, nlv, reorth = TRUE, filt = FALSE)

## S3 method for class 'Cglsr'
coef(object, ..., nlv = NULL)  

## S3 method for class 'Cglsr'
predict(object, X, ..., nlv = NULL)  

</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>For the main function: Training X-data (<code class="reqn">n, p</code>). â€” For auxiliary functions: New X-data (<code class="reqn">m, p</code>) to consider. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Univariate training Y-data (<code class="reqn">n, 1</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlv</code></td>
<td>
<p>The number(s) of CG iterations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reorth</code></td>
<td>
<p>Logical. If <code>TRUE</code>, a Gram-Schmidt reorthogonalization of the normal equation residual vectors is done.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>filt</code></td>
<td>
<p>Logical. If <code>TRUE</code>, the filter factors are computed (output <code>F</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>For auxiliary functions: A fitted model, output of a call to the main functions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>For auxiliary functions: Optional arguments. Not used.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The code for re-orthogonalization (Hansen 1998) and filter factors (Vogel 1987, Hansen 1998) computations is a transcription (with few adaptations) of the matlab function 'cgls' (Saunders et al. https://web.stanford.edu/group/SOL/software/cgls/; Hansen 2008).
</p>
<p>The filter factors can be used to compute the model complexity of CGLSR and PLSR models (see <code>dfplsr_cg</code>).
</p>
<p>Data <code class="reqn">X</code> and <code class="reqn">y</code> are internally centered. 
</p>
<p>Missing values are not allowed.
</p>


<h3>Value</h3>

	
<p>For <code>cglsr</code>: 
</p>
<table>
<tr style="vertical-align: top;">
<td><code>B</code></td>
<td>
<p>matrix with the model coefficients for the fix nlv.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gnew</code></td>
<td>
<p>squared norm of the s vector</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xmeans</code></td>
<td>
<p>variable means for the training X-data</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ymeans</code></td>
<td>
<p>variable means for the training Y-data</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>F</code></td>
<td>
<p>If <code>filt = TRUE</code>, the filter factors</p>
</td>
</tr>
</table>
<p>For <code>coef.Cglsr</code> : 
</p>
<table>
<tr style="vertical-align: top;">
<td><code>int</code></td>
<td>
<p>intercept value.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>B</code></td>
<td>
<p>matrix with the model coefficients.</p>
</td>
</tr>
</table>
<p>For <code>predict.Cglsr</code> : 
</p>
<table><tr style="vertical-align: top;">
<td><code>pred</code></td>
<td>
<p>list of matrices, with the predicted values for each number <code>nlv</code> of CG iterations</p>
</td>
</tr></table>
<h3>References</h3>

<p>Bjorck, A., 1996. Numerical Methods for Least Squares Problems, Other Titles in Applied Mathematics. 
Society for Industrial and Applied Mathematics. https://doi.org/10.1137/1.9781611971484
</p>
<p>Hansen, P.C., 1998. Rank-Deficient and Discrete Ill-Posed Problems, Mathematical Modeling and Computation. 
Society for Industrial and Applied Mathematics. https://doi.org/10.1137/1.9780898719697
</p>
<p>Hansen, P.C., 2008. Regularization Tools version 4.0 for Matlab 7.3. 
Numer Algor 46, 189-194. https://doi.org/10.1007/s11075-007-9136-9
</p>
<p>Manne R. Analysis of two partial-least-squares algorithms for multivariate calibration. Chemometrics Intell.
Lab. Syst. 1987; 2: 187-197.
</p>
<p>Phatak A, De Hoog F. Exploiting the connection between
PLS, Lanczos methods and conjugate gradients: alternative proofs of some properties of PLS. J. Chemometrics
2002; 16: 361-367.
</p>
<p>Vogel, C. R.,  "Solving ill-conditioned linear systems using the conjugate gradient method", 
Report, Dept. of Mathematical Sciences, Montana State University, 1987.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
z &lt;- ozone$X
u &lt;- which(!is.na(rowSums(z)))
X &lt;- z[u, -4]
y &lt;- z[u, 4]
dim(X)
headm(X)
Xtest &lt;- X[1:2, ]
ytest &lt;- y[1:2]

nlv &lt;- 10
fm &lt;- cglsr(X, y, nlv = nlv)

coef(fm)
coef(fm, nlv = 1)

predict(fm, Xtest)
predict(fm, Xtest, nlv = 1:3)

pred &lt;- predict(fm, Xtest)$pred
msep(pred, ytest)

cglsr(X, y, nlv = 5, filt = TRUE)$F


</code></pre>


</div>