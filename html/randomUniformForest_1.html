<div class="container">

<table style="width: 100%;"><tr>
<td>randomUniformForest-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Random Uniform Forests for Classification, Regression and Unsupervised Learning
</h2>

<h3>Description</h3>

<p>Ensemble model for classification, regression and unsupervised learning, based on a forest of unpruned and randomized binary decision trees. Unlike <em>Breiman's Random Forests</em>, each tree is grown by sampling, <em>with replacement</em>, a set of variables before splitting each node. Each cut-point is generated randomly, according to the <em>continuous Uniform distribution between two random points of each candidate variable or using its whole current support</em>. Optimal random node is, then, selected among many full random ones by maximizing Information Gain (classification) or minimizing a distance (regression), 'L2' (or 'L1'). Unlike <em>Extremely Randomized Trees</em>, data are either <em>bootstrapped or sub-sampled for each tree</em>. From the theoretical side, Random Uniform Forests are aimed to lower correlation between trees and to offer a deep analysis of variable importance. The unsupervised mode introduces clustering and dimension reduction, using a three-layer engine: dissimilarity matrix, Multidimensional Scaling (or spectral decomposition) and k-means (or hierarchical clustering). From the practical side, Random Uniform Forests are designed to provide a complete analysis of (un)supervised problems and to allow native distributed and incremental learning.
</p>


<h3>Details</h3>


<table>
<tr>
<td style="text-align: left;">
Package: </td>
<td style="text-align: left;"> randomUniformForest</td>
</tr>
<tr>
<td style="text-align: left;">
Type: </td>
<td style="text-align: left;"> Package</td>
</tr>
<tr>
<td style="text-align: left;">
Version: </td>
<td style="text-align: left;"> 1.1.5</td>
</tr>
<tr>
<td style="text-align: left;">
Date: </td>
<td style="text-align: left;"> 2015-02-16</td>
</tr>
<tr>
<td style="text-align: left;">
License: </td>
<td style="text-align: left;"> BSD_3_clause</td>
</tr>
<tr>
<td style="text-align: left;">
</td>
</tr>
</table>
<p>Installation:  install.packages("randomUniformForest")<br>
Usage:         library(randomUniformForest)<br></p>


<h3>Author(s)</h3>

<p>Saip Ciss
</p>
<p>Maintainer: Saip Ciss <a href="mailto:saip.ciss@wanadoo.fr">saip.ciss@wanadoo.fr</a>
</p>


<h3>References</h3>

<p>Amit, Y., Geman, D., 1997. Shape Quantization and Recognition with Randomized Trees. <em>Neural Computation</em> 9, 1545-1588. 
</p>
<p>Biau, G., Devroye, L., Lugosi, G., 2008. Consistency of random forests and other averaging classifiers. <em>The Journal of Machine Learning Research</em> 9, 2015-2033.
</p>
<p>Bousquet, O., Boucheron, S., Lugosi, G., 2004. <em>Introduction to Statistical Learning Theory</em>, in: Bousquet, O., Luxburg, U. von, Ratsch, G. (Eds.), Advanced Lectures on Machine Learning, Lecture Notes in Computer Science. Springer Berlin Heidelberg, pp. 169-207.
</p>
<p>Breiman, L, 1996. Heuristics of instability and stabilization in model selection. <em>The Annals of Statistics</em> 24, no. 6, 2350-2383.
</p>
<p>Breiman, L., 1996. Bagging predictors. <em>Machine learning</em> 24, 123-140.
</p>
<p>Breiman, L., 2001. Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author). <em>Statistical Science</em> 16, no. 3, 199-231.
</p>
<p>Breiman, L., 2001. Random Forests, <em>Machine Learning</em> 45(1), 5-32.
</p>
<p>Breiman, L., Friedman, J.H., Olshen, R.A., Stone, C., 1984. <em>Classification and Regression Trees</em>. New York: Chapman and Hall. 
</p>
<p>Ciss, S., 2014. PhD thesis: <em>Forets uniformement aleatoires et detection des irregularites aux cotisations sociales</em>. Universite Paris Ouest Nanterre, France. In french.<br>
English title : <em>Random Uniform Forests and Irregularity Detection in social Security contributions</em>.<br>
Link : https://www.dropbox.com/s/q7hbgeafrdd8qtc/Saip_Ciss_These.pdf?dl=0
</p>
<p>Ciss, S., 2015a. Random Uniform Forests. Preprint. hal-01104340.
</p>
<p>Ciss, S., 2015b. Variable Importance in Random Uniform Forests. Preprint. hal-01104751. 
</p>
<p>Ciss, S., 2015c. Generalization Error and Out-of-bag Bounds in Random (Uniform) Forests. Preprint. hal-01110524.  
</p>
<p>Cox, T. F., Cox, M. A. A., 2001. <em>Multidimensional Scaling. Second edition</em>. Chapman and Hall. 
</p>
<p>Devroye, L., Gyorfi, L., Lugosi, G., 1996. <em>A probabilistic theory of pattern recognition</em>. New York: Springer.
</p>
<p>Dietterich, T.G., 2000. <em>Ensemble Methods in Machine Learning</em>, in : Multiple Classifier Systems, Lecture Notes in Computer Science. Springer Berlin Heidelberg, pp. 1-15. 
</p>
<p>Efron, B., 1979. Bootstrap Methods: Another Look at the Jackknife. <em>The Annals of Statistics</em> 7, 1-26.
</p>
<p>Gower, J. C., 1966. Some distance properties of latent root and vector methods used in multivariate analysis. <em>Biometrika</em> 53, 325-328.
</p>
<p>Gyorfi, L., 2002. <em>A distribution-free theory of nonparametric regression</em>. Springer Science &amp; Business Media.
</p>
<p>Hastie, T., Tibshirani, R., Friedman, J.J.H., 2001. <em>The elements of statistical learning</em>. New York: Springer. 
</p>
<p>Ho, T.K., 1998. The random subspace method for constructing decision forests. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 20, 832-844. 
</p>
<p>Lin, Y., Jeon, Y., 2002. Random Forests and Adaptive Nearest Neighbors. <em>Journal of the American Statistical Association</em> 101-474.
</p>
<p>Ng, A. Y., Jordan, M. I., Weiss, Y., 2002. On spectral clustering: Analysis and an algorithm. <em>Advances in neural information processing systems</em>, 2, 849-856.
</p>
<p>Vapnik, V.N., 1995. <em>The nature of statistical learning theory</em>. Springer-Verlag New York, Inc., New York, NY, USA.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Presenting some of the core functions
## Not run

## 1 - Classification: iris data set (assess whole data)

## load data (included in R):
# data(iris)
# XY = iris
# p = ncol(XY)
# X = XY[,-p]
# Y = XY[,p]

## Train a model : using formula
# iris.ruf = randomUniformForest(Species ~., XY, threads = 1)

## or using matrix
## iris.ruf = randomUniformForest(X, as.factor(Y), threads = 1)

## Assess model : Out-of-bag (OOB) evaluation
# iris.ruf

## Variable Importance : base assessment
# summary(iris.ruf)

## Variable Importance : deeper assessment (explain the modelling)
# iris.importance = importance(iris.ruf, Xtest = X, maxInteractions = p - 1)

## Visualize : details of Variable Importance 
## (tile windows vertically, using the R menu, to see all plots)
# plot(iris.importance, Xtest = X)

## Analyse : get an interpretation of the model results
# iris.ruf.analysis = clusterAnalysis(iris.importance, X, components = 3, 
# clusteredObject = iris.ruf, OOB = TRUE)

## Dimension reduction, clustering and visualization : OOB evaluation
# iris.clust.ruf = clusteringObservations(iris.ruf, X, importanceObject = iris.importance)

## 2 - Regression: Boston Housing (assess a test set)

## load data :
# install.packages("mlbench") ##if not installed
# data(BostonHousing, package = "mlbench")

# XY = BostonHousing
# p = ncol(XY)
# X = XY[,-p]
# Y = XY[,p]

## get random training and test sets :
## reproducibility :
# set.seed(2015)

# train_test = init_values(X, Y, sample.size = 1/2)
# Xtrain = train_test$xtrain
# Ytrain = train_test$ytrain
# Xtest = train_test$xtest
# Ytest = train_test$ytest

## Train a model : 
# boston.ruf = randomUniformForest(Xtrain, Ytrain)

## Assess (quickly) the model : 
# boston.ruf
# plot(boston.ruf)
# summary(boston.ruf)

## Predict the test set :
# boston.pred.ruf = predict(boston.ruf, Xtest)

## or predict quantiles
# boston.predQunatile_97.5.ruf = predict(boston.ruf, Xtest, type = "quantile", 
# whichQuantile = 0.975)

## or prediction intervals
# boston.predConfInt_95.ruf = predict(boston.ruf, Xtest, type = "confInt", conf = 0.95)

## Assess predictions :
# statsModel = model.stats(boston.pred.ruf, Ytest, regression = TRUE)

## Avoiding overfitting : under the i.i.d. assumption, OOB error
## is expected to be an upper bound of MSE. Convergence is first needed.
## Convergence needs low correlation between trees residuals.

# boston.ruf

## The easy way; reduce correlation(decreasing 'mtry' value) + post-processing 
# bostonNew.ruf = randomUniformForest(Xtrain, Ytrain, mtry = 4)

## (predict and) Post-process :
# bostonNew.predAll.ruf = predict(bostonNew.ruf, Xtest, type = "all")

# bostonNew.postProcessPred.ruf = postProcessingVotes(bostonNew.ruf, 
# predObject = bostonNew.predAll.ruf)

## Assess new predictions :
# statsModel = model.stats(bostonNew.postProcessPred.ruf, Ytest, regression = TRUE)

## Convergence : grow more trees
# bostonNew.moreTrees.ruf = rUniformForest.grow(bostonNew.ruf, Xtrain, ntree = 100)
</code></pre>


</div>