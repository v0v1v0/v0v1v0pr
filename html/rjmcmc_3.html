<div class="container">

<table style="width: 100%;"><tr>
<td>defaultpost</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Perform Post-Processing Using Default Bijections</h2>

<h3>Description</h3>

<p>Performs Bayesian multimodel inference, estimating Bayes factors and 
posterior model probabilities for N candidate models. Unlike 
<code>rjmcmcpost</code>, this function uses a default bijection scheme based
on approximating each posterior by a multivariate normal distribution. The 
result is reminiscent of the algorithm of Carlin &amp; Chib (1995) with a 
multivariate normal pseudo-prior. Transformation Jacobians are computed using
automatic differentiation so do not need to be specified.
</p>


<h3>Usage</h3>

<pre><code class="language-R">defaultpost(posterior, likelihood, param.prior, model.prior,
  chainlength = 10000, TM.thin = chainlength/10, progress = TRUE,
  save.all = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>posterior</code></td>
<td>
<p>A list of N matrices containing the posterior distributions 
under each model. Generally this will be obtained from MCMC output. Note
that each parameter should be real-valued so some parameters may need to be
transformed, using logarithms for example.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>likelihood</code></td>
<td>
<p>A list of N functions specifying the log-likelihood 
functions for the data under each model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>param.prior</code></td>
<td>
<p>A list of N functions specifying the prior distributions 
for each model-specific parameter vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model.prior</code></td>
<td>
<p>A numeric vector of the prior model probabilities. Note 
that this argument is not required to sum to one as it is automatically 
normalised.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>chainlength</code></td>
<td>
<p>How many iterations to run the Markov chain for.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>TM.thin</code></td>
<td>
<p>How regularly to calculate transition matrices as the chain 
progresses.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>progress</code></td>
<td>
<p>A logical determining whether a progress bar is drawn.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>save.all</code></td>
<td>
<p>A logical determining whether to save the value of the 
universal parameter at each iteration, as well as the corresponding 
likelihoods, priors and posteriors. If <code>TRUE</code>, the output object 
occupies significantly more memory.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Returns an object of class <code>rj</code> (see <code>rjmethods</code>). 
If <code>save.all=TRUE</code>, the output has named elements <code>result</code>, 
<code>densities</code>, <code>psidraws</code>, <code>progress</code> and <code>meta</code>. If 
<code>save.all=FALSE</code>, the <code>densities</code> and <code>psidraws</code> elements 
are omitted.
</p>
<p><code>result</code> contains useful point estimates, <code>progress</code> contains
snapshots of these estimates over time, and <code>meta</code> contains
information about the function call.
</p>


<h3>References</h3>

<p>Carlin, B. P. and Chib, S. (1995) Bayesian Model Choice via 
Markov Chain Monte Carlo Methods. <em>Journal of the Royal Statistical 
Society, Series B, 473-484</em>.
</p>
<p>Barker, R. J. and Link, W. A. (2013) Bayesian multimodel 
inference by RJMCMC: A Gibbs sampling approach. <em>The American 
Statistician, 67(3), 150-156</em>.
</p>


<h3>See Also</h3>

<p><code>adiff</code> <code>rjmcmcpost</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Comparing two binomial models -- see Barker &amp; Link (2013) for further details.

y=c(8,16); sumy=sum(y)
n=c(20,30); sumn=sum(n)

L1=function(p){if((all(p&gt;=0))&amp;&amp;(all(p&lt;=1))) sum(dbinom(y,n,p,log=TRUE)) else -Inf}
L2=function(p){if((p[1]&gt;=0)&amp;&amp;(p[1]&lt;=1)) sum(dbinom(y,n,p[1],log=TRUE)) else -Inf}

p.prior1=function(p){sum(dbeta(p,1,1,log=TRUE))}
p.prior2=function(p){dbeta(p[1],1,1,log=TRUE)+dbeta(p[2],17,15,log=TRUE)}

draw1=matrix(rbeta(2000,y+1,n-y+1), 1000, 2, byrow=TRUE)  ## full conditional posterior
draw2=matrix(c(rbeta(1000,sumy+1,sumn-sumy+1),rbeta(1000,17,15)), 1000, 2)

out=defaultpost(posterior=list(draw1,draw2), likelihood=list(L1,L2), 
                param.prior=list(p.prior1,p.prior2), model.prior=c(1,1), chainlength=1000)

</code></pre>


</div>