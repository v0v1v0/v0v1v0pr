<div class="container">

<table style="width: 100%;"><tr>
<td>LinkExtractor</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Extracts links from web pages
</h2>

<h3>Description</h3>

<p>Parse a web page, capturing and returning any links found.
</p>


<h3>Usage</h3>

<pre><code class="language-R">LinkExtractor(url)
</code></pre>


<h3>Arguments</h3>

<table><tr style="vertical-align: top;">
<td><code>url</code></td>
<td>

<p>A URL to scan for links.
</p>
</td>
</tr></table>
<h3>Details</h3>

<p>This is an internal routine used by several functions in the package.
</p>


<h3>Value</h3>

<table><tr style="vertical-align: top;">
<td><code>links</code></td>
<td>
<p>A vector of link URLs</p>
</td>
</tr></table>
<h3>Note</h3>

<p>While it might be fun to try <code>LinkExtractor</code> on a large website such as Google, the results will be unpredictable and perhaps disastrous if <code>depth</code> is not set.
This is because there is no protection against infinite recursion. 
</p>


<h3>Author(s)</h3>

<p>Daniel C. Bowman <a href="mailto:danny.c.bowman@gmail.com">danny.c.bowman@gmail.com</a>
</p>


<h3>See Also</h3>

<p><code>WebCrawler</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
#Find model runs for the 
#GFS 0.5x0.5 model

## Not run: 
urls.out &lt;- LinkExtractor(
"http://nomads.ncep.noaa.gov/cgi-bin/filter_gfs_0p50.pl")

## End(Not run)

</code></pre>


</div>