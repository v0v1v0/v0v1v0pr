<div class="container">

<table style="width: 100%;"><tr>
<td>smooth.construct.pco.smooth.spec</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Principal coordinate ridge regression</h2>

<h3>Description</h3>

<p>Smooth constructor function for principal coordinate ridge regression fitted
by <code>[mgcv]{gam}</code>. When the principal coordinates are defined by a
relevant distance among functional predictors, this is a form of nonparametric
scalar-on-function regression. Reiss et al. (2016) describe the approach and
apply it to dynamic time warping distances among functional predictors.
</p>


<h3>Usage</h3>

<pre><code class="language-R">## S3 method for class 'pco.smooth.spec'
smooth.construct(object, data, knots)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>a smooth specification object, usually generated by a term of
the form <code>s(dummy, bs="pco", k, xt)</code>; see Details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>a list containing just the data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>knots</code></td>
<td>
<p>IGNORED!</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>An object of class <code>pco.smooth</code>. The resulting object has an
<code>xt</code> element which contains details of the multidimensional scaling,
which may be interesting.
</p>


<h3>Details</h3>

<p>The constructor is not normally called directly, but is
rather used internally by <code>{gam}</code>.
</p>
<p>In a <code>[mgcv]{gam}</code> term of the above form <code>s(dummy, bs="pco",
 k, xt)</code>, </p>
 <ul>
<li> <p><code>dummy</code> is an arbitrary vector (or name of a
column in <code>data</code>) whose length is the number of observations. This is
not actually used, but is required as part of the input to
<code>[mgcv]{s}</code>. Note that if multiple <code>pco</code> terms are used in
the model, there must be multiple unique term names (e.g., "<code>dummy1</code>",
"<code>dummy2</code>", etc). </p>
</li>
<li> <p><code>k</code> is the number of principal coordinates
(e.g., <code>k=9</code> will give a 9-dimensional projection of the data). </p>
</li>
<li>
<p><code>xt</code> is a list supplying the distance information, in one of two ways.
(i) A matrix <code>Dmat</code> of distances can be supplied directly via
<code>xt=list(D=Dmat,...)</code>. (ii) Alternatively, one can use
<code>xt=list(realdata=..., dist_fn=..., ...)</code> to specify a data
matrix <code>realdata</code> and distance function <code>dist_fn</code>, whereupon a
distance matrix <code>dist_fn(realdata)</code> is created. </p>
</li>
</ul>
<p> The list <code>xt</code>
also has the following optional elements: </p>
 <ul>
<li> <p><code>add</code>: Passed
to <code>{cmdscale}</code> when performing multidimensional scaling; for
details, see the help for that function. (Default <code>FALSE</code>.)<br></p>
</li>
<li>
<p><code>fastcmd</code>: if <code>TRUE</code>, multidimensional scaling is performed by
<code>{cmdscale_lanczos}</code>, which uses Lanczos iteration to
eigendecompose the distance matrix; if <code>FALSE</code>, MDS is carried out by
<code>{cmdscale}</code>. Default is <code>FALSE</code>, to use <code>cmdscale</code>. </p>
</li>
</ul>
<h3>Author(s)</h3>

<p>David L Miller, based on code from Lan Huo and Phil Reiss
</p>


<h3>References</h3>

<p>Reiss, P. T., Miller, D. L., Wu, P.-S., and Wen-Yu Hua, W.-Y.
Penalized nonparametric scalar-on-function regression via principal
coordinates.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
# a simulated example
library(refund)
library(mgcv)
require(dtw)

## First generate the data
Xnl &lt;- matrix(0, 30, 101)
set.seed(813)
tt &lt;- sort(sample(1:90, 30))
for(i in 1:30){
  Xnl[i, tt[i]:(tt[i]+4)] &lt;- -1
  Xnl[i, (tt[i]+5):(tt[i]+9)] &lt;- 1
}
X.toy &lt;- Xnl + matrix(rnorm(30*101, ,0.05), 30)
y.toy &lt;- tt + rnorm(30, 0.05)
y.rainbow &lt;- rainbow(30, end=0.9)[(y.toy-min(y.toy))/
                                   diff(range(y.toy))*29+1]

## Display the toy data
par(mfrow=c(2, 2))
matplot((0:100)/100, t(Xnl[c(4, 25), ]), type="l", xlab="t", ylab="",
        ylim=range(X.toy), main="Noiseless functions")
matplot((0:100)/100, t(X.toy[c(4, 25), ]), type="l", xlab="t", ylab="",
        ylim=range(X.toy), main="Observed functions")
matplot((0:100)/100, t(X.toy), type="l", lty=1, col=y.rainbow, xlab="t",
        ylab="", main="Rainbow plot")

## Obtain DTW distances
D.dtw &lt;- dist(X.toy, method="dtw", window.type="sakoechiba", window.size=5)

## Compare PC vs. PCo ridge regression

# matrix to store results
GCVmat &lt;- matrix(NA, 15, 2)
# dummy response variable
dummy &lt;- rep(1,30)

# loop over possible projection dimensions
for (k. in 1:15){
  # fit PC (m1) and PCo (m2) ridge regression
  m1 &lt;- gam(y.toy ~ s(dummy, bs="pco", k=k.,
            xt=list(realdata=X.toy, dist_fn=dist)), method="REML")
  m2 &lt;- gam(y.toy ~ s(dummy, bs="pco", k=k., xt=list(D=D.dtw)), method="REML")
  # calculate and store GCV scores
  GCVmat[k., ] &lt;- length(y.toy) * c(sum(m1$residuals^2)/m1$df.residual^2,
                   sum(m2$residuals^2)/m2$df.residual^2)
}

## plot the GCV scores per dimension for each model
matplot(GCVmat, lty=1:2, col=1, pch=16:17, type="o", ylab="GCV",
        xlab="Number of principal components / coordinates",
        main="GCV score")
legend("right", c("PC ridge regression", "DTW-based PCoRR"), lty=1:2, pch=16:17)

## example of making a prediction

# fit a model to the toy data
m &lt;- gam(y.toy ~ s(dummy, bs="pco", k=2, xt=list(D=D.dtw)), method="REML")

# first build the distance matrix
# in this case we just subsample the original matrix
# see ?pco_predict_preprocess for more information on formatting this data
dist_list &lt;- list(dummy = as.matrix(D.dtw)[, c(1:5,10:15)])

# preprocess the prediction data
pred_data &lt;- pco_predict_preprocess(m, newdata=NULL, dist_list)

# make the prediction
p &lt;- predict(m, pred_data)

# check that these are the same as the corresponding fitted values
print(cbind(fitted(m)[ c(1:5,10:15)],p))


## End(Not run)
</code></pre>


</div>