<div class="container">

<table style="width: 100%;"><tr>
<td>train</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Train an imputation model using Midas</h2>

<h3>Description</h3>

<p>Build and run a MIDAS neural network on the supplied missing data.
</p>


<h3>Usage</h3>

<pre><code class="language-R">train(
  data,
  binary_columns = NULL,
  softmax_columns = NULL,
  training_epochs = 10L,
  layer_structure = c(256, 256, 256),
  learn_rate = 4e-04,
  input_drop = 0.8,
  seed = 123L,
  train_batch = 16L,
  latent_space_size = 4,
  cont_adj = 1,
  binary_adj = 1,
  softmax_adj = 1,
  dropout_level = 0.5,
  vae_layer = FALSE,
  vae_alpha = 1,
  vae_sample_var = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>A data.frame (or coercible) object, or an object of class <code>midas_pre</code> created from rMIDAS::convert()</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>binary_columns</code></td>
<td>
<p>A vector of column names, containing binary variables. NOTE: if <code>data</code> is a <code>midas_pre</code> object, this argument will be overwritten.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>softmax_columns</code></td>
<td>
<p>A list of lists, each internal list corresponding to a single categorical variable and containing names of the one-hot encoded variable names. NOTE: if <code>data</code> is a <code>midas_pre</code> object, this argument will be overwritten.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>training_epochs</code></td>
<td>
<p>An integer, indicating the number of forward passes to conduct when running the model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>layer_structure</code></td>
<td>
<p>A vector of integers, The number of nodes in each layer of the network (default = <code>c(256, 256, 256)</code>, denoting a three-layer network with 256 nodes per layer). Larger networks can learn more complex data structures but require longer training and are more prone to overfitting.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>learn_rate</code></td>
<td>
<p>A number, the learning rate <code class="reqn">\gamma</code> (default = 0.0001), which controls the size of the weight adjustment in each training epoch. In general, higher values reduce training time at the expense of less accurate results.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>input_drop</code></td>
<td>
<p>A number between 0 and 1. The probability of corruption for input columns in training mini-batches (default = 0.8). Higher values increase training time but reduce the risk of overfitting. In our experience, values between 0.7 and 0.95 deliver the best performance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>An integer, the value to which Python's pseudo-random number generator is initialized. This enables users to ensure that data shuffling, weight and bias initialization, and missingness indicator vectors are reproducible.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>train_batch</code></td>
<td>
<p>An integer, the number of observations in training mini-batches (default = 16).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>latent_space_size</code></td>
<td>
<p>An integer, the number of normal dimensions used to parameterize the latent space.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cont_adj</code></td>
<td>
<p>A number, weights the importance of continuous variables in the loss function</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>binary_adj</code></td>
<td>
<p>A number, weights the importance of binary variables in the loss function</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>softmax_adj</code></td>
<td>
<p>A number, weights the importance of categorical variables in the loss function</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dropout_level</code></td>
<td>
<p>A number between 0 and 1, determines the number of nodes dropped to "thin" the network</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vae_layer</code></td>
<td>
<p>Boolean, specifies whether to include a variational autoencoder layer in the network</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vae_alpha</code></td>
<td>
<p>A number, the strength of the prior imposed on the Kullback-Leibler divergence term in the variational autoencoder loss functions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vae_sample_var</code></td>
<td>
<p>A number, the sampling variance of the normal distributions used to parameterize the latent space.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>For more information, see Lall and Robinson (2023): <a href="doi:10.18637/jss.v107.i09">doi:10.18637/jss.v107.i09</a>.
</p>


<h3>Value</h3>

<p>Object of class <code>midas</code> from which completed datasets can be drawn, using <code>rMIDAS::complete()</code>
</p>


<h3>References</h3>

<p>Lall R, Robinson T (2023).
“Efficient Multiple Imputation for Diverse Data in Python and R: MIDASpy and rMIDAS.”
<em>Journal of Statistical Software</em>, <b>107</b>(9), 1–38.
<a href="https://doi.org/10.18637/jss.v107.i09">doi:10.18637/jss.v107.i09</a>.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Generate raw data, with numeric, binary, and categorical variables
## Not run: 
# Run where Python available and configured correctly
if (python_configured()) {
set.seed(89)
n_obs &lt;- 10000
raw_data &lt;- data.table(a = sample(c("red","yellow","blue",NA),n_obs, replace = TRUE),
                       b = 1:n_obs,
                       c = sample(c("YES","NO",NA),n_obs,replace=TRUE),
                       d = runif(n_obs,1,10),
                       e = sample(c("YES","NO"), n_obs, replace = TRUE),
                       f = sample(c("male","female","trans","other",NA), n_obs, replace = TRUE))

# Names of bin./cat. variables
test_bin &lt;- c("c","e")
test_cat &lt;- c("a","f")

# Pre-process data
test_data &lt;- convert(raw_data,
                     bin_cols = test_bin,
                     cat_cols = test_cat,
                     minmax_scale = TRUE)

# Run imputations
test_imp &lt;- train(test_data)

# Generate datasets
complete_datasets &lt;- complete(test_imp, m = 5, fast = FALSE)

# Use Rubin's rules to combine m regression models
midas_pool &lt;- combine(formula = d~a+c+e+f,
                      complete_datasets)
}

## End(Not run)

</code></pre>


</div>