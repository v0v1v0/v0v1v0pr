<div class="container">

<table style="width: 100%;"><tr>
<td>Many score based regressions</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Many score based regressions
</h2>

<h3>Description</h3>

<p>Many score based GLM regressions.
</p>


<h3>Usage</h3>

<pre><code class="language-R">score.glms(y, x, oiko = NULL, logged = FALSE) 
score.multinomregs(y, x, logged = FALSE) 
score.negbinregs(y, x, type = 1, logged = FALSE) 
score.weibregs(y, x, logged = FALSE) 
score.betaregs(y, x, logged = FALSE) 
score.gammaregs(y, x, logged = FALSE) 
score.expregs(y, x, logged = FALSE)
score.invgaussregs(y, x, logged = FALSE)
score.ztpregs(y, x, logged = FALSE)
score.geomregs(y, x, logged = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>

<p>A vector with either discrete or binary data for the Poisson, geometric, or negative binomial and binary logistic 
regressions, respectively. A vector with discrete values or factor values for the multinomial regression. If the 
vector is binary and choose multinomial regression the function checks and transfers to the binary logistic regression. 
</p>
<p>For the Weibull, gamma, inverse Gaussian and exponential regressions they must be strictly positive data, 
lifetimes or durations for example. For the beta regression they must be numbers between 0 and 1. For the 
zero truncated Poisson regression (score.ztpregs) they must be integer valued data strictly greater than 0. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>A matrix with data, the predictor variables. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>oiko</code></td>
<td>

<p>This can be either "poisson" or "binomial". If you are not sure leave it NULL and the function will check 
internally. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>

<p>This argument is for the negative binomial distribution. In the negative binomial you can choose which way
your prefer. Type 1 is for smal sample sizes, whereas type 2 is for larger ones as is faster. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>logged</code></td>
<td>

<p>A boolean variable; it will return the logarithm of the pvalue if set to TRUE.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Instead of maximising the log-likelihood via the Newton-Raphson algorithm in order to perform the hypothesis 
testing that <code class="reqn">\beta_i=0</code> we use the score test. This is dramatcially faster as no model needs to be fitted. 
The first derivative (score) of the log-likelihood is known and in closed form and under the null hypothesis 
the fitted values are all equal to the mean of the response variable y. The variance of the score is also known 
in closed form. The test is not the same as the likelihood ratio test. It is size correct nonetheless but it 
is a bit less efficient and less powerful. For big sample sizes though (5000 or more) the results are the same. 
We have seen via simulation studies is that it is size correct to large sample sizes, at elast a few thousands. 
You can try for yourselves and see that even with 500 the results are pretty close. The score test is pretty 
faster than the classical log-likelihood ratio test.  
</p>


<h3>Value</h3>

<p>A matrix with two columns, the test statistic and its associated p-value. For the Poisson and logistic 
regression the p-value is derived via the t distribution, whereas for the multinomial regressions 
via the <code class="reqn">\chi^2</code> distribution. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris &lt;mtsagris@uoc.gr&gt; and Manos Papadakis &lt;papadakm95@gmail.com&gt;.
</p>


<h3>References</h3>

<p>Tsagris M., Alenazi A. and Fafalios S. (2020). Computationally efficient univariate filtering for massive data. 
Electronic Journal of Applied Statistical Analysis, 13(2):390-412.
</p>
<p>Hosmer DW. JR, Lemeshow S. and Sturdivant R.X. (2013). Applied Logistic Regression. New Jersey ,Wiley, 3rd Edition.
</p>
<p>Campbell M.J. (2001). Statistics at Square Two: Understand Modern Statistical Applications in Medicine, pg. 112.
London, BMJ Books. 
</p>
<p>Draper N.R. and Smith H. (1988). Applied regression analysis. New York, Wiley, 3rd edition.
</p>
<p>McCullagh Peter, and John A. Nelder. Generalized linear models.  CRC press, USA, 2nd edition, 1989.
</p>
<p>Agresti Alan (1996). An introduction to categorical data analysis. New York: Wiley.
</p>
<p>Joseph M.H. (2011). Negative Binomial Regression. Cambridge University Press, 2nd edition.
</p>


<h3>See Also</h3>

<p><code> univglms, logistic_only,  poisson_only, regression
</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">x &lt;- matrnorm(500, 500)
y &lt;- rbinom(500, 1, 0.6)   ## binary logistic regression
a2 &lt;- score.glms(y, x)
y &lt;- rweibull(500, 2, 3)   
a &lt;- score.weibregs(y, x) 
mean(a[, 2] &lt; 0.05) 
x &lt;- NULL
</code></pre>


</div>