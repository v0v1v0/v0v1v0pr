<div class="container">

<table style="width: 100%;"><tr>
<td>tn</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Truncated Newton minimization of an unconstrained function.</h2>

<h3>Description</h3>

<p>An R implementation of the Truncated Newton method
of Stephen Nash for  driver to call the unconstrained function
minimization. The algorithm is based on Nash (1979) 
</p>
<p>This set of codes is entirely in R to allow users to explore and
understand the method. 
</p>


<h3>Usage</h3>

<pre><code class="language-R">   tn(x, fgfun, trace, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A numeric vector of starting estimates.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fgfun</code></td>
<td>
<p>A function that returns the value of the objective at
the supplied set of parameters <code>par</code> using auxiliary data in
.... The gradient is returned as attribute "gradient". 
The first argument of <code>fgfun</code> must be <code>par</code>. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trace</code></td>
<td>
<p>TRUE if progress output is to be presented. (Not yet verified.)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Further arguments to be passed to <code>fn</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Function <code>fgfun</code> must return a numeric value in list item <code>f</code>
and a numeric vector in list item <code>g</code>.
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>xstar</code></td>
<td>
<p>The best set of parameters found.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>f</code></td>
<td>
<p>The value of the objective at the best set of parameters found.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>g</code></td>
<td>
<p>The gradient of the objective at the best set of parameters found.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ierror</code></td>
<td>
<p>An integer indicating the situation on termination. <code>0</code>
indicates that the method believes it has succeeded; <code>2</code> that
more than <code>maxfun</code> (default 150*n, where there are n parameters);
<code>3</code> if the line search appears to have failed (which may not be serious);
and <code>-1</code> if there appears to be an error in the input parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nfngr</code></td>
<td>
<p>A number giving a measure of how many conjugate gradient solutions
were used during the minimization process.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Stephen G. Nash (1984) "Newton-type minimization via the Lanczos method",
SIAM J Numerical Analysis, vol. 21, no. 4, pages 770-788.
</p>
<p>For Matlab code, see http://www.netlib.org/opt/tn
</p>


<h3>See Also</h3>

<p><code>optim</code></p>


<h3>Examples</h3>

<pre><code class="language-R">#####################
## All examples are in this .Rd file
##
## Rosenbrock Banana function
fr &lt;- function(x) {
    x1 &lt;- x[1]
    x2 &lt;- x[2]
    100 * (x2 - x1 * x1)^2 + (1 - x1)^2
}
gr &lt;- function(x) {
    x1 &lt;- x[1]
    x2 &lt;- x[2]
    g1 &lt;- -400 * (x2 - x1*x1) * x1 - 2*(1-x1)
    g2 &lt;- 200*(x2 - x1*x1) 
    gg&lt;-c(g1, g2)
}

rosefg&lt;-function(x){
   f&lt;-fr(x)
   g&lt;-gr(x)
   attr(f, "gradient") &lt;- g
   f
}

x&lt;-c(-1.2, 1)

ansrosenbrock &lt;- tn(x, rosefg)
print(ansrosenbrock) # use print to allow copy to separate file that 
cat("Compare to optim\n")
ansoptrose &lt;- optim(x, fr, gr)
print(ansoptrose)


genrose.f&lt;- function(x, gs=NULL){ # objective function
## One generalization of the Rosenbrock banana valley function (n parameters)
	n &lt;- length(x)
        if(is.null(gs)) { gs=100.0 }
	fval&lt;-1.0 + sum (gs*(x[1:(n-1)]^2 - x[2:n])^2 + (x[2:n] - 1)^2)
        return(fval)
}
genrose.g &lt;- function(x, gs=NULL){
# vectorized gradient for genrose.f
# Ravi Varadhan 2009-04-03
	n &lt;- length(x)
        if(is.null(gs)) { gs=100.0 }
	gg &lt;- as.vector(rep(0, n))
	tn &lt;- 2:n
	tn1 &lt;- tn - 1
	z1 &lt;- x[tn] - x[tn1]^2
	z2 &lt;- 1 - x[tn]
	gg[tn] &lt;- 2 * (gs * z1 - z2)
	gg[tn1] &lt;- gg[tn1] - 4 * gs * x[tn1] * z1
	gg
}

grosefg&lt;-function(x, gs=100.0) {
    f&lt;-genrose.f(x, gs)
    g&lt;-genrose.g(x, gs)
    attr(f, "gradient") &lt;- g
    f
}

n &lt;- 100
x &lt;- (1:100)/20
groseu&lt;-tn(x, grosefg, gs=10)
print(groseu)

groseuo &lt;- optim(x, fn=genrose.f, gr=genrose.g, method="BFGS",
      control=list(maxit=1000), gs=10)
cat("compare optim BFGS\n")
print(groseuo)


lower&lt;-1+(1:n)/100
upper&lt;-5-(1:n)/100
xmid&lt;-0.5*(lower+upper)

grosec&lt;-tnbc(xmid, grosefg, lower, upper)
print(grosec)

cat("compare L-BFGS-B\n")
grosecl &lt;- optim(par=xmid, fn=genrose.f, gr=genrose.g, 
     lower=lower, upper=upper, method="L-BFGS-B")
print(grosecl)


</code></pre>


</div>