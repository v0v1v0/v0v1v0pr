<div class="container">

<table style="width: 100%;"><tr>
<td>knn_mi</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>kNN Mutual Information Estimators</h2>

<h3>Description</h3>

<p>Computes mutual information based on the distribution of nearest neighborhood distances. Method available are KSG1 and KSG2 as described by Kraskov, et. al (2004) and the Local Non-Uniformity Corrected (LNC) KSG as described by Gao, et. al (2015). The LNC method is based on KSG2  but with PCA volume corrections to adjust for observed non-uniformity of the local neighborhood of each point in the sample.
</p>


<h3>Usage</h3>

<pre><code class="language-R">knn_mi(data, splits, options)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>Matrix of sample observations, each row is an observation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>splits</code></td>
<td>
<p>A vector that describes which sets of columns in <code>data</code> to compute the mutual information between. For example, to compute mutual information between two variables use <code>splits = c(1,1)</code>. To compute <em>redundancy</em> among multiple random variables use <code>splits = rep(1,ncol(data))</code>. To compute the mutual information between two random vector list the dimensions of each vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>options</code></td>
<td>
<p>A list that specifies the estimator and its necessary parameters (see details).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Current available methods are LNC, KSG1 and KSG2.
</p>
<p>For KSG1 use: <code>options = list(method = "KSG1", k = 5)</code>
</p>
<p>For KSG2 use: <code>options = list(method = "KSG2", k = 5)</code>
</p>
<p>For LNC use: <code>options = list(method = "LNC", k = 10, alpha = 0.65)</code>, order needed <code>k &gt; ncol(data)</code>.
</p>


<h3>Author</h3>

<p>Isaac Michaud, North Carolina State University, <a href="mailto:ijmichau@ncsu.edu">ijmichau@ncsu.edu</a>
</p>


<h3>References</h3>

<p>Gao, S., Ver Steeg G., &amp; Galstyan A. (2015). Efficient estimation of mutual information for strongly dependent variables. Artificial Intelligence and Statistics: 277-286.
</p>
<p>Kraskov, A., Stogbauer, H., &amp; Grassberger, P. (2004). Estimating mutual information. Physical review E 69(6): 066138.
</p>


<h3>Examples</h3>

<pre><code class="language-R">set.seed(123)
x &lt;- rnorm(1000)
y &lt;- x + rnorm(1000)
knn_mi(cbind(x,y),c(1,1),options = list(method = "KSG2", k = 6))

set.seed(123)
x &lt;- rnorm(1000)
y &lt;- 100*x + rnorm(1000)
knn_mi(cbind(x,y),c(1,1),options = list(method = "LNC", alpha = 0.65, k = 10))
#approximate analytic value of mutual information
-0.5*log(1-cor(x,y)^2)

z &lt;- rnorm(1000)
#redundancy I(x;y;z) is approximately the same as I(x;y)
knn_mi(cbind(x,y,z),c(1,1,1),options = list(method = "LNC", alpha = c(0.5,0,0,0), k = 10))
#mutual information I((x,y);z) is approximately 0
knn_mi(cbind(x,y,z),c(2,1),options = list(method = "LNC", alpha = c(0.5,0.65,0), k = 10))

</code></pre>


</div>