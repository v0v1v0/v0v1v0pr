<div class="container">

<table style="width: 100%;"><tr>
<td>rgasp</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2> Setting up the robust GaSP model

</h2>

<h3>Description</h3>


<p>Setting up the robust GaSP model for estimating the parameters (if the parameters are not given). 

</p>


<h3>Usage</h3>

<pre><code class="language-R">  rgasp(design, response,trend=matrix(1,length(response),1),zero.mean="No",nugget=0,
    nugget.est=F,range.par=NA,method='post_mode',prior_choice='ref_approx',a=0.2,
    b=1/(length(response))^{1/dim(as.matrix(design))[2]}*(a+dim(as.matrix(design))[2]),
    kernel_type='matern_5_2',isotropic=F,R0=NA, 
    optimization='lbfgs', alpha=rep(1.9,dim(as.matrix(design))[2]),
    lower_bound=T,max_eval=max(30,20+5*dim(design)[2]),
    initial_values=NA,num_initial_values=2)
 
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>design</code></td>
<td>
<p> a matrix of inputs.

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>response</code></td>
<td>
<p> a matrix of outputs.

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trend</code></td>
<td>
<p> the mean/trend matrix of inputs. The default value is a  vector of ones. 

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>zero.mean</code></td>
<td>
<p> it has zero mean or not. The default value is <code>NO</code> meaning the mean is not zero. <code>Yes</code> means the mean is zero.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nugget</code></td>
<td>
<p> numerical value of the nugget variance ratio. If nugget is equal to 0, it means there is either no nugget or the nugget is estimated. If the nugget is not equal to 0, it means a fixed nugget. The default value is 0. 

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nugget.est</code></td>
<td>
<p> boolean value. <code>T</code> means nugget should be estimated and <code>F</code> means nugget is fixed
or not estimated. The default value is F <code>F</code>.

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>range.par</code></td>
<td>
<p> either <code>NA</code> or a <code>vector</code>. If it is <code>NA</code>, it means range parameters are estimated; otherwise range parameters are given. The default value is <code>NA</code>.

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>  method of parameter estimation. <code>post_mode</code> means the marginal posterior mode is used for estimation. <code>mle</code> means the maximum likelihood estimation is used. <code>mmle</code> means the maximum marginal likelihood estimation is used. The <code>post_mode</code>  is the default method. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prior_choice</code></td>
<td>
<p>the choice of prior for range parameters and noise-variance parameters. <code>ref_xi</code> and <code>ref_gamma</code> means the reference prior with reference prior with the log of inverse range parameterization ξ or range parameterization γ. <code>ref_approx</code> uses the jointly robust prior to approximate the reference prior. The default choice is <code>ref_approx</code>.

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>a</code></td>
<td>


<p>prior parameters in the jointly robust prior. The default value is 0.2. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>b</code></td>
<td>


<p>prior parameters in the jointly robust prior. The default value is <code>n^{-1/p}(a+p)</code> where n is the number of runs and p is the dimension of the input vector. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel_type</code></td>
<td>


<p>A vector specifying the type of kernels of each coordinate of the input. <code>matern_3_2</code> and <code>matern_5_2</code> are <code>Matern correlation</code> with roughness parameter 3/2 and 5/2 respectively. <code>pow_exp</code> is power exponential correlation with roughness parameter alpha. If <code>pow_exp</code> is to be used, one needs to specify its roughness parameter alpha. The default choice is <code>matern_5_2</code>. The <code>periodic_gauss</code> means the Gaussian kernel with periodic folding method with be used. The <code>periodic_exp</code> means the exponential kernel with periodic folding method will be used.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>isotropic</code></td>
<td>

<p>a boolean value. <code>T</code> means the isotropic kernel will be used and <code>F</code> means the separable kernel will be used. The default choice is  the separable kernel. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>R0</code></td>
<td>

<p>the distance between inputs. If the value is <code>NA</code>, it will be calculated later. It can also be specified by the user. If specified by user, it is either a <code>matrix</code> or <code>list</code>. The default value is <code>NA</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>optimization</code></td>
<td>

<p>the method for numerically optimization of the kernel parameters.  Currently three methods are implemented. <code>lbfgs</code> is the low-storage version of the Broyden-Fletcher-Goldfarb-Shanno method. <code>nelder-mead</code> is the  Nelder and Mead method.  <code>brent</code> is the Brent method for one-dimensional problems. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>


<p>roughness parameters in the <code>pow_exp</code> correlation functions. The default choice is a vector with each entry being 1.9. 
</p>
</td>
</tr>
</table>
<table>
<tr style="vertical-align: top;">
<td><code>lower_bound</code></td>
<td>

<p>boolean value.  <code>T</code> means the default lower bounds of the inverse range parameters are used to constrained the optimization and <code>F</code> means the optimization is unconstrained. The default value is <code>T</code> and we also suggest to use <code>F</code> in various scenarios. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_eval</code></td>
<td>

<p>the maximum number of steps to estimate the range and nugget parameters.  
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initial_values</code></td>
<td>

<p>a matrix of initial values of the kernel parameters to be optimized numerically, where each row of the matrix contains a set of the log inverse range parameters and the log nugget parameter. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num_initial_values</code></td>
<td>

<p>the number of initial values of the kernel parameters in optimization.
</p>
</td>
</tr>
</table>
<h3>Value</h3>






<p><code>rgasp</code> returns a S4 object of class <code>rgasp</code> (see <code>rgasp-class</code>).
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>


<p>M. Gu, X. Wang and J.O. Berger (2018), Robust Gaussian stochastic process emulation, <em>Annals of Statistics</em>, 46(6A), 3038-3066.
</p>
<p>M. Gu (2018), Jointly robust prior for Gaussian stochastic process in emulation, calibration and variable selection, arXiv:1804.09329.
</p>
<p>M. Gu. (2016). Robust uncertainty quantification and scalable computation for computer models with massive output. Ph.D. thesis. Duke University.
</p>
<p>M. Gu. and J.O. Berger (2016). Parallel partial Gaussian process emulation for computer models with massive output. <em>Annals of Applied Statistics</em>, 10(3), 1317-1347.
</p>
<p>E.T. Spiller, M.J. Bayarri, J.O. Berger and E.S. Calder and A.K. Patra and E.B. Pitman, and R.L. Wolpert (2014), Automating emulator construction for geophysical hazard maps. <em>SIAM/ASA Journal on Uncertainty Quantification</em>, 2(1), 126-152.
</p>
<p>J. Nocedal (1980), Updating quasi-Newton matrices with limited storage, <em>Math. Comput.</em>, 35, 773-782.
</p>
<p>D. C. Liu and J. Nocedal (1989), On the limited memory BFGS method for large scale optimization, <em>Math. Programming</em>, 45, p. 503-528.
</p>
<p>Brent, R. (1973), Algorithms for Minimization without Derivatives. Englewood Cliffs N.J.: Prentice-Hall.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
  library(RobustGaSP)
  #------------------------
  # a 3 dimensional example
  #------------------------
  # dimensional of the inputs
  dim_inputs &lt;- 3    
  # number of the inputs
  num_obs &lt;- 50       
  # uniform samples of design
  input &lt;- matrix(runif(num_obs*dim_inputs), num_obs,dim_inputs) 
  
  # Following codes use maximin Latin Hypercube Design, which is typically better than uniform
  # library(lhs)
  # input &lt;- maximinLHS(n=num_obs, k=dim_inputs)  ##maximin lhd sample
  
  ####
  # outputs from the 3 dim dettepepel.3.data function
  
  output = matrix(0,num_obs,1)
  for(i in 1:num_obs){
    output[i]&lt;-dettepepel.3.data (input[i,])
  }
  
  # use constant mean basis, with no constraint on optimization
  # and marginal posterior mode estimation
  m1&lt;- rgasp(design = input, response = output, lower_bound=FALSE)
  
  # you can use specify the estimation as maximum likelihood estimation (MLE)
  m2&lt;- rgasp(design = input, response = output, method='mle',lower_bound=FALSE)
  
  ##let's do some comparison on prediction
  n_testing=1000
  testing_input=matrix(runif(n_testing*dim_inputs),n_testing,dim_inputs)
  
  m1_pred=predict(m1,testing_input=testing_input)
  m2_pred=predict(m2,testing_input=testing_input)
  
  
  ##root of mean square error and interval
  test_output = matrix(0,n_testing,1)
  for(i in 1:n_testing){
    test_output[i]&lt;-dettepepel.3.data (testing_input[i,])
  }
  
  ##root of mean square error
  sqrt(mean( (m1_pred$mean-test_output)^2))
  sqrt(mean( (m2_pred$mean-test_output)^2))
  sd(test_output)
  #---------------------------------------
  # a 1 dimensional example with zero mean
  #---------------------------------------


  input=10*seq(0,1,1/14)
  output&lt;-higdon.1.data(input)
  #the following code fit a GaSP with zero mean by setting zero.mean="Yes"
  model&lt;- rgasp(design = input, response = output, zero.mean="Yes")
  model
  
  testing_input = as.matrix(seq(0,10,1/100))
  model.predict&lt;-predict(model,testing_input)
  names(model.predict)
  
  #########plot predictive distribution
  testing_output=higdon.1.data(testing_input)
  plot(testing_input,model.predict$mean,type='l',col='blue',
       xlab='input',ylab='output')
  polygon( c(testing_input,rev(testing_input)),c(model.predict$lower95,
        rev(model.predict$upper95)),col =  "grey80", border = FALSE)
  lines(testing_input, testing_output)
  lines(testing_input,model.predict$mean,type='l',col='blue')
  lines(input, output,type='p')
  
  ## mean square erros
  mean((model.predict$mean-testing_output)^2)


  #-----------------------------------
  # a 2 dimensional example with trend
  #-----------------------------------
  # dimensional of the inputs
  dim_inputs &lt;- 2    
  # number of the inputs
  num_obs &lt;- 20       
  
  # uniform samples of design
  input &lt;-matrix(runif(num_obs*dim_inputs), num_obs,dim_inputs) 
  # Following codes use maximin Latin Hypercube Design, which is typically better than uniform
  # library(lhs)
  # input &lt;- maximinLHS(n=num_obs, k=dim_inputs)  # maximin lhd sample
  
  # outputs from a 2 dim function
  
  output &lt;- matrix(0,num_obs,1)
  for(i in 1:num_obs){
    output[i]&lt;-limetal.2.data (input[i,])
  }
  
  ####trend or mean basis
  X&lt;-cbind(rep(1,num_obs), input )
  
  
  # use constant mean basis with trend, with no constraint on optimization
  m2&lt;- rgasp(design = input, response = output,trend =X,  lower_bound=FALSE)

  show(m2)      # show this rgasp object 
  
  m2@beta_hat       # estimated inverse range parameters
  m2@theta_hat      # estimated trend parameters

  #--------------------------------------------------------------------------------------
  # an 8 dimensional example using only a subset inputs and a noise with unknown variance
  #--------------------------------------------------------------------------------------
  set.seed(1)
  # dimensional of the inputs
  dim_inputs &lt;- 8    
  # number of the inputs
  num_obs &lt;- 50       
  
  # uniform samples of design
  input &lt;-matrix(runif(num_obs*dim_inputs), num_obs,dim_inputs) 
  # Following codes use maximin Latin Hypercube Design, which is typically better than uniform
  # library(lhs)
  # input &lt;- maximinLHS(n=num_obs, k=dim_inputs)  # maximin lhd sample
  
  # rescale the design to the domain
  input[,1]&lt;-0.05+(0.15-0.05)*input[,1];
  input[,2]&lt;-100+(50000-100)*input[,2];
  input[,3]&lt;-63070+(115600-63070)*input[,3];
  input[,4]&lt;-990+(1110-990)*input[,4];
  input[,5]&lt;-63.1+(116-63.1)*input[,5];
  input[,6]&lt;-700+(820-700)*input[,6];
  input[,7]&lt;-1120+(1680-1120)*input[,7];
  input[,8]&lt;-9855+(12045-9855)*input[,8];
  
  # outputs from the 8 dim Borehole function
  
  output=matrix(0,num_obs,1)
  for(i in 1:num_obs){
    output[i]=borehole(input[i,])
  }
  
  
    
    
  
  # use constant mean basis with trend, with no constraint on optimization
  m3&lt;- rgasp(design = input[,c(1,4,6,7,8)], response = output, 
            nugget.est=TRUE, lower_bound=FALSE)

  m3@beta_hat       # estimated inverse range parameters
  m3@nugget     



</code></pre>


</div>