<div class="container">

<table style="width: 100%;"><tr>
<td>vip</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Variable Importance in Projection (VIP)</h2>

<h3>Description</h3>

<p><code>vip</code> calculates the Variable Importance in Projection (VIP) for a PLS model. 
</p>


<h3>Usage</h3>

<pre><code class="language-R">
vip(object, X, Y = NULL, nlv = NULL)

</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>A fitted model, output of a call to a fitting function among <code>plskern</code>, <code>plsnipals</code>, <code>plsrannar</code>, <code>plsrda</code>, <code>plslda</code>), <code>plsqda</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>X-data involved in the fitted model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>Y-data involved in the fitted model.
If <code>Y</code> is NULL (default value), the VIP calculation is based on the proportion of Y-variance explained by the components, as proposed by Mehmood et al (2012, 2020).
If <code>Y</code> is not NULL, the VIP calculation is based on the redundancy, as proposed by Tenenhaus (1998).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlv</code></td>
<td>
<p>Number of components (LVs) to consider.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>matrix (<code class="reqn">(q,nlv)</code>) with VIP values, for models with 1 to nlv latent variables.
</p>


<h3>References</h3>

<p>Mehmood, T.,Liland, K.H.,Snipen, L.,Sæbø, S., 2012. A review of variable selection methods in Partial Least Squares Regression. Chemometrics and Intelligent Laboratory Systems, 118, 62-69. 
</p>
<p>Mehmood, T., Sæbø, S.,Liland, K.H., 2020. Comparison of variable selection methods in partial least squares regression. Journal of Chemometrics, 34, e3226.
</p>
<p>Tenenhaus, M., 1998. La régression PLS: théorie et pratique. Editions Technip, Paris, France.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## EXAMPLE OF PLS

n &lt;- 50 ; p &lt;- 4
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- rnorm(n)
Ytrain &lt;- cbind(y1 = ytrain, y2 = 100 * ytrain)
m &lt;- 3
Xtest &lt;- Xtrain[1:m, , drop = FALSE] 
Ytest &lt;- Ytrain[1:m, , drop = FALSE] ; ytest &lt;- Ytest[1:m, 1]

nlv &lt;- 3
fm &lt;- plskern(Xtrain, Ytrain, nlv = nlv)
vip(fm, Xtrain, Ytrain, nlv = nlv)
vip(fm, Xtrain, nlv = nlv)

fm &lt;- plskern(Xtrain, ytrain, nlv = nlv)
vip(fm, Xtrain, ytrain, nlv = nlv)
vip(fm, Xtrain, nlv = nlv)

## EXAMPLE OF PLSDA

n &lt;- 50 ; p &lt;- 8
Xtrain &lt;- matrix(rnorm(n * p), ncol = p)
ytrain &lt;- sample(c("1", "4", "10"), size = n, replace = TRUE)

Xtest &lt;- Xtrain[1:5, ] ; ytest &lt;- ytrain[1:5]

nlv &lt;- 5
fm &lt;- plsrda(Xtrain, ytrain, nlv = nlv)
vip(fm, Xtrain, ytrain, nlv = nlv)

</code></pre>


</div>