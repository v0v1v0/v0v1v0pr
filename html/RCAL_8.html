<div class="container">

<table style="width: 100%;"><tr>
<td>glm.regu</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Regularied M-estimation for fitting generalized linear models with a fixed tuning parameter</h2>

<h3>Description</h3>

<p>This function implements regularized M-estimation for fitting generalized linear models with continuous or binary responses 
for a fixed choice of tuning parameters.
</p>


<h3>Usage</h3>

<pre><code class="language-R">glm.regu(y, x, iw = NULL, loss = "cal", init = NULL, rhos, test = NULL,
  offs = NULL, id = NULL, Wmat = NULL, Rmat = NULL, zzs = NULL,
  xxs = NULL, n.iter = 100, eps = 1e-06, bt.lim = 3, nz.lab = NULL,
  pos = 10000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>An <code class="reqn">n</code> x <code class="reqn">1</code> response vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>An <code class="reqn">n</code> x <code class="reqn">p</code> matix of covariates, excluding a constant.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iw</code></td>
<td>
<p>An <code class="reqn">n</code> x <code class="reqn">1</code> weight vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss</code></td>
<td>
<p>A loss function, which can be specified as "gaus" for continuous responses, or "ml" or "cal" for binary respones.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>init</code></td>
<td>
<p>A <code class="reqn">(p+1)</code> x <code class="reqn">1</code> vector of initial values (the intercept and coefficients).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rhos</code></td>
<td>
<p>A <code class="reqn">p</code> x <code class="reqn">1</code> vector of Lasso tuning parameters, usually a constant vector, associated with the <code class="reqn">p</code> coefficients.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>test</code></td>
<td>
<p>A vector giving the indices of observations between 1 and <code class="reqn">n</code> which are included in the test set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>offs</code></td>
<td>
<p>An <code class="reqn">n</code> x <code class="reqn">1</code> vector of offset values, similarly as in <code>glm</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>id</code></td>
<td>
<p>An argument which can be used to speed up computation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Wmat</code></td>
<td>
<p>An argument which can be used to speed up computation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Rmat</code></td>
<td>
<p>An argument which can be used to speed up computation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>zzs</code></td>
<td>
<p>An argument which can be used to speed up computation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xxs</code></td>
<td>
<p>An argument which can be used to speed up computation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.iter</code></td>
<td>
<p>The maximum number of iterations allowed. An iteration is defined by computing an quadratic approximation and solving a least-squares Lasso problem.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>The tolerance at which the difference in the objective (loss plus penalty) values is considered close enough to 0 to declare convergence.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bt.lim</code></td>
<td>
<p>The maximum number of backtracking steps allowed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nz.lab</code></td>
<td>
<p>A <code class="reqn">p</code> x <code class="reqn">1</code> logical vector (useful for simulations), indicating which covariates are included when calculating the number of nonzero coefficients. 
If <code>nz.lab=NULL</code>, then <code>nz.lab</code> is reset to a vector of 0s.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pos</code></td>
<td>
<p>A value which can be used to facilitate recording the numbers of nonzero coefficients with or without the restriction by <code>nz.lab</code>. 
If <code>nz.lab=NULL</code>, then <code>pos</code> is reset to 1.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>For continuous responses, this function uses an active-set descent algorithm (Osborne et al. 2000; Yang and Tan 2018) to solve the least-squares Lasso problem. 
For binary responses, regularized calibrated estimation is implemented using the Fisher scoring descent algorithm in Tan (2020), whereas
regularized maximum likelihood estimation is implemented in a similar manner based on quadratic approximation as in the R package <span class="pkg">glmnet</span>.
</p>


<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>iter</code></td>
<td>
<p>The number of iterations performed up to <code>n.iter</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>conv</code></td>
<td>
<p>1 if convergence is obtained, 0 if exceeding the maximum number of iterations, or -1 if exceeding maximum number of backtracking steps.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nz</code></td>
<td>
<p>A value defined as (nz0 * <code>pos</code> + nz1) to record the numbers of nonzero coefficients without or with the restriction 
(denoted as nz0 and nz1) by <code>nz.lab</code>.
If <code>nz.lab=NULL</code>, then nz1 is 0, <code>pos</code> is 1, and hence <code>nz</code> is nz0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>inter</code></td>
<td>
<p>The estimated intercept.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bet</code></td>
<td>
<p>The <code class="reqn">p</code> x <code class="reqn">1</code> vector of estimated coefficients, excluding the intercept.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fit</code></td>
<td>
<p>The vector of fitted values in the training set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eta</code></td>
<td>
<p>The vector of linear predictors in the training set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tau</code></td>
<td>
<p>The <code class="reqn">p</code> x <code class="reqn">1</code> vector of generalized signs, which should be -1 or 1 for a negative or positive estimate and between -1 and 1 for a zero estimate.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>obj.train</code></td>
<td>
<p>The average loss in the training set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pen</code></td>
<td>
<p>The Lasso penalty of the estimates.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>obj</code></td>
<td>
<p>The average loss plus the Lasso penalty.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fit.test</code></td>
<td>
<p>The vector of fitted values in the test set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eta.test</code></td>
<td>
<p>The vector of linear predictors in the test set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>obj.test</code></td>
<td>
<p>The average loss in the test set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>id</code></td>
<td>
<p>This can be re-used to speed up computation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Wmat</code></td>
<td>
<p>This can be re-used to speed up computation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Rmat</code></td>
<td>
<p>This can be re-used to speed up computation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>zzs</code></td>
<td>
<p>This can be re-used to speed up computation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xxs</code></td>
<td>
<p>This can be re-used to speed up computation.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Osborne, M., Presnell, B., and Turlach, B. (2000) A new approach to variable selection in least squares problems, <em>IMA Journal of Numerical Analysis</em>, 20, 389-404.
</p>
<p>Yang, T. and Tan, Z. (2018) Backfitting algorithms for total-variation and empirical-norm penalized additive modeling with high-dimensional data, <em>Stat</em>, 7, e198.
</p>
<p>Tibshirani, R. (1996) Regression shrinkage and selection via the Lasso, <em>Journal of the Royal Statistical Society</em>, Ser. B, 58, 267-288.
</p>
<p>Tan, Z. (2020) Regularized calibrated estimation of propensity scores with model misspecification and high-dimensional data, <em>Biometrika</em>, 107, 137â€“158.
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(simu.data)
n &lt;- dim(simu.data)[1]
p &lt;- dim(simu.data)[2]-2

y &lt;- simu.data[,1]
tr &lt;- simu.data[,2]
x &lt;- simu.data[,2+1:p]
x &lt;- scale(x)

### Example 1: linear regression
# rhos should be a vector of length p, even though a constant vector
out.rgaus &lt;- glm.regu(y[tr==1], x[tr==1,], rhos=rep(.05,p), loss="gaus")

# the intercept
out.rgaus$inter

# the estimated coefficients and generalized signs; the first 10 are shown
cbind(out.rgaus$bet, out.rgaus$tau)[1:10,]

# the number of nonzero coefficients 
out.rgaus$nz

### Example 2: logistic regression using likelihood loss
out.rml &lt;- glm.regu(tr, x, rhos=rep(.01,p), loss="ml")
out.rml$inter
cbind(out.rml$bet, out.rml$tau)[1:10,]
out.rml$nz

### Example 3: logistic regression using calibration loss
out.rcal &lt;- glm.regu(tr, x, rhos=rep(.05,p), loss="cal")
out.rcal$inter
cbind(out.rcal$bet, out.rcal$tau)[1:10,]
out.rcal$nz

</code></pre>


</div>