<div class="container">

<table style="width: 100%;"><tr>
<td>KLdiv</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Kullback-Leibler divergence between two multivariate normal distributions</h2>

<h3>Description</h3>

<p>Function calculating the Kullback-Leibler divergence between two
multivariate normal distributions.
</p>


<h3>Usage</h3>

<pre><code class="language-R">KLdiv(Mtest, Mref, Stest, Sref, symmetric = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Mtest</code></td>
<td>
<p>A <code>numeric</code> mean vector for the approximating multivariate
normal distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Mref</code></td>
<td>
<p>A <code>numeric</code> mean vector for the true/reference multivariate
normal distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Stest</code></td>
<td>
<p>A covariance <code>matrix</code> for the approximating multivariate
normal distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Sref</code></td>
<td>
<p>A covariance <code>matrix</code> for the true/reference multivariate
normal distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>symmetric</code></td>
<td>
<p>A <code>logical</code> indicating if the symmetric version of
Kullback-Leibler divergence should be calculated.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The Kullback-Leibler (KL) information (Kullback and Leibler, 1951; also known
as relative entropy) is a measure of divergence between two probability
distributions. Typically, one distribution is taken to represent the ‘true’
distribution and functions as the reference distribution while the other is
taken to be an approximation of the true distribution. The criterion then
measures the loss of information in approximating the reference distribution.
The KL divergence between two <code class="reqn">p</code>-dimensional multivariate normal
distributions
<code class="reqn">\mathcal{N}^{0}_{p}(\boldsymbol{\mu}_{0}, \mathbf{\Sigma}_{0})</code> and <code class="reqn">\mathcal{N}^{1}_{p}(\boldsymbol{\mu}_{1}, \mathbf{\Sigma}_{1})</code>
is given as
</p>
<p style="text-align: center;"><code class="reqn">
    \mathrm{I}_{KL}(\mathcal{N}^{0}_{p} \| \mathcal{N}^{1}_{p}) =
     \frac{1}{2}\left\{\mathrm{tr}(\mathbf{\Omega}_{1}\mathbf{\Sigma}_{0})
     + (\boldsymbol{\mu}_{1} - \boldsymbol{\mu}_{0})^{\mathrm{T}}
     \mathbf{\Omega}_{1}(\boldsymbol{\mu}_{1} - \boldsymbol{\mu}_{0}) - p
     - \ln|\mathbf{\Sigma}_{0}| + \ln|\mathbf{\Sigma}_{1}| \right\},
  </code>
</p>

<p>where <code class="reqn">\mathbf{\Omega} = \mathbf{\Sigma}^{-1}</code>. The KL divergence is not
a proper metric as <code class="reqn">\mathrm{I}_{KL}(\mathcal{N}^{0}_{p} \|
\mathcal{N}^{1}_{p}) \neq \mathrm{I}_{KL}(\mathcal{N}^{1}_{p} \|
\mathcal{N}^{0}_{p})</code>. When <code>symmetric = TRUE</code> the function calculates
the symmetric KL divergence (also referred to as Jeffreys information), given
as
</p>
<p style="text-align: center;"><code class="reqn">
    \mathrm{I}_{KL}(\mathcal{N}^{0}_{p} \| \mathcal{N}^{1}_{p}) +
    \mathrm{I}_{KL}(\mathcal{N}^{1}_{p} \| \mathcal{N}^{0}_{p}).
  </code>
</p>



<h3>Value</h3>

<p>Function returns a <code>numeric</code> representing the (symmetric)
Kullback-Leibler divergence.
</p>


<h3>Author(s)</h3>

<p>Wessel N. van Wieringen, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;
</p>


<h3>References</h3>

<p>Kullback, S. and Leibler, R.A. (1951). On Information and
Sufficiency. Annals of Mathematical Statistics 22: 79-86.
</p>


<h3>See Also</h3>

<p><code>covML</code>, <code>ridgeP</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## Define population
set.seed(333)
p = 25
n = 1000
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]
Cov0  &lt;- covML(X)
mean0 &lt;- colMeans(X)

## Obtain sample from population
samples &lt;- X[sample(nrow(X), 10),]
Cov1  &lt;- covML(samples)
mean1 &lt;- colMeans(samples)

## Regularize singular Cov1
P &lt;- ridgeP(Cov1, 10)
CovR &lt;- solve(P)

## Obtain KL divergence
KLdiv(mean1, mean0, CovR, Cov0)

</code></pre>


</div>