<div class="container">

<table style="width: 100%;"><tr>
<td>RoughSets-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Getting started with the RoughSets package</h2>

<h3>Description</h3>

<p>This part contains global explanations about the implementation and use of the <code>RoughSets</code> package.
The package <code>RoughSets</code> attempts to provide a complete tool to model and analyze
information systems based on rough set theory (RST) and fuzzy rough set theory (FRST).
From fundamental point of view, this package allows to construct rough sets by defining lower and upper approximations.
Furthermore, recent methods for tackling common tasks in data mining, such as data preprocessing (e.g., discretization, feature selection, missing value completion,
and instance selection), rule induction, and prediction classes or decision values of new datasets
are available as well.
</p>


<h3>Details</h3>

<p>There are two main parts considered in this package which are RST and FRST.
RST was introduced by (Pawlak, 1982; Pawlak, 1991) which provides sophisticated mathematical tools to model
and analyze information systems that involve uncertainty and imprecision. By employing indiscernibility relation among objects, RST does not require
additional parameters to extract information.
The detailed explanation about fundamental concepts of RST can be read in Section <code>Introduction-RoughSets</code>. Secondly, FRST, an extension of
RST, was introduced by (Dubois and Prade, 1990) as a combination between
fuzzy sets proposed by (Zadeh, 1965) and RST. This concept allows to analyze continuous attributes without
performing discretization on data first. Basic concepts of FRST
can be seen in <code>Introduction-FuzzyRoughSets</code>.
</p>
<p>Based on the above concepts, many methods have been proposed and applied for dealing with several different domains.
In order to solve the problems, methods employ the indiscernibility relation and lower and upper approximation concepts.
All methods that have been implemented in this package will be explained by grouping based on their domains. The following is
a list of domains considered in this package:
</p>

<ul>
<li>
<p> Basic concepts: This part, we can divide into four different tasks which are
indiscernibility relation, lower and upper approximations, positive region and discernibility matrix.
All of those tasks have been explained briefly in Section <code>Introduction-RoughSets</code> and
</p>
<p><code>Introduction-FuzzyRoughSets</code>.
</p>
</li>
<li>
<p> Discretization: It is used to convert real-valued attributes into nominal/symbolic ones in an information system.
In RST point of view, this task attempts to maintain the discernibility between objects.
</p>
</li>
<li>
<p> Feature selection: It is a process for finding a subset of features which have the same quality as the complete feature set.
In other words, its purpose is to select the significant features and eliminate the dispensible ones.
It is a useful and necessary process when we are facing datasets containing large numbers of features. From RST and FRST perspective,
feature selection refers to searching superreducts and reducts. The detailed information about reducts can be read in
<code>Introduction-RoughSets</code> and <code>Introduction-FuzzyRoughSets</code>.
</p>
</li>
<li>
<p> Instance selection: This process is aimed to remove noisy, superfluous, or inconsistent instances from training datasets but retain consistent ones.
In other words, good accuracy of classification is achieved by removing instances which do not give positive contributions.
</p>
</li>
<li>
<p> Prediction/classification: This task is used to predict decision values of a new dataset (test data).
We consider implementing some methods to perform this task, such as fuzzy-rough nearest neighbor approaches, etc.
</p>
</li>
<li>
<p> Rule induction: This task refers to generate IF - THEN rules. The rule represents knowledge which is contained in a dataset.
One advantage of building rules is that naturally the model is easy to interpret. Then, predicted values over new datasets can be determined by
considering the rules.
</p>
</li>
</ul>
<p>As we mentioned before, we have embedded many well-known algorithms or techniques for handling the above domains. The algorithms were considered
since experimentally it has been proven that they were able to tackle complex tasks. They are implemented as functions that were organized
to work with the same data structures. So, users can perform various approaches for a particular task easily and then compare their results.
In order to be recognized quickly, generally we have chosen the names of the functions with some conventions. The names contain three parts
which are <code>prefix</code>, <code>suffix</code>, and <code>middle</code> that are separated by a point. The following is a description of each
part.
</p>

<ul>
<li> <p><code>prefix</code>: There are some different prefixes for names of functions expressing a kind of task to be performed.
The function names with prefix <code>BC</code> refer to <em>basic concepts</em> which means that the functions are created for
implementing the basic concepts of RST and FRST.
While prefix <code>D</code> refers to <em>discretization</em>, <code>FS</code>, <code>IS</code>, <code>RI</code>, <code>MV</code>, and <code>C</code> refer to <em>feature selection</em>,
<em>instance selection</em>, <em>rule induction</em>, <em>missing value completion</em>, and <em>classifier based on nearest neighbor</em> domains. Furthermore, <code>SF</code> and <code>X</code> mean that
functions are used as <em>supporting functions</em> which are not related directly with RST and FRST and <em>auxiliary</em> functions which are called as a parameter.
</p>
</li>
<li> <p><code>suffix</code>: It is located at the end of names. There are two types available: <code>RST</code> and <code>FRST</code>. <code>RST</code> represents <em>rough set theory</em>
while <code>FRST</code> shows that the function is applied to <em>fuzzy rough set theory</em>. Additionally, some functions that do not have
<code>RST</code> or <code>FRST</code> suffix are used for both theories.
</p>
</li>
<li> <p><code>middle</code>: All other words in the middle of the names are used to express the actual name of a particular method/algorithm or functionality.
In this case, it could consist of more than one word separated by points.
</p>
</li>
</ul>
<p>For instance, the function <code>BC.IND.relation.RST</code> is used to calculate the indiscernibility relation which is one of the basic concepts of RST.
Other functions that have names not based on the above rules are S3 functions e.g., <code>summary</code> and <code>predict</code> which are
used to summarize objects and predict new data, respectively.
</p>
<p>The following description explains domains and their algorithms implemented in the package:
</p>

<ol>
<li> <p><b>The implementations of RST</b>: This part outlines some considered algorihtms/methods based on RST.
The approaches can be classified based on their tasks as follows:
</p>

<ol>
<li>
<p> The basic concepts: The following is a list showing tasks and their implementations as functions.
</p>

<ul>
<li>
<p> Indiscernibility relation: It is a relation determining whether two objects are indiscernible by some attributes.
It is implemented in <code>BC.IND.relation.RST</code>.
</p>
</li>
<li>
<p> Lower and upper approximations: These approximations show whether objects can be classified with certainty or not.
It is implemented in <code>BC.LU.approximation.RST</code>.
</p>
</li>
<li>
<p> Positive region: It is used to determine objects that are included in positive region and the degree of dependency.
It is implemented in <code>BC.positive.reg.RST</code>.
</p>
</li>
<li>
<p> Discernibility matrix: It is used to create a discernibility matrix showing attributes that discern each pair of objects.
It is implemented in <code>BC.discernibility.mat.RST</code>.
</p>
</li>
</ul>
</li>
<li>
<p> Discretization: There are a few methods included in the package:
</p>

<ul>
<li> <p><code>D.global.discernibility.heuristic.RST</code>: It implements the global discernibility algorithm
which is computing globally semi-optimal cuts using the maximum discernibility heuristic.
</p>
</li>
<li> <p><code>D.discretize.quantiles.RST</code>: It is a function used for computing cuts of the "quantile-based" discretization into <code class="reqn">n</code> intervals.
</p>
</li>
<li> <p><code>D.discretize.equal.intervals.RST</code>: It is a function used for computing cuts of the "equal interval size" discretization into <code class="reqn">n</code> intervals.
</p>
</li>
</ul>
<p>The output of these functions is a list of cut values which are the values for converting real to nominal values.
So, in order to generate a new decision table according to the cut values, we need to call <code>SF.applyDecTable</code>.
Additionally, we have implemented <code>D.discretization.RST</code> as a wrapper function collecting all methods considered to perform discretization tasks.
</p>
</li>
<li>
<p> Feature selection: According to its output, it can be classified into the following groups:
</p>

<ul>
<li>
<p> Feature subset: It refers to a superreduct which is not necessarily minimal. In other words, the methods in this group
might generate just a subset of attributes.
</p>

<ul>
<li>
<p> QuickReduct algorithm: It has been implemented in <code>FS.quickreduct.RST</code>.
</p>
</li>
<li>
<p> Superreduct generation: It is based on some criteria:
entropy, gini index, discernibility measure, size of positive region.
</p>
<p>It is implemented in <code>FS.greedy.heuristic.superreduct.RST</code>.
</p>
</li>
</ul>
<p>Furthermore, we provide a wrapper function <code>FS.feature.subset.computation</code> in order to give a user interface for many methods of RST and FRST that are included in this group.
</p>
</li>
<li>
<p> Reduct: The following are methods that produce a single decision reduct:
</p>

<ul>
<li>
<p> Reduct generation based on criteria: It is based on different criteria which are
entropy, gini index, discernibility measure, size of positive region.
It has been implemented in <code>FS.greedy.heuristic.reduct.RST</code>.
</p>
</li>
<li>
<p> Permutation reduct: It is based on a permutation schema over all attributes.
It has been implemented in <code>FS.permutation.heuristic.reduct.RST</code>.
</p>
</li>
</ul>
<p>Furthermore, we provide a wrapper function <code>FS.reduct.computation</code> in order to give a user interface toward many methods of RST and FRST that are included in this group.
</p>
</li>
<li>
<p> All reducts: In order to generate all reducts, we execute <code>FS.all.reducts.computation</code>. However,
before doing that, we need to call <code>BC.discernibility.mat.RST</code> for
constructing a decision-relative discernibility matrix
</p>
</li>
</ul>
<p>It should be noted that the outputs of the functions are decision reducts. So, for generating a new decision table according to the decision reduct,
we need to call <code>SF.applyDecTable</code>.
</p>
</li>
<li>
<p> Rule induction: We provide several functions used to generate rules, as follows:
</p>

<ul>
<li>
<p> The function <code>RI.indiscernibilityBasedRules.RST</code>: This function requires the output of the feature selection functions.
</p>
</li>
<li>
<p> The function <code>RI.CN2Rules.RST</code>: It is a rule induction method based on the CN2 algorithm.
</p>
</li>
<li>
<p> The function <code>RI.LEM2Rules.RST</code>: It implements a rule induction method based on the LEM2 algorithm.
</p>
</li>
<li>
<p> The function <code>RI.AQRules.RST</code>: It is a rule induction based on the AQ-style algorithm.
</p>
</li>
</ul>
<p>After obtaining the rules, we execute <code>predict.RuleSetRST</code> considering our rules and given newdata/testing data to obtain predicted values/classes.
</p>
</li>
</ol>
</li>
<li> <p><b>The implementations of FRST</b>: As in the <code>RST</code> part, this part contains several algorithms that can be classified into several groups based on their purpose.
The following is a description of all methods that have been implemented in functions:
</p>

<ol>
<li>
<p> Basic concepts: The following is a list showing tasks and their implementations:
</p>

<ul>
<li>
<p> Indiscernibility relations: they are fuzzy relations determining to which degree two objects are similar.
This package provides several types of relations which are implemented in a single function
called <code>BC.IND.relation.FRST</code>. We consider several types of relations e.g.,
fuzzy equivalence, tolerance, and <code class="reqn">T</code>-similarity relations. These relations can be chosen by
assigning <code>type.relation</code>. Additionally, in this function, we provide several options to
calculate aggregation e.g., triangular norm operators (e.g., <code>"lukasiewicz"</code>, <code>"min"</code>, etc)
and user-defined operators.
</p>
</li>
<li>
<p> Lower and upper approximations: These approximations show to what extent objects can be classified with certainty or not.
This task has been implemented in
</p>
<p><code>BC.LU.approximation.FRST</code>. There are many approaches available in this package that can be selected by assigning the parameter <code>type.LU</code>.
The considered methods are
implication/t-norm, <code class="reqn">\beta</code>-precision fuzzy rough sets (<code class="reqn">\beta</code>-PFRS), vaguely quantified rough sets (VQRS), fuzzy variable precision rough sets (FVPRS), ordered weighted average (OWA),
soft fuzzy rough sets (SFRS), and robust fuzzy rough sets (RFRS). Furthermore, we provide a facility, which is <code>"custom"</code>, where users can create their own approximations by
defining functions to calculate lower and upper approximations. Many options to calculate implicator and triangular norm are also available.
</p>
</li>
<li>
<p> Positive region: It is used to determine the membership degree of each object to the positive region and the degree of dependency.
It is implemented in <code>BC.positive.reg.FRST</code>.
</p>
</li>
<li>
<p> Discernibility matrix: It is used to construct the decision-relative discernibility matrix. There are some approaches to construct the matrix,
e.g., based on standard approach, Gaussian reduction, alpha reduction, and minimal element in discernibility matrix. They have been implemented
in <code>BC.discernibility.mat.FRST</code>.
</p>
</li>
</ul>
</li>
<li>
<p> Feature selection: According to the output of functions,
we may divide them into three groups: those that produce a superreduct, a set of reducts, or a single reduct. The following is a description of functions based on their types:
</p>

<ul>
<li>
<p> Feature subset: It refers to methods which produce a superreduct which is not necessarily a reduct. In other words methods in this group
might generate just a subset of attributes.
The following is a complete list of methods considered in this package:
</p>

<ul>
<li>
<p> positive region based algorithms: It refers to
positive regions, as a way to evaluate attributes to be selected. They are implemented in <code>FS.quickreduct.FRST</code>.
Furthermore, we provide several different measures based on the positive region in this function.
All methods included in this part employ the QuickReduct algorithm to obtain selected features.
In order to choose a particular algorithm, we need to assign parameter <code>type.method</code> in <code>FS.quickreduct.FRST</code>.
</p>
</li>
<li>
<p> boundary region based algorithm: This algorithm is based on the membership degree to the fuzzy boundary region.
This algorithm has been implemented in <code>FS.quickreduct.FRST</code>.
</p>
</li>
</ul>
<p>Furthermore, we provide a wrapper function <code>FS.feature.subset.computation</code> in order to give a user interface for many methods of RST and FRST.
</p>
</li>
<li>
<p> Reduct: It refers to a method that produces a single decision reduct. We provide one algorithm which is the near-optimal reduction proposed by Zhao et al.
It is implemented in <code>FS.nearOpt.fvprs.FRST</code>.
Furthermore, we provide a wrapper function <code>FS.reduct.computation</code> in order to provide a user interface toward many methods of RST and FRST.
</p>
</li>
<li>
<p> All reducts:  In order to get all decision reducts, we execute <code>FS.all.reducts.computation</code>. However, before doing that, we firstly execute the <code>BC.discernibility.mat.FRST</code> function for
constructing a decision-relative discernibility matrix.
</p>
</li>
</ul>
<p>The output of the above methods is a class containing a decision reduct/feature subset and other descriptions.
For generating a new decision table according to the decision reduct, we provide the function <code>SF.applyDecTable</code>.
</p>
</li>
<li>
<p> Rule induction: It is a task used to generate
rules representing knowledge of a decision table. Commonly, this process is called learning phase in machine learning.
The following methods are considered to generate rules:
</p>

<ul>
<li> <p><code>RI.hybridFS.FRST</code>: It combines fuzzy-rough rule induction
and feature selection.
</p>
</li>
<li> <p><code>RI.GFRS.FRST</code>: It refers to rule induction based on generalized fuzzy rough sets (GFRS).
</p>
</li>
</ul>
<p>After generating rules, we can use them to predict decision values/classes of new data
by executing the S3 function <code>predict.RuleSetFRST</code>.
</p>
</li>
<li>
<p> Instance selection: The following functions select instances to improve accuracy by
removing noisy, superfluous or inconsistent ones from training datasets.
</p>

<ul>
<li> <p><code>IS.FRIS.FRST</code>: It refers to the fuzzy rough instance selection (FRIS). It evaluates the degree of membership to the positive region of each instance.
If an instance's membership degree is less than the threshold, then the instance can be removed.
</p>
</li>
<li> <p><code>IS.FRPS.FRST</code>: It refers to the fuzzy-rough prototype selection (FRPS). It employs prototype selection (PS) to improve the accuracy of the $k$-nearest neighbor (kNN) method.
</p>
</li>
</ul>
<p>We provide the function <code>SF.applyDecTable</code> that is used to generate a new decision table according to the output of instance selection functions.
</p>
</li>
<li>
<p> Fuzzy-rough nearest neighbors: This part provides methods based on nearest neighbors for
predicting decision values/classes of new datasets. In other words, by supplying a decision table as training data
we can predict decision values of new data at the same time.
We have considered the following methods:
</p>

<ul>
<li> <p><code>C.FRNN.FRST</code>: It refers to the fuzzy-rough nearest neighbors based on Jensen and Cornelis' technique.
</p>
</li>
<li> <p><code>C.FRNN.O.FRST</code>: It refers to the fuzzy-rough ownership nearest neighbor algorithm based on Sarkar's method.
</p>
</li>
<li> <p><code>C.POSNN.FRST</code>: The positive region based fuzzy-rough nearest neighbor algorithm based on Verbiest et al's technique.
</p>
</li>
</ul>
</li>
</ol>
</li>
</ol>
<p>Furthermore, we provide an additional feature which is missing value completion. Even though algorithms, included in this feature, are not based on RST and FRST, they will be usefull to do data analysis.
The following is a list of functions implemented for handling missing values in the data preprocessing step:
</p>

<ul>
<li> <p><code>MV.deletionCases</code>: it refers to the approach deleting instances.
</p>
</li>
<li> <p><code>MV.mostCommonValResConcept</code>: it refers to the approach based on the most common value or mean of an attribute restricted to a concept.
</p>
</li>
<li> <p><code>MV.mostCommonVal</code>: it refers to the approach replacing missing attribute values by the attribute mean or common values.
</p>
</li>
<li> <p><code>MV.globalClosestFit</code>: it refers to the approach based on the global closest fit approach.
</p>
</li>
<li> <p><code>MV.conceptClosestFit</code>: it refers to the approach based on the concept closest fit approach.
</p>
</li>
</ul>
<p>Additionally, we provide a wrapper function which is <code>MV.missingValueCompletion</code>
in order to give a user interface for the methods.
</p>
<p>To get started with the package, the user can have a look at the examples included in
the documentation on each function. Additionally, to show general usage of the package briefly,
we also provide some examples showing general usage in this section.
</p>
<p>If you have problems using the package, find a bug, or have suggestions,
please contact the package maintainer by email, instead of writing to the general R lists
or to other internet forums and mailing lists.
</p>
<p>There are many demos that ship with the package. To get a list of them, type:
</p>
<p><code>demo()</code>
</p>
<p>Then, to start a demo, type <code>demo(&lt;demo_name_here&gt;)</code>. All the demos are presented as
R scripts in the package sources in the "demo" subdirectory.
</p>
<p>Currently, there are the following demos available:
</p>

<ul>
<li>
<p> Basic concepts of RST and FRST:
</p>
<p><code>demo(BasicConcept.RST)</code>,
<code>demo(BasicConcept.FRST)</code>,
</p>
<p><code>demo(DiscernibilityMatrix.RST)</code>,
<code>demo(DiscernibilityMatrix.FRST)</code>.
</p>
</li>
<li>
<p> Discretization based on RST:
</p>
<p><code>demo(D.local.discernibility.matrix.RST)</code>,
<code>demo(D.max.discernibility.matrix.RST)</code>,
</p>
<p><code>demo(D.global.discernibility.heuristic.RST)</code>,
<code>demo(D.discretize.quantiles.RST)</code>,
</p>
<p><code>demo(D.discretize.equal.intervals.RST)</code>.
</p>
</li>
<li>
<p> Feature selection based on RST:
</p>
<p><code>demo(FS.permutation.heuristic.reduct.RST)</code>,
<code>demo(FS.quickreduct.RST)</code>,
</p>
<p><code>demo(FS.greedy.heuristic.reduct.RST)</code>,
<code>demo(FS.greedy.heuristic.reduct.RST)</code>.
</p>
</li>
<li>
<p> Feature selection based on FRST:
</p>
<p><code>demo(FS.QuickReduct.FRST.Ex1)</code>,
<code>demo(FS.QuickReduct.FRST.Ex2)</code>,
</p>
<p><code>demo(FS.QuickReduct.FRST.Ex3)</code>,
<code>demo(FS.QuickReduct.FRST.Ex4)</code>,
</p>
<p><code>demo(FS.QuickReduct.FRST.Ex5)</code>,
<code>demo(FS.nearOpt.fvprs.FRST)</code>.
</p>
</li>
<li>
<p> Instance selection based on FRST:
</p>
<p><code>demo(IS.FRIS.FRST)</code>,
<code>demo(IS.FRPS.FRST)</code>
</p>
</li>
<li>
<p> Classification using the Iris dataset:
</p>
<p><code>demo(FRNN.O.iris)</code>,
<code>demo(POSNN.iris)</code>,
<code>demo(FRNN.iris)</code>.
</p>
</li>
<li>
<p> Rule induction based on RST:
</p>
<p><code>demo(RI.indiscernibilityBasedRules.RST)</code>.
</p>
</li>
<li>
<p> Rule induction based on FRST:
</p>
<p><code>demo(RI.classification.FRST)</code>,
<code>demo(RI.regression.FRST)</code>.
</p>
</li>
<li>
<p> Missing value completion:
<code>demo(MV.simpleData)</code>.
</p>
</li>
</ul>
<p>Some decision tables have been embedded in this package which can be seen in
<code>RoughSetData</code>.
</p>


<h3>Introduction to Rough Set Theory</h3>

<p>This part attempts to introduce rough set theory (RST) and its application to data analysis. 
While the classical RST proposed by Pawlak in 1982 is explained in detail in this section, 
some recent advancements will be treated in the documentation of the related functions.  
</p>
<p>In RST, a data set is represented as a table called an information system <code class="reqn">\mathcal{A} = (U, A)</code>, where
<code class="reqn">U</code> is a non-empty set of finite objects known as the universe of discourse (note: it refers to all instances/rows 
in datasets) and <code class="reqn">A</code> is a non-empty finite set of attributes, such that <code class="reqn">a : U \to V_{a}</code> for every <code class="reqn">a \in A</code>. 
The set <code class="reqn">V_{a}</code> is the set of values that attribute <code class="reqn">a</code> may take. Information systems that involve a decision attribute, 
containing classes for each object, are called decision systems or decision tables. More formally, it is a pair <code class="reqn">\mathcal{A} = (U, A \cup \{d\})</code>,
where <code class="reqn">d \notin A</code> is the decision attribute. The elements of <code class="reqn">A</code> are called conditional attributes. The information system
representing all data in a particular system may contain redundant parts. It could happen because there are the same 
or indiscernible objects or some superfluous attributes. The indiscernibility relation is a binary relation showing the relation between two objects.
This relation is an equivalence relation.  
Let <code class="reqn">\mathcal{A} = (U, A)</code> be an information system, then for any <code class="reqn">B \subseteq A</code> there is an equivalence 
relation <code class="reqn">R_B(x,y)</code>:
</p>
<p><code class="reqn">R_B(x,y)= \{(x,y) \in U^2 | \forall a \in B, a(x) = a(y)\}</code>
</p>
<p>If <code class="reqn">(x,y) \in R_B(x,y)</code>, then <code class="reqn">x</code> and <code class="reqn">y</code> are indiscernible by attributes from <code class="reqn">B</code>. The equivalence
classes of the <code class="reqn">B</code>-indiscernibility relation are denoted <code class="reqn">[x]_{B}</code>. The indiscernibility relation will be further used to define basic concepts of rough
set theory which are lower and upper approximations.
</p>
<p>Let <code class="reqn">B \subseteq A</code> and <code class="reqn">X \subseteq U</code>,
<code class="reqn">X</code> can be approximated using the information contained within <code class="reqn">B</code> by constructing 
the <code class="reqn">B</code>-lower and <code class="reqn">B</code>-upper approximations of <code class="reqn">X</code>:
</p>
<p><code class="reqn">R_B \downarrow X = \{ x \in U | [x]_{B} \subseteq X \}</code>
</p>
<p><code class="reqn">R_B \uparrow X = \{ x \in U | [x]_{B} \cap X \not= \emptyset \}</code>
</p>
<p>The tuple <code class="reqn">\langle R_B \downarrow X, R_B \uparrow X \rangle</code> is called a rough set.
The objects in <code class="reqn">R_B \downarrow X</code> mean that they can be with certainty classified as members of <code class="reqn">X</code> on the basis of knowledge in <code class="reqn">B</code>, while
the objects in <code class="reqn">R_B \uparrow X</code> can be only classified as possible members of <code class="reqn">X</code> on the basis of knowledge in <code class="reqn">B</code>. 
</p>
<p>In a decision system, for <code class="reqn">X</code> we use decision concepts (equivalence classes of decision attribute) <code class="reqn">[x]_d</code>. 
We can define <code class="reqn">B</code>-lower and <code class="reqn">B</code>-upper approximations as follows.
</p>
<p><code class="reqn">R_B \downarrow [x]_d = \{ x \in U | [x]_{B} \subseteq [x]_d \}</code>
</p>
<p><code class="reqn">R_B \uparrow [x]_d = \{ x \in U | [x]_{B} \cap [x]_d \not= \emptyset \}</code>
</p>
<p>The positive, negative and boundary of <code class="reqn">B</code> regions can be defined as:
</p>
<p><code class="reqn">POS_{B} = \bigcup_{x \in U } R_B \downarrow [x]_d</code>
</p>
<p>The boundary region, <code class="reqn">BND_{B}</code>, is the set of objects that can possibly, but not certainly, be classified.
</p>
<p><code class="reqn">BND_{B} = \bigcup_{x \in U} R_B \uparrow [x]_d - \bigcup_{x \in U} R_B \downarrow [x]_d</code>
</p>
<p>Furthermore, we can calculate the degree of dependency of the decision on a set of attributes. The decision attribute <code class="reqn">d</code>
depends totally on a set of attributes <code class="reqn">B</code>, denoted <code class="reqn">B \Rightarrow d</code>,
if all  attribute values from <code class="reqn">d</code> are uniquely determined by values of attributes from <code class="reqn">B</code>. It can be defined as follows.
For <code class="reqn">B \subseteq A</code>, it is said that <code class="reqn">d</code> depends on <code class="reqn">B</code> in a degree of dependency <code class="reqn">\gamma_{B} = \frac{|POS_{B}|}{|U|}</code>. 
</p>
<p>A decision reduct is a set <code class="reqn">B \subseteq A</code> such that <code class="reqn">\gamma_{B} = \gamma_{A}</code> and <code class="reqn">\gamma_{B'} &lt; \gamma_{B}</code> for every <code class="reqn">B' \subset B</code>.
One algorithm to determine all reducts is by constructing the decision-relative discernibility matrix. 
The discernibility matrix <code class="reqn">M(\mathcal{A})</code> is an <code class="reqn">n \times n</code> matrix <code class="reqn">(c_{ij})</code> where
</p>
<p><code class="reqn">c_{ij} = \{a \in A: a(x_i) \neq a(x_j) \}</code> if <code class="reqn">d(x_i) \neq d(x_j)</code> and
</p>
<p><code class="reqn">c_{ij} = \oslash</code> otherwise
</p>
<p>The discernibility function <code class="reqn">f_{\mathcal{A}}</code> for a decision system <code class="reqn">\mathcal{A}</code> is a boolean function of <code class="reqn">m</code> boolean variables <code class="reqn">\bar{a}_1, \ldots, \bar{a}_m</code>
corresponding to the attributes <code class="reqn">a_1, \ldots, a_m</code> respectively, and defined by
</p>
<p><code class="reqn">f_{\mathcal{A}}(\bar{a_1}, \ldots, \bar{a_m}) = \wedge \{\vee \bar{c}_{ij}: 1 \le j &lt; i \le n, c_{ij} \neq \oslash \}</code>
</p>
<p>where <code class="reqn">\bar{c}_{ij}= \{ \bar{a}: a \in c_{ij}\}</code>. The decision reducts of <code class="reqn">A</code> are then the prime implicants of the function <code class="reqn">f_{\mathcal{A}}</code>. 
The complete explanation of the algorithm can be seen in (Skowron and Rauszer, 1992).
</p>
<p>The implementations of the RST concepts can be seen in <code>BC.IND.relation.RST</code>, 
</p>
<p><code>BC.LU.approximation.RST</code>, <code>BC.positive.reg.RST</code>, and 
</p>
<p><code>BC.discernibility.mat.RST</code>.
</p>


<h3>Introduction to Fuzzy Rough Set Theory</h3>

<p>This part introduces briefly fuzzy rough set theory (FRST) and its application to data analysis. 
Since recently there are a lot of FRST variants that have been
proposed by researchers, in this introduction, we only provide some basic concepts of FRST based on (Radzikowska and Kerre, 2002). 
</p>
<p>Just like in RST (see <code>Introduction-RoughSets</code>),
a data set is represented as a table called an information system <code class="reqn">\mathcal{A} = (U, A)</code>, where
<code class="reqn">U</code> is a non-empty set of finite objects as the universe of discourse (note: it refers to all instances/experiments/rows 
in datasets) and <code class="reqn">A</code> is a non-empty finite set of attributes, such that <code class="reqn">a : U \to V_{a}</code> for every <code class="reqn">a \in A</code>. 
The set <code class="reqn">V_{a}</code> is the set of values that attribute <code class="reqn">a</code> may take. Information systems that involve a decision attribute, 
containing classes or decision values of each objects, are called decision systems (or said as decision tables). More formally, it is a pair <code class="reqn">\mathcal{A} = (U, A \cup \{d\})</code>,
where <code class="reqn">d \notin A</code> is the decision attribute. The elements of <code class="reqn">A</code> are called conditional attributes. However, different from RST, FRST has several ways
to express indiscernibility. 
</p>
<p>Fuzzy indiscernibility relation (FIR) is used for any fuzzy relation that determines the degree to which two objects are indiscernible. 
We consider some special cases of FIR.
</p>

<ul>
<li>
<p> fuzzy tolerance relation: this relation has properties which are reflexive and symmetric where
</p>
<p>reflexive: <code class="reqn">R(x,x) = 1</code>
</p>
<p>symmetric: <code class="reqn">R(x,y) = R(y,x)</code>
</p>
</li>
<li>
<p> similarity relation (also called fuzzy equivalence relation): this relation has properties not only reflexive and symmetric but also
transitive defined as
</p>
<p><code class="reqn">min(R(x,y), R(y,z)) \le R(x,z)</code>
</p>
</li>
<li> <p><code class="reqn">\mathcal{T}</code>-similarity relation (also called fuzzy <code class="reqn">\mathcal{T}</code>-equivalence relation): this relation is a fuzzy tolerance relation that is also <code class="reqn">\mathcal{T}</code>-transitive.
</p>
<p><code class="reqn">\mathcal{T}(R(x,y), R(y,z)) \le R(x,z)</code>, for a given triangular norm <code class="reqn">\mathcal{T}</code>. 
</p>
</li>
</ul>
<p>The following equations are the tolerance relations on a quantitative attribute <code class="reqn">a</code>, <code class="reqn">R_a</code>, proposed by (Jensen and Shen, 2009).
</p>

<ul>
<li> <p><code>eq.1</code>: <code class="reqn">R_a(x,y) = 1 - \frac{|a(x) - a(y)|}{|a_{max} - a_{min}|}</code>
</p>
</li>
<li> <p><code>eq.2</code>: <code class="reqn">R_a(x,y) = exp(-\frac{(a(x) - a(y))^2}{2 \sigma_a^2})</code>
</p>
</li>
<li> <p><code>eq.3</code>: <code class="reqn">R_a(x,y) = max(min(\frac{a(y) - a(x) + \sigma_a}{\sigma_a}, \frac{a(x) - a(y) + \sigma_a}{\sigma_a}), 0)</code>
</p>
</li>
</ul>
<p>where <code class="reqn">\sigma_{a}^2</code> is the variance of feature <code class="reqn">a</code> and <code class="reqn">a_{min}</code> and <code class="reqn">a_{max}</code> are the minimal and maximal values of data supplied by user. 
Additionally, other relations have been implemented in <code>BC.IND.relation.FRST</code>
</p>
<p>For a qualitative (i.e., nominal) attribute <code class="reqn">a</code>, the classical manner of discerning objects is used, i.e., <code class="reqn">R_a(x,y) = 1</code>
if <code class="reqn">a(x) = a(y)</code> and <code class="reqn">R_a(x,y) = 0</code>, otherwise. We can then define, for any subset <code class="reqn">B</code> of <code class="reqn">A</code>, the fuzzy <code class="reqn">B</code>-indiscernibility relation by
</p>
<p><code class="reqn">R_B(x,y) = \mathcal{T}(R_a(x,y))</code>,
</p>
<p>where <code class="reqn">\mathcal{T}</code> is a t-norm operator, for instance minimum, product and Lukasiewicz t-norm. 
In general, <code class="reqn">\mathcal{T}</code> can be replaced by any aggregation operator, like e.g., the average.
</p>
<p>In the context of FRST, according to (Radzikowska and Kerre, 2002) lower and upper approximation 
are generalized by means of an implicator <code class="reqn">\mathcal{I}</code> and a t-norm <code class="reqn">\mathcal{T}</code>. 
The following are the fuzzy <code class="reqn">B</code>-lower and <code class="reqn">B</code>-upper approximations of a fuzzy set <code class="reqn">A</code> in <code class="reqn">U</code>
</p>
<p><code class="reqn">(R_B \downarrow A)(y) = inf_{x \in U} \mathcal{I}(R_B(x,y), A(x))</code>
</p>
<p><code class="reqn">(R_B \uparrow A)(y) = sup_{x \in U} \mathcal{T}(R_B(x,y), A(x))</code>
</p>
<p>The underlying meaning is that <code class="reqn">R_B \downarrow A</code> is the set of elements <em>necessarily</em> satisfying
the concept (strong membership), while <code class="reqn">R_B \uparrow A</code> is the set of elements <em>possibly</em> belonging
to the concept (weak membership). Many other ways to define the approximations can be found in <code>BC.LU.approximation.FRST</code>.
Mainly, these were designed to deal with noise in the data and to make the approximations more robust.
</p>
<p>Based on fuzzy <code class="reqn">B</code>-indiscernibility relations, we define the fuzzy <code class="reqn">B</code>-positive region by, for <code class="reqn">y \in X</code>,
</p>
<p><code class="reqn">POS_B(y) = (\cup_{x \in U} R_B \downarrow R_dx)(y)</code>
</p>
<p>We can define the degree of dependency of <code class="reqn">d</code> on <code class="reqn">B</code>, <code class="reqn">\gamma_{B}</code> by
</p>
<p><code class="reqn">\gamma_{B} = \frac{|POS_{B}|}{|U|} = \frac{\sum_{x \in U} POS_{B}(x)}{|U|}</code>
</p>
<p>A decision reduct is a set <code class="reqn">B \subseteq A</code> such that <code class="reqn">\gamma_{B} = \gamma_{A}</code> and <code class="reqn">\gamma_{B'} = \gamma_{B}</code> for every <code class="reqn">B' \subset B</code>. 
</p>
<p>As we know from rough set concepts (See <code>Introduction-RoughSets</code>), we are able to calculate the 
decision reducts by constructing the decision-relative discernibility matrix. Based on (Tsang et al, 2008), the discernibility matrix
can be defined as follows. 
The discernibility matrix is an <code class="reqn">n \times n</code> matrix <code class="reqn">(c_{ij})</code> where 
for <code class="reqn">i,j = 1, \ldots, n</code>
</p>
<p>1) <code class="reqn">c_{ij}= \{a \in A : 1 - R_{a}(x_i, x_j) \ge	\lambda_i\}</code> if <code class="reqn">\lambda_j &lt; \lambda_i</code>.
</p>
<p>2) <code class="reqn">c_{ij}={\oslash}</code>, otherwise. 
</p>
<p>with <code class="reqn">\lambda_i = (R_A \downarrow R_{d}x_{i})(x_i)</code> and <code class="reqn">\lambda_j = (R_A \downarrow R_{d}x_{j})(x_{j})</code>
</p>
<p>Other approaches of discernibility matrix can be read at <code>BC.discernibility.mat.FRST</code>.
</p>
<p>The other implementations of the FRST concepts can be seen at <code>BC.IND.relation.FRST</code>, 
</p>
<p><code>BC.LU.approximation.FRST</code>, and <code>BC.positive.reg.FRST</code>.
</p>


<h3>Author(s)</h3>

<p>Lala Septem Riza <a href="mailto:lala.s.riza@decsai.ugr.es">lala.s.riza@decsai.ugr.es</a>,
</p>
<p>Andrzej Janusz <a href="mailto:andrzejanusz@gmail.com">andrzejanusz@gmail.com</a>,
</p>
<p>Chris Cornelis <a href="mailto:chriscornelis@decsai.ugr.es">chriscornelis@decsai.ugr.es</a>,
</p>
<p>Francisco Herrera <a href="mailto:herrera@decsai.ugr.es">herrera@decsai.ugr.es</a>,
</p>
<p>Dominik Slezak <a href="mailto:slezak@mimuw.edu.pl">slezak@mimuw.edu.pl</a>,
</p>
<p>and Jose Manuel Benitez <a href="mailto:j.m.benitez@decsai.ugr.es">j.m.benitez@decsai.ugr.es</a>
</p>
<p>DiCITS Lab, SCI2S group, CITIC-UGR, DECSAI, University of Granada,
</p>
<p><a href="https://sci2s.ugr.es">https://sci2s.ugr.es</a>
</p>
<p>Institute of Mathematics, University of Warsaw.
</p>


<h3>References</h3>

<p>D. Dubois and H. Prade, "Rough Fuzzy Sets and Fuzzy Rough Sets",
International Journal of General Systems, vol. 17, p. 91 - 209 (1990).
</p>
<p><em>General references</em>
</p>
<p>L.A. Zadeh, "Fuzzy Sets",
Information and Control, vol. 8, p. 338 - 353 (1965).
</p>
<p>Z. Pawlak, "Rough Sets",
International Journal of Computer and Information System,
vol. 11, no. 5, p. 341 - 356 (1982).
</p>
<p>Z. Pawlak, "Rough Sets: Theoretical Aspects of Reasoning About Data, System Theory, Knowledge Engineering and Problem Solving",
vol. 9, Kluwer Academic Publishers, Dordrecht, Netherlands (1991).
</p>
<p><em>Introduction to Rough Set Theory  references</em>
</p>
<p>A. Skowron and C. Rauszer,  
"The Discernibility Matrices and Functions in Information Systems", 
in: R. Slowinski (Ed.), Intelligent Decision Support: Handbook of Applications and
Advances of Rough Sets Theory, Kluwer Academic Publishers, Dordrecht, Netherland,  
p. 331 - 362 (1992).
</p>
<p>Z. Pawlak, "Rough Sets", 
International Journal of Computer and Information System, 
vol. 11, no.5, p. 341 - 356 (1982).
</p>
<p>Z. Pawlak, "Rough Sets: Theoretical Aspects of Reasoning about Data, System Theory, Knowledge Engineering and Problem Solving",
vol. 9, Kluwer Academic Publishers, Dordrecht, Netherlands (1991). 
</p>
<p><em>Introduction to Fuzzy Rough Set Theory references</em>
</p>
<p>A. M. Radzikowska and E. E. Kerre, "A Comparative Study of Fuzzy Rough Sets", 
Fuzzy Sets and Systems, vol. 126, p. 137 - 156 (2002). 
</p>
<p>D. Dubois and H. Prade, "Rough Fuzzy Sets and Fuzzy Rough Sets",
International Journal of General Systems, vol. 17, p. 91 - 209 (1990).
</p>
<p>E. C. C. Tsang, D. G. Chen, D. S. Yeung, X. Z. Wang, and J. W. T. Lee, 
"Attributes Reduction Using Fuzzy Rough Sets", IEEE Trans. Fuzzy Syst., 
vol. 16, no. 5, p. 1130 - 1141 (2008).
</p>
<p>L. A. Zadeh, "Fuzzy Sets",
Information and Control, vol. 8, p. 338 - 353 (1965).
</p>
<p>R. Jensen and Q. Shen,  
"New Approaches to Fuzzy-Rough Feature Selection", 
IEEE Trans. on Fuzzy Systems, vol. 19, no. 4,
p. 824 - 838 (2009).
</p>
<p>Z. Pawlak, "Rough Sets", International Journal of Computer and Information Sciences, 
vol. 11, no. 5, p. 341 - 356 (1982).
</p>


<h3>Examples</h3>

<pre><code class="language-R">##############################################################
## A.1 Example: Basic concepts of rough set theory
##############################################################
## Using hiring data set, see RoughSetData
data(RoughSetData)
decision.table &lt;- RoughSetData$hiring.dt

## define considered attributes which are first, second, and
## third attributes
attr.P &lt;- c(1,2,3)

## compute indiscernibility relation
IND &lt;- BC.IND.relation.RST(decision.table, feature.set = attr.P)

## compute lower and upper approximations
roughset &lt;- BC.LU.approximation.RST(decision.table, IND)

## Determine regions
region.RST &lt;- BC.positive.reg.RST(decision.table, roughset)

## The decision-relative discernibility matrix and reduct
disc.mat &lt;- BC.discernibility.mat.RST(decision.table, range.object = NULL)

##############################################################
## A.2 Example: Basic concepts of fuzzy rough set theory
##############################################################
## Using pima7 data set, see RoughSetData
data(RoughSetData)
decision.table &lt;- RoughSetData$pima7.dt

## In this case, let us consider the first and second attributes
conditional.attr &lt;- c(1, 2)

## We are using the "lukasiewicz" t-norm and the "tolerance" relation
## with "eq.1" as fuzzy similarity equation
control.ind &lt;- list(type.aggregation = c("t.tnorm", "lukasiewicz"),
                    type.relation = c("tolerance", "eq.1"))

## Compute fuzzy indiscernibility relation
IND.condAttr &lt;- BC.IND.relation.FRST(decision.table, attributes = conditional.attr,
                            control = control.ind)

## Compute fuzzy lower and upper approximation using type.LU : "implicator.tnorm"
## Define index of decision attribute
decision.attr = c(9)

## Compute fuzzy indiscernibility relation of decision attribute
## We are using "crisp" for type of aggregation and type of relation
control.dec &lt;- list(type.aggregation = c("crisp"), type.relation = "crisp")

IND.decAttr &lt;- BC.IND.relation.FRST(decision.table, attributes = decision.attr,
                            control = control.dec)

## Define control parameter containing type of implicator and t-norm
control &lt;- list(t.implicator = "lukasiewicz", t.tnorm = "lukasiewicz")

## Compute fuzzy lower and upper approximation
FRST.LU &lt;- BC.LU.approximation.FRST(decision.table, IND.condAttr, IND.decAttr,
              type.LU = "implicator.tnorm", control = control)

## Determine fuzzy positive region and its degree of dependency
fuzzy.region &lt;- BC.positive.reg.FRST(decision.table, FRST.LU)

###############################################################
## B Example : Data analysis based on RST and FRST
## In this example, we are using wine dataset for both RST and FRST
###############################################################
## Load the data
## Not run: data(RoughSetData)
dataset &lt;- RoughSetData$wine.dt

## Shuffle the data with set.seed
set.seed(5)
dt.Shuffled &lt;- dataset[sample(nrow(dataset)),]

## Split the data into training and testing
idx &lt;- round(0.8 * nrow(dt.Shuffled))
  wine.tra &lt;-SF.asDecisionTable(dt.Shuffled[1:idx,],
decision.attr = 14, indx.nominal = 14)
  wine.tst &lt;- SF.asDecisionTable(dt.Shuffled[
 (idx+1):nrow(dt.Shuffled), -ncol(dt.Shuffled)])

## DISCRETIZATION
cut.values &lt;- D.discretization.RST(wine.tra,
type.method = "global.discernibility")
d.tra &lt;- SF.applyDecTable(wine.tra, cut.values)
d.tst &lt;- SF.applyDecTable(wine.tst, cut.values)

## FEATURE SELECTION
red.rst &lt;- FS.feature.subset.computation(d.tra,
  method="quickreduct.rst")
fs.tra &lt;- SF.applyDecTable(d.tra, red.rst)

## RULE INDUCTION
rules &lt;- RI.indiscernibilityBasedRules.RST(d.tra,
  red.rst)

## predicting newdata
pred.vals &lt;- predict(rules, d.tst)

#################################################
## Examples: Data analysis using the wine dataset
## 2. Learning and prediction using FRST
#################################################

## FEATURE SELECTION
reduct &lt;- FS.feature.subset.computation(wine.tra,
 method = "quickreduct.frst")

## generate new decision tables
wine.tra.fs &lt;- SF.applyDecTable(wine.tra, reduct)
wine.tst.fs &lt;- SF.applyDecTable(wine.tst, reduct)

## INSTANCE SELECTION
indx &lt;- IS.FRIS.FRST(wine.tra.fs,
 control = list(threshold.tau = 0.2, alpha = 1))

## generate a new decision table
wine.tra.is &lt;- SF.applyDecTable(wine.tra.fs, indx)

## RULE INDUCTION (Rule-based classifiers)
control.ri &lt;- list(
 type.aggregation = c("t.tnorm", "lukasiewicz"),
 type.relation = c("tolerance", "eq.3"),
 t.implicator = "kleene_dienes")

decRules.hybrid &lt;- RI.hybridFS.FRST(wine.tra.is,
  control.ri)

## predicting newdata
predValues.hybrid &lt;- predict(decRules.hybrid,
  wine.tst.fs)

#################################################
## Examples: Data analysis using the wine dataset
## 3. Prediction using fuzzy nearest neighbor classifiers
#################################################

## using FRNN.O
control.frnn.o &lt;- list(m = 2,
  type.membership = "gradual")

predValues.frnn.o &lt;- C.FRNN.O.FRST(wine.tra.is,
  newdata = wine.tst.fs, control = control.frnn.o)

## Using FRNN
control.frnn &lt;- list(type.LU = "implicator.tnorm",k=20,
  type.aggregation = c("t.tnorm", "lukasiewicz"),
  type.relation = c("tolerance", "eq.1"),
  t.implicator = "lukasiewicz")

predValues.frnn &lt;- C.FRNN.FRST(wine.tra.is,
  newdata = wine.tst.fs, control = control.frnn)

## calculating error
real.val &lt;- dt.Shuffled[(idx+1):nrow(dt.Shuffled),
  ncol(dt.Shuffled), drop = FALSE]

err.1 &lt;- 100*sum(pred.vals!=real.val)/nrow(pred.vals)
err.2 &lt;- 100*sum(predValues.hybrid!=real.val)/
  nrow(predValues.hybrid)
err.3 &lt;- 100*sum(predValues.frnn.o!=real.val)/
  nrow(predValues.frnn.o)
err.4 &lt;- 100*sum(predValues.frnn!=real.val)/
  nrow(predValues.frnn)

cat("The percentage error = ", err.1, "\n")
cat("The percentage error = ", err.2, "\n")
cat("The percentage error = ", err.3, "\n")
cat("The percentage error = ", err.4, "\n")
## End(Not run)
</code></pre>


</div>