<div class="container">

<table style="width: 100%;"><tr>
<td>WebCrawler</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Get web pages 
</h2>

<h3>Description</h3>

<p>Discover all links on a given web page, follow each one, and recursively scan every link found.
Return a list of web addresses whose pages contain no links.
</p>


<h3>Usage</h3>

<pre><code class="language-R">WebCrawler(url, depth = NULL, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>url</code></td>
<td>

<p>A URL to scan for links.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>depth</code></td>
<td>

<p>How many links to return.
This avoids having to recursively scan hundreds of links.
Defaults to <code>NULL</code>, which returns everything.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>

<p>Print out each link as it is discovered.
Defaults to <code>TRUE</code>.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>CrawlModels</code> uses this function to get all links present on a model page.
</p>


<h3>Value</h3>

<table><tr style="vertical-align: top;">
<td><code>urls.out</code></td>
<td>
<p>A list of web page addresses, each of which corresponds to a model instance.</p>
</td>
</tr></table>
<h3>Note</h3>

<p>While it might be fun to try <code>WebCrawler</code> on a large website such as Google, the results will be unpredictable and perhaps disastrous if <code>depth</code> is not set.
This is because there is no protection against infinite recursion. 
</p>


<h3>Author(s)</h3>

<p>Daniel C. Bowman <a href="mailto:danny.c.bowman@gmail.com">danny.c.bowman@gmail.com</a>
</p>


<h3>See Also</h3>

<p><code>CrawlModels</code>, <code>ParseModelPage</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
#Find the first 10 model runs for the 
#GFS 0.5x0.5 model

## Not run: urls.out &lt;- WebCrawler(
"http://nomads.ncep.noaa.gov/cgi-bin/filter_gfs_0p50.pl", depth = 10)
## End(Not run)

</code></pre>


</div>