<div class="container">

<table style="width: 100%;"><tr>
<td>nbclust</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Data read and estimate the cluster number</h2>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt="[Experimental]"></a>
</p>
<p><code>nbclust()</code> Reads and prepares the data, and
determine the optimum number of clusters using Average
Silhouette Method by <code>factoextra</code> package.
The average silhouette approach assesses the quality of
clustering by evaluating how well each object fits within
its cluster. A high average silhouette width signifies
effective clustering. This method calculates the average
silhouette for different values of k, and the optimal
number of clusters (k) is the one that maximizes the
average silhouette across a range of potential k values.
</p>


<h3>Usage</h3>

<pre><code class="language-R">nbclust(datap, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>datap</code></td>
<td>
<p>The data set</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>If <code>verbose = TRUE</code> then some results are</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The <code>silhouette</code> coefficient (SC) refers to a criterion
to decide number of clusters.It is defined as follows.
Though there are numerous methods determining number
of clusters such as <code style="white-space: pre;">⁠the gap statistic⁠</code> etc.
</p>
<script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script>
\[SC = maxK{\bar{S}K}\]<p>.
</p>
<p>In other words, for each observation \(i\), the
<code style="white-space: pre;">⁠silhouette width⁠</code> \(s(i)\) is defined as follows:
Put \(a(i)\) = average dissimilarity between i and all
other points of the cluster to which i belongs (if i is the only
observation in its cluster, \(s(i):= 0\) without further
calculations). For all other clusters C, put \(d(i, C)\)
= average dissimilarity of i to all observations of C.
The smallest of these \(d(i, C)\) is
\(b(i)=min\it(C) d(i, C)\), and can be seen as the
dissimilarity between i and its “neighbor” cluster, i.e.,
the nearest one to which it does not belong. Finally,
\[s(i)=:=\frac{b(i)-a(i)}{max(a(i), b(i))}\]
</p>

<ul><li>
<p> Note: The clustering methods can be: "average", "centroid",
"complete", "mcquitty", "median", "single", "ward.D", "ward.D2"
and, Distance methods can be as: "binary", "canberra", "euclidean",
"manhattan", "minkowski", "maximum", "pearson", "spearman", "kendall"
which may be used in <code>shipunov</code> or <code>factoextra</code> packages.
In this package we just applied <code>average=UPGMA</code> and <code>ward</code> algorithms.
</p>
</li></ul>
<h3>Value</h3>

<p>Returns a data frame
</p>


<h3>Author(s)</h3>


<p>Ali Arminian <a href="mailto:abeyran@gmail.com">abeyran@gmail.com</a>

</p>


<h3>References</h3>

<p>Lletı, R., Ortiz, M.C., Sarabia, L.A., Sánchez, M.S. 2004.
Selecting variables for k-means cluster analysis by using
a genetic algorithm that optimizes the silhouettes,
Analytica Chimica Acta, 515(1): 87-100.
</p>
<p>Rousseeuw, P.J. (1987) Silhouettes: A graphical aid to the
interpretation and validation of cluster analysis.
J. Comput. Appl. Math., 20, 53-65.
</p>
<p>https://uc-r.github.io/
</p>


<h3>Examples</h3>

<pre><code class="language-R">
library(factoextra)

data(maize)
maize &lt;- as.data.frame(maize)
row.names(maize) &lt;- maize[, 1]
maize[, 1] = NULL
GEN &lt;- row.names(maize)
maize &lt;- scale(maize)
nbclust(maize, verbose = FALSE)

# Performing bootstrap or jackknife clustering
# by shipunov package. The examples should be run in the
# console manually due to problems occurs in the ORPHANED
# package `shipunov`.
#
# 1- Bootstrap clustering:
# data.jb &lt;- Jclust(maize,
#   method.d = "euclidean",
#    method.c = "average", n.cl = 2,
#     bootstrap = TRUE)
#
# plot.Jclust(data.jb, top=TRUE, lab.pos=1,
#  lab.offset=1, lab.col=2, lab.font=2)
# Fence(data.jb$hclust, GEN)
#
# data.jb &lt;- Jclust(maize,
#  method.d = "euclidean",
#    method.c = "ward.D", n.cl = 2,
#     bootstrap = TRUE)
#
# plot.Jclust(data.jb, top=TRUE, lab.pos=1,
#            lab.offset=1, lab.col=2, lab.font=2)
# Fence(data.jb$hclust, GEN)
#
# if(verbose = TRUE):
# cat("\nnumber of iterations:\n", data.jb$iter, "\n")
#
# for "bootstrap":
# data.jb$mat &lt;- as.matrix((data.jb$mat))
# data.jb$mat
# cat("\nmatrix of results:\n", data.jb$mat, "\n")
# cat("clustering info, by eucledean distance measure:\n")
# print(data.jb$hclust)
# cat("groups:\n", data.jb$gr, "\n")
# cat("\nsupport values:\n", data.jb$supp, "\n")
# cat("\nnumber of clusters used:\n", data.jb$n.cl, "\n")

# 2- Jackknife clustering:
# data.jb &lt;- Bclust(maize,
#   method.d = "euclidean", method.c = "average",
#    bootstrap = FALSE)
# plot(data.jb)
#
# data.jb &lt;- Bclust(maize,
#   method.d = "euclidean", method.c = "ward.D",
#    bootstrap = FALSE)
# plot(data.jb)
#
# if(verbose = TRUE):
# For"jackknife":
# cat("Consensus:\n", data.jb$consensus, "\n")
# cat("Vlaues:\n", data.jb$values, "\n")

</code></pre>


</div>