<div class="container">

<table style="width: 100%;"><tr>
<td>starspace</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Interface to Starspace for training a Starspace model</h2>

<h3>Description</h3>

<p>Interface to Starspace for training a Starspace model, providing raw access to the C++ functionality.
</p>


<h3>Usage</h3>

<pre><code class="language-R">starspace(
  model = "textspace.bin",
  file,
  trainMode = 0,
  fileFormat = c("fastText", "labelDoc"),
  label = "__label__",
  dim = 100,
  epoch = 5,
  lr = 0.01,
  loss = c("hinge", "softmax"),
  margin = 0.05,
  similarity = c("cosine", "dot"),
  negSearchLimit = 50,
  adagrad = TRUE,
  ws = 5,
  minCount = 1,
  minCountLabel = 1,
  ngrams = 1,
  thread = 1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>the full path to where the model file will be saved. Defaults to 'textspace.bin'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>file</code></td>
<td>
<p>the full path to the file on disk which will be used for training.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trainMode</code></td>
<td>
<p>integer with the training mode. Possible values are 0, 1, 2, 3, 4 or 5. Defaults to 0. The use cases are
</p>

<ul>
<li>
<p> 0: tagspace (classification tasks) and search tasks
</p>
</li>
<li>
<p> 1: pagespace &amp; docspace (interest-based or content-based recommendation)
</p>
</li>
<li>
<p> 2: articlespace (sentences within document)
</p>
</li>
<li>
<p> 3: sentence embeddings and entity similarity 
</p>
</li>
<li>
<p> 4: multi-relational graphs
</p>
</li>
<li>
<p> 5: word embeddings 
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fileFormat</code></td>
<td>
<p>either one of 'fastText' or 'labelDoc'. See the documentation of StarSpace</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>label</code></td>
<td>
<p>labels prefix (character string identifying how a label is prefixed, defaults to '__label__')</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dim</code></td>
<td>
<p>the size of the embedding vectors (integer, defaults to 100)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epoch</code></td>
<td>
<p>number of epochs (integer, defaults to 5)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lr</code></td>
<td>
<p>learning rate (numeric, defaults to 0.01)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss</code></td>
<td>
<p>loss function (either 'hinge' or 'softmax')</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>margin</code></td>
<td>
<p>margin parameter in case of hinge loss (numeric, defaults to 0.05)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>similarity</code></td>
<td>
<p>cosine or dot product similarity in cas of hinge loss (character, defaults to 'cosine')</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>negSearchLimit</code></td>
<td>
<p>number of negatives sampled (integer, defaults to 50)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>adagrad</code></td>
<td>
<p>whether to use adagrad in training (logical)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ws</code></td>
<td>
<p>the size of the context window for word level training - only used in trainMode 5 (integer, defaults to 5)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>minCount</code></td>
<td>
<p>minimal number of word occurences for being part of the dictionary (integer, defaults to 1 keeping all words)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>minCountLabel</code></td>
<td>
<p>minimal number of label occurences for being part of the dictionary (integer, defaults to 1 keeping all labels)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ngrams</code></td>
<td>
<p>max length of word ngram (integer, defaults to 1, using only unigrams)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>thread</code></td>
<td>
<p>integer with the number of threads to use. Defaults to 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>arguments passed on to ruimtehol:::textspace. See the details below.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>an object of class textspace which is a list with elements 
</p>

<ul>
<li>
<p> model: a Rcpp pointer to the model
</p>
</li>
<li>
<p> args: a list with elements
</p>

<ol>
<li>
<p> file: the binary file of the model saved on disk
</p>
</li>
<li>
<p> dim: the dimension of the embedding
</p>
</li>
<li>
<p> data: data-specific Starspace training parameters
</p>
</li>
<li>
<p> param: algorithm-specific Starspace training parameters
</p>
</li>
<li>
<p> dictionary: parameters which define ths dictionary of words and labels in Starspace
</p>
</li>
<li>
<p> options: parameters specific to duration of training, the text preparation and the training batch size
</p>
</li>
<li>
<p> test: parameters specific to model testing
</p>
</li>
</ol>
</li>
<li>
<p> iter: a list with element epoch, lr, error and error_validation showing the error after each epoch
</p>
</li>
</ul>
<h3>Note</h3>

<p>The function <code>starspace</code> is a tiny wrapper over the internal function ruimtehol:::textspace which 
allows direct access to the C++ code in order to run Starspace. <br>
The following arguments are available in that functionality when you do the training. 
Default settings are shown next to the definition. Some of these arguments are directly set in the <code>starspace</code> function,
others can be passed on with ... . <br></p>
<p><strong>Arguments which define how the training is done:</strong>
</p>

<ul>
<li>
<p> dim:             size of embedding vectors [100]
</p>
</li>
<li>
<p> epoch:           number of epochs [5]
</p>
</li>
<li>
<p> lr:              learning rate [0.01]
</p>
</li>
<li>
<p> loss:            loss function: hinge, softmax [hinge]
</p>
</li>
<li>
<p> margin:          margin parameter in hinge loss. It's only effective if hinge loss is used. [0.05]
</p>
</li>
<li>
<p> similarity:      takes value in [cosine, dot]. Whether to use cosine or dot product as similarity function in  hinge loss. It's only effective if hinge loss is used. [cosine]
</p>
</li>
<li>
<p> negSearchLimit:  number of negatives sampled [50]
</p>
</li>
<li>
<p> maxNegSamples:   max number of negatives in a batch update [10]
</p>
</li>
<li>
<p> p:               normalization parameter: normalize sum of embeddings by dividing Size^p [0.5]
</p>
</li>
<li>
<p> adagrad:         whether to use adagrad in training [1]
</p>
</li>
<li>
<p> ws:              only used in trainMode 5, the size of the context window for word level training. [5]
</p>
</li>
<li>
<p> dropoutLHS:      dropout probability for LHS features. [0]
</p>
</li>
<li>
<p> dropoutRHS:      dropout probability for RHS features. [0]
</p>
</li>
<li>
<p> shareEmb:        whether to use the same embedding matrix for LHS and RHS. [1]
</p>
</li>
<li>
<p> initRandSd:      initial values of embeddings are randomly generated from normal distribution with mean=0, standard deviation=initRandSd. [0.001]
</p>
</li>
</ul>
<p><strong>Arguments specific to the dictionary of words and labels:</strong>
</p>

<ul>
<li>
<p> minCount:        minimal number of word occurences [1]
</p>
</li>
<li>
<p> minCountLabel:   minimal number of label occurences [1]
</p>
</li>
<li>
<p> ngrams:          max length of word ngram [1]
</p>
</li>
<li>
<p> bucket:          number of buckets [100000]
</p>
</li>
<li>
<p> label:           labels prefix [__label__]
</p>
</li>
</ul>
<p><strong>Arguments which define early stopping or proceeding of model building:</strong>
</p>

<ul>
<li>
<p> initModel:       if not empty, it loads a previously trained model in -initModel and carry on training.
</p>
</li>
<li>
<p> validationFile:  validation file path
</p>
</li>
<li>
<p> validationPatience:    number of iterations of validation where does not improve before we stop training [10]
</p>
</li>
<li>
<p> saveEveryEpoch:  save intermediate models after each epoch [0]
</p>
</li>
<li>
<p> saveTempModel:   save intermediate models after each epoch with an unique name including epoch number [0]
</p>
</li>
<li>
<p> maxTrainTime:    max train time (secs) [8640000]
</p>
</li>
</ul>
<p><strong>Other:</strong>
</p>

<ul>
<li>
<p> trainWord:       whether to train word level together with other tasks (for multi-tasking). [0]
</p>
</li>
<li>
<p> wordWeight:      if trainWord is true, wordWeight specifies example weight for word level training examples. [0.5]
</p>
</li>
<li>
<p> useWeight        whether input file contains weights [0]
</p>
</li>
</ul>
<h3>References</h3>

<p><a href="https://github.com/facebookresearch">https://github.com/facebookresearch</a>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
data(dekamer, package = "ruimtehol")
x &lt;- strsplit(dekamer$question, "\\W")
x &lt;- lapply(x, FUN = function(x) x[x != ""])
x &lt;- sapply(x, FUN = function(x) paste(x, collapse = " "))

idx &lt;- sample.int(n = nrow(dekamer), size = round(nrow(dekamer) * 0.7))
writeLines(x[idx], con = "traindata.txt")
writeLines(x[-idx], con = "validationdata.txt")

set.seed(123456789)
m &lt;- starspace(file = "traindata.txt", validationFile = "validationdata.txt", 
               trainMode = 5, dim = 10, 
               loss = "softmax", lr = 0.01, ngrams = 2, minCount = 5,
               similarity = "cosine", adagrad = TRUE, ws = 7, epoch = 3,
               maxTrainTime = 10)
str(starspace_dictionary(m))              
wordvectors &lt;- as.matrix(m)
wv &lt;- starspace_embedding(m, 
                          x = c("Nationale Loterij", "migranten", "pensioen"),
                          type = "ngram")
wv
mostsimilar &lt;- embedding_similarity(wordvectors, wv["pensioen", ])
head(sort(mostsimilar[, 1], decreasing = TRUE), 10)
starspace_knn(m, "koning")

## clean up for cran
file.remove(c("traindata.txt", "validationdata.txt"))

## End(Not run)
</code></pre>


</div>